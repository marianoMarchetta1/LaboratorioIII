{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Datasets/final_dataset_descr.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>99.43861</td>\n",
       "      <td>99.43861</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>10002</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>38.68301</td>\n",
       "      <td>35.72806</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>10003</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>143.49426</td>\n",
       "      <td>143.49426</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>10004</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>184.72927</td>\n",
       "      <td>184.72927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>10005</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periodo  customer_id  product_id  plan_precios_cuidados  cust_request_qty  \\\n",
       "0   201701        10001       20001                      0                11   \n",
       "1   201701        10002       20001                      0                17   \n",
       "2   201701        10003       20001                      0                17   \n",
       "3   201701        10004       20001                      0                 9   \n",
       "4   201701        10005       20001                      0                23   \n",
       "\n",
       "   cust_request_tn         tn cat1         cat2     cat3  brand  sku_size  \\\n",
       "0         99.43861   99.43861   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "1         38.68301   35.72806   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "2        143.49426  143.49426   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "3        184.72927  184.72927   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "4         19.08407   19.08407   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "\n",
       "  descripcion quarter  month  close_quarter  age  \n",
       "0      genoma      Q1      1              0    0  \n",
       "1      genoma      Q1      1              0    0  \n",
       "2      genoma      Q1      1              0    0  \n",
       "3      genoma      Q1      1              0    0  \n",
       "4      genoma      Q1      1              0    0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1: Filtrar y eliminar productos con poca historia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completamos el dataset con 0 para los producto / cliente que no existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_ids = [20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df[df['product_id'].isin(product_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "\n",
    "df = df[df['periodo'] >= '2018-12-01']\n",
    "\n",
    "product_info = df[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion']].drop_duplicates()\n",
    "\n",
    "min_max_periods = df.groupby(['customer_id', 'product_id'])['periodo'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "cont = 1\n",
    "\n",
    "for _, row in min_max_periods.iterrows():\n",
    "    customer_id = row['customer_id']\n",
    "    product_id = row['product_id']\n",
    "    min_period = row['min']\n",
    "    max_period = '2019-12-01'\n",
    "    all_periods = pd.date_range(min_period, max_period, freq='MS')\n",
    "    \n",
    "    combinations = pd.DataFrame({\n",
    "        'customer_id': [customer_id] * len(all_periods),\n",
    "        'product_id': [product_id] * len(all_periods),\n",
    "        'periodo': all_periods\n",
    "    })\n",
    "    \n",
    "    merged_df = pd.merge(combinations, df, on=['customer_id', 'product_id', 'periodo'], how='left')\n",
    "    \n",
    "    merged_df['tn'] = merged_df['tn'].fillna(0)\n",
    "    \n",
    "    merged_df['tn'] = merged_df['tn'].fillna(0)\n",
    "    merged_df['cat1'] = merged_df['product_id'].map(product_info.set_index('product_id')['cat1'])\n",
    "    merged_df['cat2'] = merged_df['product_id'].map(product_info.set_index('product_id')['cat2'])\n",
    "    merged_df['cat3'] = merged_df['product_id'].map(product_info.set_index('product_id')['cat3'])\n",
    "    merged_df['brand'] = merged_df['product_id'].map(product_info.set_index('product_id')['brand'])\n",
    "    merged_df['sku_size'] = merged_df['product_id'].map(product_info.set_index('product_id')['sku_size'])\n",
    "    merged_df['descripcion'] = merged_df['product_id'].map(product_info.set_index('product_id')['descripcion'])\n",
    "    \n",
    "    merged_df['quarter'] = 'Q' + merged_df['periodo'].dt.to_period('Q').astype(str).str[-1]\n",
    "    merged_df['month'] = merged_df['periodo'].dt.month.astype(str).str.zfill(2)\n",
    "    \n",
    "    merged_df['plan_precios_cuidados'] = merged_df['plan_precios_cuidados'].fillna(0)\n",
    "    merged_df['cust_request_qty'] = merged_df['cust_request_qty'].fillna(0)\n",
    "    merged_df['cust_request_tn'] = merged_df['cust_request_tn'].fillna(0)\n",
    "    merged_df['close_quarter'] = merged_df['close_quarter'].fillna(0)\n",
    "    merged_df['age'] = merged_df['age'].fillna(0)\n",
    "    merged_df['mes_inicial'] = min_period\n",
    "    \n",
    "    all_dfs.append(merged_df)\n",
    "    \n",
    "    print(f\"procesado {cont} de {len(min_max_periods)}\")\n",
    "    cont += 1\n",
    "\n",
    "df_full = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "df_full = df_full.sort_values(by=['customer_id', 'product_id', 'periodo'])\n",
    "\n",
    "df_full['periodo'] = df_full['periodo'].dt.strftime('%Y%m')\n",
    "\n",
    "display(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('final_dataset_completo_con_ceros.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"final_dataset_completo_con_ceros.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10618</td>\n",
       "      <td>20845</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>PISOS</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Profesional menta</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>Gel</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Industrial 5L</td>\n",
       "      <td>Q4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>Gel</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Industrial 5L</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>PISOS</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Profesinal pisos plastificados</td>\n",
       "      <td>Q4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>PISOS</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Profesinal pisos plastificados</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10618       20845   201912                    0.0   \n",
       "2040579        10618       20886   201911                    0.0   \n",
       "2040580        10618       20886   201912                    0.0   \n",
       "2040581        10618       20953   201911                    0.0   \n",
       "2040582        10618       20953   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn         tn cat1         cat2  \\\n",
       "0                    20.0        254.62373  254.62373   HC  ROPA LAVADO   \n",
       "1                    53.0        393.26092  386.60688   HC  ROPA LAVADO   \n",
       "2                    39.0        309.90610  309.90610   HC  ROPA LAVADO   \n",
       "3                    23.0        142.87158  130.54927   HC  ROPA LAVADO   \n",
       "4                    33.0        364.37071  364.37071   HC  ROPA LAVADO   \n",
       "...                   ...              ...        ...  ...          ...   \n",
       "2040578               0.0          0.00000    0.00000   HC  PROFESIONAL   \n",
       "2040579               1.0          0.01884    0.01884   HC  PROFESIONAL   \n",
       "2040580               0.0          0.00000    0.00000   HC  PROFESIONAL   \n",
       "2040581               1.0          0.01817    0.01817   HC  PROFESIONAL   \n",
       "2040582               0.0          0.00000    0.00000   HC  PROFESIONAL   \n",
       "\n",
       "            cat3    brand  sku_size                     descripcion quarter  \\\n",
       "0        Liquido    ARIEL      3000                          genoma      Q4   \n",
       "1        Liquido    ARIEL      3000                          genoma      Q1   \n",
       "2        Liquido    ARIEL      3000                          genoma      Q1   \n",
       "3        Liquido    ARIEL      3000                          genoma      Q1   \n",
       "4        Liquido    ARIEL      3000                          genoma      Q2   \n",
       "...          ...      ...       ...                             ...     ...   \n",
       "2040578    PISOS  MUSCULO      5000               Profesional menta      Q4   \n",
       "2040579      Gel  MUSCULO      5000                   Industrial 5L      Q4   \n",
       "2040580      Gel  MUSCULO      5000                   Industrial 5L      Q4   \n",
       "2040581    PISOS  MUSCULO      5000  Profesinal pisos plastificados      Q4   \n",
       "2040582    PISOS  MUSCULO      5000  Profesinal pisos plastificados      Q4   \n",
       "\n",
       "         month  close_quarter   age mes_inicial  \n",
       "0           12            1.0  23.0  2018-12-01  \n",
       "1            1            0.0  24.0  2018-12-01  \n",
       "2            2            0.0  25.0  2018-12-01  \n",
       "3            3            1.0  26.0  2018-12-01  \n",
       "4            4            0.0  27.0  2018-12-01  \n",
       "...        ...            ...   ...         ...  \n",
       "2040578     12            0.0   0.0  2019-11-01  \n",
       "2040579     11            0.0   4.0  2019-11-01  \n",
       "2040580     12            0.0   0.0  2019-11-01  \n",
       "2040581     11            0.0   4.0  2019-11-01  \n",
       "2040582     12            0.0   0.0  2019-11-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Aplicar LabelEncoder a las columnas categóricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10618</td>\n",
       "      <td>20845</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10618       20845   201912                    0.0   \n",
       "2040579        10618       20886   201911                    0.0   \n",
       "2040580        10618       20886   201912                    0.0   \n",
       "2040581        10618       20953   201911                    0.0   \n",
       "2040582        10618       20953   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn         tn  cat1  cat2  cat3  \\\n",
       "0                    20.0        254.62373  254.62373     1    10    47   \n",
       "1                    53.0        393.26092  386.60688     1    10    47   \n",
       "2                    39.0        309.90610  309.90610     1    10    47   \n",
       "3                    23.0        142.87158  130.54927     1    10    47   \n",
       "4                    33.0        364.37071  364.37071     1    10    47   \n",
       "...                   ...              ...        ...   ...   ...   ...   \n",
       "2040578               0.0          0.00000    0.00000     1     8    54   \n",
       "2040579               1.0          0.01884    0.01884     1     8    31   \n",
       "2040580               0.0          0.00000    0.00000     1     8    31   \n",
       "2040581               1.0          0.01817    0.01817     1     8    54   \n",
       "2040582               0.0          0.00000    0.00000     1     8    54   \n",
       "\n",
       "         brand  sku_size  descripcion  quarter  month  close_quarter   age  \\\n",
       "0            0      3000          384        3     12            1.0  23.0   \n",
       "1            0      3000          384        0      1            0.0  24.0   \n",
       "2            0      3000          384        0      2            0.0  25.0   \n",
       "3            0      3000          384        0      3            1.0  26.0   \n",
       "4            0      3000          384        1      4            0.0  27.0   \n",
       "...        ...       ...          ...      ...    ...            ...   ...   \n",
       "2040578     21      5000          260        3     12            0.0   0.0   \n",
       "2040579     21      5000          158        3     11            0.0   4.0   \n",
       "2040580     21      5000          158        3     12            0.0   0.0   \n",
       "2040581     21      5000          256        3     11            0.0   4.0   \n",
       "2040582     21      5000          256        3     12            0.0   0.0   \n",
       "\n",
       "        mes_inicial  \n",
       "0        2018-12-01  \n",
       "1        2018-12-01  \n",
       "2        2018-12-01  \n",
       "3        2018-12-01  \n",
       "4        2018-12-01  \n",
       "...             ...  \n",
       "2040578  2019-11-01  \n",
       "2040579  2019-11-01  \n",
       "2040580  2019-11-01  \n",
       "2040581  2019-11-01  \n",
       "2040582  2019-11-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'quarter']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_full[col] = le.fit_transform(df_full[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "display(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: Agrupar ventas por periodo, cat1, cat2, cat3, brand, customer_id y product_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "scaled_data = []\n",
    "\n",
    "for (product_id, customer_id), group in df_full.groupby(['product_id', 'customer_id']):\n",
    "    group = group.copy()\n",
    "    scaler = StandardScaler()\n",
    "    group['tn'] = scaler.fit_transform(group[['tn']])\n",
    "    \n",
    "    # Guardar el scaler en el diccionario con una clave única\n",
    "    key = f'{product_id}_{customer_id}'\n",
    "    scalers[key] = scaler\n",
    "    \n",
    "    # Añadir el grupo escalado a la lista\n",
    "    scaled_data.append(group)\n",
    "\n",
    "scaled_df = pd.concat(scaled_data, ignore_index=True)\n",
    "\n",
    "joblib.dump(scalers, 'scalers.pkl')\n",
    "\n",
    "scaled_df.to_csv('scaled_final_dataset.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10618</td>\n",
       "      <td>20845</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10618       20845   201912                    0.0   \n",
       "2040579        10618       20886   201911                    0.0   \n",
       "2040580        10618       20886   201912                    0.0   \n",
       "2040581        10618       20953   201911                    0.0   \n",
       "2040582        10618       20953   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn         tn  cat1  cat2  cat3  \\\n",
       "0                    20.0        254.62373  254.62373     1    10    47   \n",
       "1                    53.0        393.26092  386.60688     1    10    47   \n",
       "2                    39.0        309.90610  309.90610     1    10    47   \n",
       "3                    23.0        142.87158  130.54927     1    10    47   \n",
       "4                    33.0        364.37071  364.37071     1    10    47   \n",
       "...                   ...              ...        ...   ...   ...   ...   \n",
       "2040578               0.0          0.00000    0.00000     1     8    54   \n",
       "2040579               1.0          0.01884    0.01884     1     8    31   \n",
       "2040580               0.0          0.00000    0.00000     1     8    31   \n",
       "2040581               1.0          0.01817    0.01817     1     8    54   \n",
       "2040582               0.0          0.00000    0.00000     1     8    54   \n",
       "\n",
       "         brand  sku_size  descripcion  quarter  month  close_quarter   age  \\\n",
       "0            0      3000          384        3     12            1.0  23.0   \n",
       "1            0      3000          384        0      1            0.0  24.0   \n",
       "2            0      3000          384        0      2            0.0  25.0   \n",
       "3            0      3000          384        0      3            1.0  26.0   \n",
       "4            0      3000          384        1      4            0.0  27.0   \n",
       "...        ...       ...          ...      ...    ...            ...   ...   \n",
       "2040578     21      5000          260        3     12            0.0   0.0   \n",
       "2040579     21      5000          158        3     11            0.0   4.0   \n",
       "2040580     21      5000          158        3     12            0.0   0.0   \n",
       "2040581     21      5000          256        3     11            0.0   4.0   \n",
       "2040582     21      5000          256        3     12            0.0   0.0   \n",
       "\n",
       "        mes_inicial  \n",
       "0        2018-12-01  \n",
       "1        2018-12-01  \n",
       "2        2018-12-01  \n",
       "3        2018-12-01  \n",
       "4        2018-12-01  \n",
       "...             ...  \n",
       "2040578  2019-11-01  \n",
       "2040579  2019-11-01  \n",
       "2040580  2019-11-01  \n",
       "2040581  2019-11-01  \n",
       "2040582  2019-11-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>0.300570</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>1.379834</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>0.752630</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>-0.714023</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1.198002</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10495</td>\n",
       "      <td>21276</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>-0.238688</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.729144</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00371</td>\n",
       "      <td>1.696976</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.729144</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10495       21276   201912                    0.0   \n",
       "2040579        10550       21276   201909                    0.0   \n",
       "2040580        10550       21276   201910                    0.0   \n",
       "2040581        10550       21276   201911                    0.0   \n",
       "2040582        10550       21276   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn        tn  cat1  cat2  cat3  brand  \\\n",
       "0                    20.0        254.62373  0.300570     1    10    47      0   \n",
       "1                    53.0        393.26092  1.379834     1    10    47      0   \n",
       "2                    39.0        309.90610  0.752630     1    10    47      0   \n",
       "3                    23.0        142.87158 -0.714023     1    10    47      0   \n",
       "4                    33.0        364.37071  1.198002     1    10    47      0   \n",
       "...                   ...              ...       ...   ...   ...   ...    ...   \n",
       "2040578               0.0          0.00000 -0.333333     2     6    18     23   \n",
       "2040579               1.0          0.00075 -0.238688     2     6    18     23   \n",
       "2040580               0.0          0.00000 -0.729144     2     6    18     23   \n",
       "2040581               2.0          0.00371  1.696976     2     6    18     23   \n",
       "2040582               0.0          0.00000 -0.729144     2     6    18     23   \n",
       "\n",
       "         sku_size  descripcion  quarter  month  close_quarter   age  \\\n",
       "0            3000          384        3     12            1.0  23.0   \n",
       "1            3000          384        0      1            0.0  24.0   \n",
       "2            3000          384        0      2            0.0  25.0   \n",
       "3            3000          384        0      3            1.0  26.0   \n",
       "4            3000          384        1      4            0.0  27.0   \n",
       "...           ...          ...      ...    ...            ...   ...   \n",
       "2040578       140          412        3     12            0.0   0.0   \n",
       "2040579       140          412        2      9            1.0   6.0   \n",
       "2040580       140          412        3     10            0.0   0.0   \n",
       "2040581       140          412        3     11            0.0   8.0   \n",
       "2040582       140          412        3     12            0.0   0.0   \n",
       "\n",
       "        mes_inicial  \n",
       "0        2018-12-01  \n",
       "1        2018-12-01  \n",
       "2        2018-12-01  \n",
       "3        2018-12-01  \n",
       "4        2018-12-01  \n",
       "...             ...  \n",
       "2040578  2019-03-01  \n",
       "2040579  2019-09-01  \n",
       "2040580  2019-09-01  \n",
       "2040581  2019-09-01  \n",
       "2040582  2019-09-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id                int64\n",
       "product_id                 int64\n",
       "periodo                    int64\n",
       "plan_precios_cuidados    float64\n",
       "cust_request_qty         float64\n",
       "cust_request_tn          float64\n",
       "tn                       float64\n",
       "cat1                       int64\n",
       "cat2                       int64\n",
       "cat3                       int64\n",
       "brand                      int64\n",
       "sku_size                   int64\n",
       "descripcion                int64\n",
       "quarter                    int64\n",
       "month                      int64\n",
       "close_quarter            float64\n",
       "age                      float64\n",
       "mes_inicial               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupo y sumarizo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = scaled_df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id', 'quarter', 'month']).agg({\n",
    "    'cust_request_qty': 'sum',\n",
    "    'cust_request_tn': 'sum',\n",
    "    'tn': 'sum'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15840</th>\n",
       "      <td>201812</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>0.300570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80495</th>\n",
       "      <td>201901</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>1.379834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175202</th>\n",
       "      <td>201902</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>0.752630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294172</th>\n",
       "      <td>201903</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>-0.714023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431929</th>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1.198002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583070</th>\n",
       "      <td>201905</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>439.90647</td>\n",
       "      <td>1.815680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744214</th>\n",
       "      <td>201906</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.92436</td>\n",
       "      <td>-1.242479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916759</th>\n",
       "      <td>201907</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>144.78714</td>\n",
       "      <td>-0.597595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099078</th>\n",
       "      <td>201908</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.63991</td>\n",
       "      <td>-1.506478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288478</th>\n",
       "      <td>201909</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>111.51691</td>\n",
       "      <td>-0.889808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485389</th>\n",
       "      <td>201910</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>21.0</td>\n",
       "      <td>178.49426</td>\n",
       "      <td>-0.342115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687214</th>\n",
       "      <td>201911</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>240.59870</td>\n",
       "      <td>0.153639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892409</th>\n",
       "      <td>201912</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>214.72185</td>\n",
       "      <td>-0.307856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         periodo  cat1  cat2  cat3  brand  customer_id  product_id  quarter  \\\n",
       "15840     201812     1    10    47      0        10001       20001        3   \n",
       "80495     201901     1    10    47      0        10001       20001        0   \n",
       "175202    201902     1    10    47      0        10001       20001        0   \n",
       "294172    201903     1    10    47      0        10001       20001        0   \n",
       "431929    201904     1    10    47      0        10001       20001        1   \n",
       "583070    201905     1    10    47      0        10001       20001        1   \n",
       "744214    201906     1    10    47      0        10001       20001        1   \n",
       "916759    201907     1    10    47      0        10001       20001        2   \n",
       "1099078   201908     1    10    47      0        10001       20001        2   \n",
       "1288478   201909     1    10    47      0        10001       20001        2   \n",
       "1485389   201910     1    10    47      0        10001       20001        3   \n",
       "1687214   201911     1    10    47      0        10001       20001        3   \n",
       "1892409   201912     1    10    47      0        10001       20001        3   \n",
       "\n",
       "         month  cust_request_qty  cust_request_tn        tn  \n",
       "15840       12              20.0        254.62373  0.300570  \n",
       "80495        1              53.0        393.26092  1.379834  \n",
       "175202       2              39.0        309.90610  0.752630  \n",
       "294172       3              23.0        142.87158 -0.714023  \n",
       "431929       4              33.0        364.37071  1.198002  \n",
       "583070       5              31.0        439.90647  1.815680  \n",
       "744214       6               7.0         65.92436 -1.242479  \n",
       "916759       7              14.0        144.78714 -0.597595  \n",
       "1099078      8               9.0         33.63991 -1.506478  \n",
       "1288478      9              18.0        111.51691 -0.889808  \n",
       "1485389     10              21.0        178.49426 -0.342115  \n",
       "1687214     11              21.0        240.59870  0.153639  \n",
       "1892409     12              18.0        214.72185 -0.307856  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(grouped_df[(grouped_df[\"customer_id\"] == 10001) & (grouped_df[\"product_id\"] == 20001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico DTW para agrupar los registros (series de categorias/clientes similares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivoted_df = grouped_df.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'], columns='periodo', values='tn').fillna(0)\n",
    "\n",
    "inertia = []\n",
    "max_clusters = 30\n",
    "\n",
    "for k in range(4, max_clusters + 1):\n",
    "    print(f\"Running K: {k}\")\n",
    "    model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", random_state=0)\n",
    "    model.fit(pivoted_df.values)\n",
    "    inertia.append(model.inertia_)\n",
    "    display(inertia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(4, max_clusters + 1), inertia, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máximo valor de la columna 'nombre_de_la_columna': 3.4641016151377557\n",
      "Mínimo valor de la columna 'nombre_de_la_columna': -2.479356382671416\n"
     ]
    }
   ],
   "source": [
    "# Debug de los valores\n",
    "max_value = pivoted_df[201812].max()\n",
    "min_value = pivoted_df[201812].min()\n",
    "\n",
    "print(f\"Máximo valor de la columna 'nombre_de_la_columna': {max_value}\")\n",
    "print(f\"Mínimo valor de la columna 'nombre_de_la_columna': {min_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2.296914</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10002</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.27780</td>\n",
       "      <td>1.211273</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10003</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.27256</td>\n",
       "      <td>0.342311</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10004</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.13628</td>\n",
       "      <td>-1.040164</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10005</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>-0.011106</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10367</td>\n",
       "      <td>21222</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10482</td>\n",
       "      <td>21192</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10482</td>\n",
       "      <td>21222</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10513</td>\n",
       "      <td>21222</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>21192</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.534522</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         periodo  cat1  cat2  cat3  brand  customer_id  product_id  quarter  \\\n",
       "0         201812     0     0     4     22        10001       20609        3   \n",
       "1         201812     0     0     4     22        10002       20609        3   \n",
       "2         201812     0     0     4     22        10003       20609        3   \n",
       "3         201812     0     0     4     22        10004       20609        3   \n",
       "4         201812     0     0     4     22        10005       20609        3   \n",
       "...          ...   ...   ...   ...    ...          ...         ...      ...   \n",
       "2040578   201912     3    13    82     32        10367       21222        3   \n",
       "2040579   201912     3    13    82     32        10482       21192        3   \n",
       "2040580   201912     3    13    82     32        10482       21222        3   \n",
       "2040581   201912     3    13    82     32        10513       21222        3   \n",
       "2040582   201912     3    13    82     32        10552       21192        3   \n",
       "\n",
       "         month  cust_request_qty  cust_request_tn        tn  cluster  \n",
       "0           12               6.0          0.87535  2.296914       25  \n",
       "1           12               8.0          0.27780  1.211273       15  \n",
       "2           12               1.0          0.27256  0.342311       13  \n",
       "3           12               1.0          0.13628 -1.040164       27  \n",
       "4           12               7.0          0.06290 -0.011106       28  \n",
       "...        ...               ...              ...       ...      ...  \n",
       "2040578     12               0.0          0.00000 -0.426401        3  \n",
       "2040579     12               0.0          0.00000 -0.377964        0  \n",
       "2040580     12               0.0          0.00000 -0.377964        0  \n",
       "2040581     12               0.0          0.00000 -0.577350       24  \n",
       "2040582     12               0.0          0.00000 -0.534522        4  \n",
       "\n",
       "[2040583 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clusters = 30\n",
    "model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=0)\n",
    "cluster_labels = model.fit_predict(pivoted_df.values)\n",
    "\n",
    "pivoted_df['cluster'] = cluster_labels\n",
    "\n",
    "grouped_df = grouped_df.merge(pivoted_df['cluster'], left_on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'], right_index=True)\n",
    "\n",
    "grouped_df.to_csv('grouped_with_30_clusters_scaled.csv', index=False)\n",
    "\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5: Armar un modelo LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "le_factor = 0.1\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(le_factor), input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(LSTM(256, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    \n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(256, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(128, activation='tanh', kernel_regularizer=l2(le_factor)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(128, activation='tanh', kernel_regularizer=l2(le_factor)))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(le_factor))) \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 6: Entrenar y predecir con el modelo LSTM para cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "  Número de registros: 150078\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 21329\n",
      "\n",
      "Cluster 1:\n",
      "  Número de registros: 45339\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3798\n",
      "\n",
      "Cluster 2:\n",
      "  Número de registros: 17991\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 7463\n",
      "\n",
      "Cluster 3:\n",
      "  Número de registros: 76999\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5923\n",
      "\n",
      "Cluster 4:\n",
      "  Número de registros: 140100\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 13456\n",
      "\n",
      "Cluster 5:\n",
      "  Número de registros: 29631\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 2417\n",
      "\n",
      "Cluster 6:\n",
      "  Número de registros: 66404\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5108\n",
      "\n",
      "Cluster 7:\n",
      "  Número de registros: 19475\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3212\n",
      "\n",
      "Cluster 8:\n",
      "  Número de registros: 81501\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 7180\n",
      "\n",
      "Cluster 9:\n",
      "  Número de registros: 106399\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 9811\n",
      "\n",
      "Cluster 10:\n",
      "  Número de registros: 63869\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4913\n",
      "\n",
      "Cluster 11:\n",
      "  Número de registros: 118606\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 10816\n",
      "\n",
      "Cluster 12:\n",
      "  Número de registros: 211805\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 19800\n",
      "\n",
      "Cluster 13:\n",
      "  Número de registros: 74657\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 8945\n",
      "\n",
      "Cluster 14:\n",
      "  Número de registros: 41593\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3553\n",
      "\n",
      "Cluster 15:\n",
      "  Número de registros: 44096\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3392\n",
      "\n",
      "Cluster 16:\n",
      "  Número de registros: 67374\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5661\n",
      "\n",
      "Cluster 17:\n",
      "  Número de registros: 68278\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5720\n",
      "\n",
      "Cluster 18:\n",
      "  Número de registros: 65505\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 7851\n",
      "\n",
      "Cluster 19:\n",
      "  Número de registros: 60274\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5573\n",
      "\n",
      "Cluster 20:\n",
      "  Número de registros: 49884\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4266\n",
      "\n",
      "Cluster 21:\n",
      "  Número de registros: 38015\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3701\n",
      "\n",
      "Cluster 22:\n",
      "  Número de registros: 37441\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3437\n",
      "\n",
      "Cluster 23:\n",
      "  Número de registros: 70929\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 6186\n",
      "\n",
      "Cluster 24:\n",
      "  Número de registros: 66083\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 14036\n",
      "\n",
      "Cluster 25:\n",
      "  Número de registros: 32071\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 2467\n",
      "\n",
      "Cluster 26:\n",
      "  Número de registros: 18550\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 1428\n",
      "\n",
      "Cluster 27:\n",
      "  Número de registros: 73702\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 6268\n",
      "\n",
      "Cluster 28:\n",
      "  Número de registros: 50813\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4350\n",
      "\n",
      "Cluster 29:\n",
      "  Número de registros: 53121\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4798\n",
      "\n",
      "Total 206858\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "total = 0\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_number = i\n",
    "\n",
    "    # Filtrar el DataFrame por el cluster deseado\n",
    "    cluster_data = grouped_df[grouped_df['cluster'] == cluster_number]\n",
    "\n",
    "    unique_combinations = cluster_data[['customer_id', 'product_id']].drop_duplicates().shape[0]\n",
    "    total += unique_combinations\n",
    "    print(f\"Cluster {cluster_number}:\")\n",
    "    print(f\"  Número de registros: {len(cluster_data)}\")\n",
    "    print(f\"  Número de combinaciones únicas 'customer_id' y 'product_id': {unique_combinations}\")\n",
    "    print()\n",
    "    \n",
    "print(f\"Total {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo los pesos y los guardo en un dic para mejorar la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20001: 1504.68856,\n",
       " 20002: 1087.30855,\n",
       " 20003: 892.50129,\n",
       " 20004: 637.90002,\n",
       " 20005: 593.24443,\n",
       " 20006: 417.23228,\n",
       " 20007: 390.43432,\n",
       " 20008: 195.36854,\n",
       " 20009: 495.03574000000003,\n",
       " 20010: 359.59998,\n",
       " 20011: 392.3829,\n",
       " 20012: 173.13004,\n",
       " 20013: 318.09141,\n",
       " 20014: 272.02812,\n",
       " 20015: 297.27663,\n",
       " 20016: 273.20202,\n",
       " 20017: 216.90773,\n",
       " 20018: 141.63569999999999,\n",
       " 20019: 351.54708,\n",
       " 20020: 266.06358,\n",
       " 20021: 203.76721,\n",
       " 20022: 210.8346,\n",
       " 20023: 181.13277,\n",
       " 20024: 270.45018,\n",
       " 20025: 241.83432000000002,\n",
       " 20026: 235.10419,\n",
       " 20027: 155.25876,\n",
       " 20028: 109.92618,\n",
       " 20029: 150.64869,\n",
       " 20030: 102.7572,\n",
       " 20031: 139.91577,\n",
       " 20032: 527.79811,\n",
       " 20033: 96.76212,\n",
       " 20035: 179.97912,\n",
       " 20037: 63.37274,\n",
       " 20038: 157.68477000000001,\n",
       " 20039: 128.40394,\n",
       " 20041: 113.11379,\n",
       " 20042: 124.20086,\n",
       " 20043: 93.77222,\n",
       " 20044: 59.61747,\n",
       " 20045: 149.89961,\n",
       " 20046: 149.9563,\n",
       " 20047: 71.49763,\n",
       " 20049: 124.84836,\n",
       " 20050: 117.02742,\n",
       " 20051: 132.46038000000001,\n",
       " 20052: 95.51068,\n",
       " 20053: 146.36584,\n",
       " 20054: 121.2091,\n",
       " 20055: 65.85747,\n",
       " 20056: 63.7182,\n",
       " 20057: 141.36774,\n",
       " 20058: 117.82985,\n",
       " 20059: 111.51138999999999,\n",
       " 20061: 106.44895,\n",
       " 20062: 90.97637,\n",
       " 20063: 103.52372,\n",
       " 20065: 77.98518,\n",
       " 20066: 119.28134,\n",
       " 20067: 88.49566,\n",
       " 20068: 92.92737,\n",
       " 20069: 136.63984,\n",
       " 20070: 68.44316,\n",
       " 20071: 104.77091,\n",
       " 20072: 32.2686,\n",
       " 20073: 122.47019,\n",
       " 20074: 98.07553,\n",
       " 20075: 105.42961,\n",
       " 20076: 68.45922,\n",
       " 20077: 34.58847,\n",
       " 20079: 27.68496,\n",
       " 20080: 8.84426,\n",
       " 20081: 94.25871,\n",
       " 20082: 78.87082,\n",
       " 20084: 94.68236,\n",
       " 20085: 221.56404,\n",
       " 20086: 42.51979,\n",
       " 20087: 89.63008,\n",
       " 20089: 102.33626,\n",
       " 20090: 35.97712,\n",
       " 20091: 92.24345,\n",
       " 20092: 47.45286,\n",
       " 20093: 73.28412,\n",
       " 20094: 59.12954,\n",
       " 20095: 28.52853,\n",
       " 20096: 44.24611,\n",
       " 20097: 55.29078,\n",
       " 20099: 57.827130000000004,\n",
       " 20100: 1.3792,\n",
       " 20101: 72.80091,\n",
       " 20102: 19.17797,\n",
       " 20103: 63.99622,\n",
       " 20106: 74.51262,\n",
       " 20107: 86.05666,\n",
       " 20108: 58.34718,\n",
       " 20109: 61.8432,\n",
       " 20111: 93.88818,\n",
       " 20112: 107.06263,\n",
       " 20114: 44.16048,\n",
       " 20116: 118.60205,\n",
       " 20117: 36.0325,\n",
       " 20118: 61.15123,\n",
       " 20119: 36.26867,\n",
       " 20120: 72.21068,\n",
       " 20121: 78.96685000000001,\n",
       " 20122: 64.62255,\n",
       " 20123: 59.148179999999996,\n",
       " 20124: 38.10841,\n",
       " 20125: 47.559329999999996,\n",
       " 20126: 94.95288,\n",
       " 20127: 170.32792,\n",
       " 20129: 52.92339,\n",
       " 20130: 83.3077,\n",
       " 20132: 40.09161,\n",
       " 20133: 30.835350000000002,\n",
       " 20134: 49.98357,\n",
       " 20135: 101.03175,\n",
       " 20137: 37.55127,\n",
       " 20138: 28.33495,\n",
       " 20139: 53.48889,\n",
       " 20140: 58.149,\n",
       " 20142: 35.89411,\n",
       " 20143: 29.92779,\n",
       " 20144: 41.30604,\n",
       " 20145: 65.41274,\n",
       " 20146: 30.472260000000002,\n",
       " 20148: 57.163380000000004,\n",
       " 20150: 51.35196,\n",
       " 20151: 44.81184,\n",
       " 20152: 23.926669999999998,\n",
       " 20153: 48.78881,\n",
       " 20155: 61.25175,\n",
       " 20157: 55.77624,\n",
       " 20158: 55.02588,\n",
       " 20159: 61.78339,\n",
       " 20160: 43.18815,\n",
       " 20161: 34.9225,\n",
       " 20162: 51.43634,\n",
       " 20164: 71.3349,\n",
       " 20166: 33.52579,\n",
       " 20167: 43.851729999999996,\n",
       " 20168: 15.155560000000001,\n",
       " 20170: 64.4108,\n",
       " 20174: 91.1178,\n",
       " 20175: 19.70895,\n",
       " 20176: 23.87612,\n",
       " 20177: 24.51696,\n",
       " 20179: 39.70194,\n",
       " 20180: 36.95611,\n",
       " 20181: 33.36284,\n",
       " 20182: 20.87961,\n",
       " 20183: 20.47573,\n",
       " 20184: 26.13296,\n",
       " 20187: 19.99026,\n",
       " 20188: 22.88825,\n",
       " 20189: 32.59489,\n",
       " 20192: 21.45354,\n",
       " 20193: 24.92724,\n",
       " 20196: 35.180600000000005,\n",
       " 20197: 27.90918,\n",
       " 20198: 22.40804,\n",
       " 20200: 20.87857,\n",
       " 20201: 26.166990000000002,\n",
       " 20202: 24.48113,\n",
       " 20203: 55.2097,\n",
       " 20205: 29.13191,\n",
       " 20206: 23.43432,\n",
       " 20207: 18.51692,\n",
       " 20208: 20.14227,\n",
       " 20209: 16.08116,\n",
       " 20210: 58.78261,\n",
       " 20211: 16.10922,\n",
       " 20212: 22.494790000000002,\n",
       " 20213: 58.84043,\n",
       " 20215: 23.14009,\n",
       " 20216: 9.20284,\n",
       " 20218: 31.369320000000002,\n",
       " 20219: 23.10671,\n",
       " 20220: 19.46485,\n",
       " 20222: 21.01334,\n",
       " 20224: 21.26943,\n",
       " 20225: 16.66252,\n",
       " 20226: 11.32801,\n",
       " 20227: 38.712650000000004,\n",
       " 20228: 5.0385100000000005,\n",
       " 20230: 18.17921,\n",
       " 20231: 21.31542,\n",
       " 20232: 28.2828,\n",
       " 20233: 35.59532,\n",
       " 20234: 4.84796,\n",
       " 20235: 32.13552,\n",
       " 20236: 34.70341,\n",
       " 20237: 29.51673,\n",
       " 20238: 16.61631,\n",
       " 20239: 20.10279,\n",
       " 20240: 19.51977,\n",
       " 20241: 12.8158,\n",
       " 20242: 4.03604,\n",
       " 20244: 26.46154,\n",
       " 20246: 24.60939,\n",
       " 20249: 8.00053,\n",
       " 20251: 14.79641,\n",
       " 20252: 12.20597,\n",
       " 20253: 15.26885,\n",
       " 20254: 17.46847,\n",
       " 20255: 12.15816,\n",
       " 20256: 17.63436,\n",
       " 20257: 51.77148,\n",
       " 20259: 12.43223,\n",
       " 20261: 43.74576,\n",
       " 20262: 25.6396,\n",
       " 20263: 20.855059999999998,\n",
       " 20264: 16.582530000000002,\n",
       " 20265: 4.57848,\n",
       " 20266: 49.55769,\n",
       " 20267: 18.01748,\n",
       " 20268: 11.99271,\n",
       " 20269: 19.13008,\n",
       " 20270: 6.778370000000001,\n",
       " 20271: 8.34905,\n",
       " 20272: 12.66437,\n",
       " 20273: 12.4634,\n",
       " 20275: 14.93964,\n",
       " 20276: 18.62694,\n",
       " 20277: 9.61208,\n",
       " 20278: 6.27424,\n",
       " 20280: 17.02489,\n",
       " 20281: 9.81001,\n",
       " 20282: 12.8247,\n",
       " 20283: 12.82352,\n",
       " 20284: 11.79748,\n",
       " 20285: 4.3322,\n",
       " 20286: 50.55931,\n",
       " 20288: 21.12175,\n",
       " 20289: 10.01,\n",
       " 20290: 21.56585,\n",
       " 20291: 9.75289,\n",
       " 20292: 9.80518,\n",
       " 20295: 6.04878,\n",
       " 20296: 18.64666,\n",
       " 20297: 35.089,\n",
       " 20298: 11.28594,\n",
       " 20299: 10.61151,\n",
       " 20300: 17.14787,\n",
       " 20301: 8.31315,\n",
       " 20302: 2.35687,\n",
       " 20303: 18.3878,\n",
       " 20304: 5.88588,\n",
       " 20305: 2.56676,\n",
       " 20306: 32.270920000000004,\n",
       " 20307: 13.525310000000001,\n",
       " 20309: 8.112119999999999,\n",
       " 20310: 9.749849999999999,\n",
       " 20311: 18.16724,\n",
       " 20313: 27.15804,\n",
       " 20314: 18.852050000000002,\n",
       " 20315: 16.64451,\n",
       " 20316: 27.23436,\n",
       " 20317: 14.823310000000001,\n",
       " 20319: 11.81172,\n",
       " 20320: 13.48012,\n",
       " 20321: 27.62797,\n",
       " 20322: 5.57864,\n",
       " 20323: 24.40797,\n",
       " 20324: 10.957410000000001,\n",
       " 20325: 5.26778,\n",
       " 20326: 15.6484,\n",
       " 20327: 11.90046,\n",
       " 20328: 9.333870000000001,\n",
       " 20329: 11.05355,\n",
       " 20330: 13.79536,\n",
       " 20332: 12.96845,\n",
       " 20334: 0.18631999999999999,\n",
       " 20335: 11.7811,\n",
       " 20336: 8.61019,\n",
       " 20337: 13.82469,\n",
       " 20338: 8.13739,\n",
       " 20340: 8.06882,\n",
       " 20341: 8.46183,\n",
       " 20342: 11.97748,\n",
       " 20343: 11.96007,\n",
       " 20344: 15.9908,\n",
       " 20346: 14.96339,\n",
       " 20348: 11.72618,\n",
       " 20349: 17.46373,\n",
       " 20350: 1.23931,\n",
       " 20351: 14.08152,\n",
       " 20352: 17.35029,\n",
       " 20353: 6.63564,\n",
       " 20354: 12.29584,\n",
       " 20355: 25.633470000000003,\n",
       " 20356: 13.82404,\n",
       " 20357: 18.325,\n",
       " 20358: 10.92,\n",
       " 20359: 14.16642,\n",
       " 20361: 13.57393,\n",
       " 20362: 6.61718,\n",
       " 20364: 30.711399999999998,\n",
       " 20365: 10.90371,\n",
       " 20366: 10.34722,\n",
       " 20367: 9.55006,\n",
       " 20368: 10.4105,\n",
       " 20372: 4.66326,\n",
       " 20375: 4.38548,\n",
       " 20377: 7.24521,\n",
       " 20378: 7.68167,\n",
       " 20379: 6.4533,\n",
       " 20380: 14.64208,\n",
       " 20381: 18.21145,\n",
       " 20382: 9.3097,\n",
       " 20383: 7.43444,\n",
       " 20384: 7.21169,\n",
       " 20385: 7.4722,\n",
       " 20386: 11.22573,\n",
       " 20387: 8.58751,\n",
       " 20388: 6.6502799999999995,\n",
       " 20389: 34.54417,\n",
       " 20390: 9.2709,\n",
       " 20395: 26.65657,\n",
       " 20396: 5.93867,\n",
       " 20398: 1.0473,\n",
       " 20400: 9.74484,\n",
       " 20401: 9.16454,\n",
       " 20402: 15.10782,\n",
       " 20403: 10.9533,\n",
       " 20404: 9.25704,\n",
       " 20406: 10.276150000000001,\n",
       " 20407: 6.63483,\n",
       " 20408: 11.77663,\n",
       " 20409: 2.40926,\n",
       " 20410: 10.14597,\n",
       " 20411: 9.12526,\n",
       " 20414: 28.50819,\n",
       " 20415: 4.85906,\n",
       " 20416: 0.8585,\n",
       " 20417: 7.38835,\n",
       " 20418: 0.5267000000000001,\n",
       " 20419: 13.59539,\n",
       " 20421: 11.95817,\n",
       " 20422: 0.92601,\n",
       " 20424: 5.42003,\n",
       " 20426: 8.05316,\n",
       " 20428: 1.65654,\n",
       " 20429: 0.9946900000000001,\n",
       " 20432: 5.4624,\n",
       " 20433: 15.75243,\n",
       " 20434: 8.25951,\n",
       " 20438: 16.27824,\n",
       " 20440: 34.28062,\n",
       " 20442: 23.79055,\n",
       " 20443: 8.00212,\n",
       " 20447: 6.65567,\n",
       " 20449: 1.08866,\n",
       " 20450: 6.24297,\n",
       " 20453: 16.32757,\n",
       " 20456: 6.10371,\n",
       " 20458: 22.48458,\n",
       " 20459: 15.42616,\n",
       " 20460: 8.94893,\n",
       " 20463: 10.21467,\n",
       " 20464: 3.55347,\n",
       " 20465: 3.54101,\n",
       " 20466: 7.27034,\n",
       " 20469: 7.43516,\n",
       " 20470: 8.27397,\n",
       " 20473: 6.6120600000000005,\n",
       " 20474: 9.65317,\n",
       " 20476: 19.73377,\n",
       " 20477: 6.01202,\n",
       " 20478: 5.35491,\n",
       " 20479: 7.92463,\n",
       " 20480: 5.21702,\n",
       " 20481: 8.74287,\n",
       " 20482: 5.68619,\n",
       " 20483: 8.52225,\n",
       " 20484: 1.56548,\n",
       " 20488: 8.62499,\n",
       " 20490: 0.48618999999999996,\n",
       " 20491: 11.94289,\n",
       " 20495: 12.65039,\n",
       " 20496: 6.49094,\n",
       " 20497: 5.5455,\n",
       " 20500: 8.61567,\n",
       " 20502: 5.83704,\n",
       " 20503: 0.83948,\n",
       " 20505: 1.41934,\n",
       " 20508: 4.02445,\n",
       " 20509: 1.10399,\n",
       " 20510: 9.0775,\n",
       " 20513: 12.77476,\n",
       " 20514: 5.77288,\n",
       " 20517: 4.0131,\n",
       " 20520: 6.9888,\n",
       " 20521: 9.54402,\n",
       " 20522: 12.64676,\n",
       " 20523: 7.63168,\n",
       " 20524: 3.37671,\n",
       " 20525: 15.9413,\n",
       " 20526: 13.65706,\n",
       " 20527: 7.74072,\n",
       " 20530: 2.70755,\n",
       " 20531: 12.40991,\n",
       " 20532: 6.38886,\n",
       " 20536: 5.33565,\n",
       " 20537: 12.0668,\n",
       " 20538: 6.02223,\n",
       " 20539: 5.81923,\n",
       " 20540: 6.4430000000000005,\n",
       " 20541: 8.18859,\n",
       " 20542: 8.25353,\n",
       " 20544: 6.43633,\n",
       " 20547: 12.23047,\n",
       " 20548: 12.24028,\n",
       " 20549: 4.18343,\n",
       " 20551: 5.13514,\n",
       " 20552: 2.39711,\n",
       " 20553: 7.04809,\n",
       " 20555: 1.79267,\n",
       " 20558: 3.4579999999999997,\n",
       " 20559: 7.1281,\n",
       " 20560: 5.1622,\n",
       " 20561: 0.37611,\n",
       " 20563: 7.4654,\n",
       " 20565: 3.83425,\n",
       " 20567: 7.02946,\n",
       " 20568: 2.85831,\n",
       " 20569: 12.84918,\n",
       " 20570: 6.88673,\n",
       " 20571: 7.32115,\n",
       " 20572: 3.90293,\n",
       " 20574: 2.60938,\n",
       " 20575: 9.11891,\n",
       " 20576: 12.3194,\n",
       " 20577: 8.44628,\n",
       " 20578: 3.4644,\n",
       " 20579: 3.101,\n",
       " 20580: 15.91044,\n",
       " 20583: 3.65662,\n",
       " 20585: 7.09842,\n",
       " 20586: 0.33821,\n",
       " 20588: 0.6005999999999999,\n",
       " 20589: 6.74483,\n",
       " 20592: 12.04725,\n",
       " 20593: 8.18131,\n",
       " 20596: 4.49828,\n",
       " 20597: 0.23346,\n",
       " 20599: 4.55627,\n",
       " 20600: 1.55091,\n",
       " 20601: 0.57274,\n",
       " 20602: 3.26558,\n",
       " 20603: 7.70408,\n",
       " 20604: 13.179879999999999,\n",
       " 20605: 6.43393,\n",
       " 20606: 2.34434,\n",
       " 20609: 0.47173,\n",
       " 20611: 9.19572,\n",
       " 20612: 4.18386,\n",
       " 20614: 2.23479,\n",
       " 20615: 14.06805,\n",
       " 20617: 1.36963,\n",
       " 20620: 6.91217,\n",
       " 20621: 7.23179,\n",
       " 20622: 4.72215,\n",
       " 20623: 10.20951,\n",
       " 20624: 5.34485,\n",
       " 20627: 9.34447,\n",
       " 20628: 3.28467,\n",
       " 20629: 2.32608,\n",
       " 20632: 2.05983,\n",
       " 20633: 6.92866,\n",
       " 20636: 0.37304,\n",
       " 20637: 4.74964,\n",
       " 20638: 8.33849,\n",
       " 20639: 4.9791,\n",
       " 20640: 2.72739,\n",
       " 20641: 2.19101,\n",
       " 20642: 3.02384,\n",
       " 20644: 2.6208,\n",
       " 20646: 4.05327,\n",
       " 20647: 1.92544,\n",
       " 20649: 14.79183,\n",
       " 20651: 2.37627,\n",
       " 20652: 3.69362,\n",
       " 20653: 3.43338,\n",
       " 20654: 5.23921,\n",
       " 20655: 2.58001,\n",
       " 20657: 3.71165,\n",
       " 20658: 3.40902,\n",
       " 20659: 7.73487,\n",
       " 20660: 0.22825,\n",
       " 20661: 3.46726,\n",
       " 20662: 11.50205,\n",
       " 20663: 0.17495,\n",
       " 20664: 2.96002,\n",
       " 20666: 2.1839999999999997,\n",
       " 20667: 2.54736,\n",
       " 20670: 0.8472799999999999,\n",
       " 20672: 3.4144799999999997,\n",
       " 20673: 9.15194,\n",
       " 20674: 8.38539,\n",
       " 20676: 2.1457800000000002,\n",
       " 20677: 2.29468,\n",
       " 20678: 0.391,\n",
       " 20679: 10.29418,\n",
       " 20680: 5.18177,\n",
       " 20681: 8.83008,\n",
       " 20682: 5.3508000000000004,\n",
       " 20684: 0.76875,\n",
       " 20685: 0.9511499999999999,\n",
       " 20686: 9.60049,\n",
       " 20689: 2.2932,\n",
       " 20691: 7.99868,\n",
       " 20693: 3.71123,\n",
       " 20694: 8.90416,\n",
       " 20696: 4.999,\n",
       " 20697: 2.59869,\n",
       " 20699: 3.03395,\n",
       " 20700: 3.11353,\n",
       " 20701: 3.76215,\n",
       " 20702: 2.03063,\n",
       " 20703: 9.4657,\n",
       " 20705: 1.0319399999999999,\n",
       " 20706: 4.85225,\n",
       " 20708: 4.31759,\n",
       " 20709: 8.64319,\n",
       " 20711: 7.00535,\n",
       " 20713: 1.468,\n",
       " 20714: 4.22997,\n",
       " 20715: 1.62241,\n",
       " 20719: 7.74604,\n",
       " 20720: 8.95012,\n",
       " 20721: 0.10375,\n",
       " 20724: 1.47147,\n",
       " 20729: 0.3598,\n",
       " 20730: 1.58932,\n",
       " 20732: 5.96173,\n",
       " 20733: 1.11124,\n",
       " 20735: 1.37555,\n",
       " 20737: 2.07177,\n",
       " 20739: 2.92295,\n",
       " 20741: 1.35364,\n",
       " 20742: 0.91993,\n",
       " 20743: 1.11232,\n",
       " 20744: 3.90804,\n",
       " 20745: 1.30308,\n",
       " 20746: 4.26365,\n",
       " 20749: 2.73283,\n",
       " 20750: 1.37322,\n",
       " 20751: 3.04284,\n",
       " 20754: 8.55531,\n",
       " 20756: 4.40403,\n",
       " 20757: 2.37376,\n",
       " 20758: 4.3724300000000005,\n",
       " 20759: 1.49063,\n",
       " 20761: 2.47668,\n",
       " 20762: 6.8284899999999995,\n",
       " 20765: 0.25988,\n",
       " 20768: 1.51197,\n",
       " 20771: 0.95168,\n",
       " 20772: 2.53867,\n",
       " 20773: 0.18296,\n",
       " 20774: 5.44871,\n",
       " 20775: 2.19749,\n",
       " 20777: 2.07783,\n",
       " 20781: 0.70834,\n",
       " 20783: 3.65745,\n",
       " 20785: 2.93047,\n",
       " 20786: 0.60138,\n",
       " 20788: 1.47053,\n",
       " 20789: 1.4464299999999999,\n",
       " 20793: 2.64061,\n",
       " 20795: 3.56276,\n",
       " 20800: 0.66127,\n",
       " 20801: 1.66194,\n",
       " 20802: 1.04494,\n",
       " 20803: 1.01973,\n",
       " 20807: 2.14955,\n",
       " 20809: 3.19917,\n",
       " 20810: 1.60242,\n",
       " 20811: 2.65751,\n",
       " 20812: 0.76986,\n",
       " 20815: 6.33522,\n",
       " 20817: 1.35408,\n",
       " 20818: 1.56334,\n",
       " 20820: 1.8666099999999999,\n",
       " 20822: 5.92346,\n",
       " 20823: 1.7907,\n",
       " 20824: 0.28688,\n",
       " 20826: 1.78647,\n",
       " 20827: 2.60942,\n",
       " 20828: 0.47617,\n",
       " 20830: 1.11598,\n",
       " 20831: 2.65266,\n",
       " 20832: 0.23123,\n",
       " 20835: 0.9755199999999999,\n",
       " 20838: 1.96905,\n",
       " 20840: 1.73797,\n",
       " 20843: 0.77417,\n",
       " 20845: 2.22201,\n",
       " 20846: 0.55875,\n",
       " 20847: 1.02539,\n",
       " 20849: 2.0799600000000003,\n",
       " 20852: 0.29701,\n",
       " 20853: 2.8984199999999998,\n",
       " 20855: 1.27043,\n",
       " 20859: 3.06268,\n",
       " 20862: 1.30819,\n",
       " 20863: 1.09065,\n",
       " 20864: 1.22199,\n",
       " 20865: 1.69039,\n",
       " 20870: 1.32583,\n",
       " 20877: 0.8133,\n",
       " 20878: 1.26697,\n",
       " 20879: 1.46764,\n",
       " 20882: 1.48505,\n",
       " 20883: 0.86097,\n",
       " 20885: 1.4784899999999999,\n",
       " 20886: 1.09253,\n",
       " 20892: 1.0854000000000001,\n",
       " 20894: 1.89225,\n",
       " 20899: 2.32761,\n",
       " 20901: 0.73891,\n",
       " 20902: 1.56067,\n",
       " 20904: 4.55923,\n",
       " 20906: 0.72535,\n",
       " 20907: 3.47802,\n",
       " 20908: 2.29554,\n",
       " 20910: 5.20738,\n",
       " 20912: 3.03888,\n",
       " 20913: 0.78256,\n",
       " 20914: 1.06175,\n",
       " 20917: 2.29184,\n",
       " 20919: 0.43074,\n",
       " 20920: 2.4836,\n",
       " 20922: 1.04686,\n",
       " 20924: 3.70952,\n",
       " 20925: 0.53211,\n",
       " 20927: 3.05867,\n",
       " 20928: 2.92793,\n",
       " 20931: 1.43487,\n",
       " 20932: 1.8699999999999999,\n",
       " 20933: 3.68027,\n",
       " 20936: 0.16453,\n",
       " 20937: 1.03081,\n",
       " 20941: 0.82051,\n",
       " 20942: 1.72156,\n",
       " 20945: 0.86828,\n",
       " 20946: 3.14228,\n",
       " 20947: 0.45535000000000003,\n",
       " 20948: 0.41822,\n",
       " 20949: 0.88794,\n",
       " 20951: 0.3287,\n",
       " 20953: 2.3988,\n",
       " 20956: 0.59267,\n",
       " 20957: 1.10726,\n",
       " 20961: 0.83519,\n",
       " 20962: 1.99182,\n",
       " 20965: 0.55034,\n",
       " 20966: 1.73472,\n",
       " 20967: 1.55408,\n",
       " 20968: 1.91479,\n",
       " 20970: 0.43677,\n",
       " 20975: 1.69045,\n",
       " 20976: 0.61586,\n",
       " 20982: 0.13177999999999998,\n",
       " 20985: 0.7459899999999999,\n",
       " 20986: 0.57885,\n",
       " 20987: 0.16926,\n",
       " 20990: 0.4538,\n",
       " 20991: 0.14446,\n",
       " 20994: 0.18545,\n",
       " 20995: 1.55285,\n",
       " 20996: 0.22495,\n",
       " 20997: 0.88972,\n",
       " 21001: 0.52382,\n",
       " 21003: 0.44116,\n",
       " 21006: 0.96358,\n",
       " 21007: 1.8007300000000002,\n",
       " 21008: 0.99321,\n",
       " 21014: 1.09473,\n",
       " 21016: 0.45426,\n",
       " 21022: 0.76216,\n",
       " 21024: 0.39419,\n",
       " 21027: 1.73954,\n",
       " 21028: 0.54486,\n",
       " 21032: 0.17637,\n",
       " 21033: 1.2670000000000001,\n",
       " 21034: 0.55135,\n",
       " 21035: 1.80884,\n",
       " 21037: 0.4412,\n",
       " 21038: 0.4357,\n",
       " 21039: 1.9437399999999998,\n",
       " 21040: 0.6441600000000001,\n",
       " 21042: 1.61798,\n",
       " 21044: 1.76904,\n",
       " 21048: 0.45052000000000003,\n",
       " 21049: 0.30829999999999996,\n",
       " 21055: 0.41164,\n",
       " 21056: 0.64179,\n",
       " 21057: 0.67593,\n",
       " 21058: 1.84115,\n",
       " 21064: 0.32759,\n",
       " 21065: 0.59662,\n",
       " 21073: 0.75292,\n",
       " 21074: 0.00311,\n",
       " 21077: 0.21584,\n",
       " 21079: 0.68459,\n",
       " 21080: 0.37668999999999997,\n",
       " 21084: 0.47287999999999997,\n",
       " 21086: 0.63302,\n",
       " 21087: 1.02205,\n",
       " 21088: 0.08061,\n",
       " 21092: 0.99937,\n",
       " 21093: 0.2881,\n",
       " 21097: 1.34469,\n",
       " 21099: 0.17034,\n",
       " 21105: 0.43487,\n",
       " 21109: 0.9539,\n",
       " 21110: 1.52502,\n",
       " 21111: 1.46188,\n",
       " 21112: 0.55859,\n",
       " 21114: 1.49352,\n",
       " 21118: 0.30167,\n",
       " 21119: 1.14641,\n",
       " 21126: 0.47548,\n",
       " 21129: 0.7841,\n",
       " 21131: 0.08076,\n",
       " 21135: 0.29346,\n",
       " 21140: 0.55694,\n",
       " 21142: 0.10701999999999999,\n",
       " 21144: 0.36366,\n",
       " 21146: 0.39513,\n",
       " 21153: 0.25441,\n",
       " 21154: 0.12383000000000001,\n",
       " 21155: 0.23501,\n",
       " 21157: 0.16893,\n",
       " 21159: 0.14194,\n",
       " 21163: 0.339,\n",
       " 21164: 0.15114,\n",
       " 21167: 0.10550999999999999,\n",
       " 21168: 0.14085,\n",
       " 21170: 0.04078,\n",
       " 21171: 0.30362,\n",
       " 21176: 0.44148000000000004,\n",
       " 21179: 0.29605000000000004,\n",
       " 21180: 0.02,\n",
       " 21182: 0.06688,\n",
       " 21184: 0.15858,\n",
       " 21190: 0.13297,\n",
       " 21191: 0.09023,\n",
       " 21192: 0.012379999999999999,\n",
       " 21194: 0.08985,\n",
       " 21196: 0.19440000000000002,\n",
       " 21200: 0.12786,\n",
       " 21201: 0.18025,\n",
       " 21202: 0.04587,\n",
       " 21207: 0.1208,\n",
       " 21209: 0.07853,\n",
       " 21212: 0.10563,\n",
       " 21214: 0.24428,\n",
       " 21218: 0.03348,\n",
       " 21222: 0.02184,\n",
       " 21224: 0.07537,\n",
       " 21226: 0.04866,\n",
       " 21227: 0.42182000000000003,\n",
       " 21233: 0.03557,\n",
       " 21244: 0.01552,\n",
       " 21245: 0.02403,\n",
       " 21246: 0.02117,\n",
       " 21248: 0.01129,\n",
       " 21252: 0.08560000000000001,\n",
       " 21256: 0.012709999999999999,\n",
       " 21259: 0.01412,\n",
       " 21262: 0.01834,\n",
       " 21263: 0.0127,\n",
       " 21265: 0.050069999999999996,\n",
       " 21266: 0.05121,\n",
       " 21267: 0.015690000000000003,\n",
       " 21276: 0.008919999999999999}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_tn_dict = df[df['periodo'] == 201912].groupby('product_id')['tn'].sum().to_dict()\n",
    "display(total_tn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "86/86 - 69s - 804ms/step - loss: 79.2314 - val_loss: 7.2578 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "86/86 - 50s - 581ms/step - loss: 3.8821 - val_loss: 1.5016 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "86/86 - 50s - 580ms/step - loss: 1.3306 - val_loss: 0.5688 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "86/86 - 50s - 578ms/step - loss: 0.8310 - val_loss: 0.3466 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "86/86 - 50s - 582ms/step - loss: 0.6943 - val_loss: 0.2825 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "86/86 - 60s - 694ms/step - loss: 0.6450 - val_loss: 0.2537 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "86/86 - 50s - 579ms/step - loss: 0.6220 - val_loss: 0.2289 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "86/86 - 47s - 550ms/step - loss: 0.6089 - val_loss: 0.2310 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "86/86 - 47s - 542ms/step - loss: 0.6076 - val_loss: 0.2326 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "86/86 - 47s - 547ms/step - loss: 0.6026 - val_loss: 0.2107 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "86/86 - 46s - 540ms/step - loss: 0.5991 - val_loss: 0.2113 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "86/86 - 51s - 598ms/step - loss: 0.6024 - val_loss: 0.2348 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "86/86 - 51s - 594ms/step - loss: 0.6009 - val_loss: 0.2025 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "86/86 - 51s - 593ms/step - loss: 0.6011 - val_loss: 0.2035 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "86/86 - 53s - 617ms/step - loss: 0.6032 - val_loss: 0.2047 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "86/86 - 50s - 577ms/step - loss: 0.6038 - val_loss: 0.2060 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "86/86 - 50s - 578ms/step - loss: 0.5993 - val_loss: 0.2041 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "86/86 - 51s - 595ms/step - loss: 0.5983 - val_loss: 0.2176 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "86/86 - 49s - 569ms/step - loss: 0.6010 - val_loss: 0.2137 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "86/86 - 48s - 557ms/step - loss: 0.6003 - val_loss: 0.2023 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "86/86 - 53s - 621ms/step - loss: 0.6036 - val_loss: 0.2190 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "86/86 - 53s - 611ms/step - loss: 0.5984 - val_loss: 0.2022 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "86/86 - 57s - 664ms/step - loss: 0.5984 - val_loss: 0.2073 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "86/86 - 58s - 676ms/step - loss: 0.6000 - val_loss: 0.2064 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "86/86 - 55s - 636ms/step - loss: 0.5999 - val_loss: 0.2025 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "86/86 - 54s - 633ms/step - loss: 0.5972 - val_loss: 0.2028 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "86/86 - 53s - 612ms/step - loss: 0.5994 - val_loss: 0.2048 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "86/86 - 53s - 615ms/step - loss: 0.5982 - val_loss: 0.2103 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "86/86 - 55s - 640ms/step - loss: 0.5987 - val_loss: 0.2022 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "86/86 - 57s - 660ms/step - loss: 0.5960 - val_loss: 0.2427 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "86/86 - 54s - 627ms/step - loss: 0.6012 - val_loss: 0.2051 - learning_rate: 2.0000e-04\n",
      "Epoch 32/100\n",
      "86/86 - 55s - 644ms/step - loss: 0.5964 - val_loss: 0.2024 - learning_rate: 2.0000e-04\n",
      "Epoch 33/100\n",
      "86/86 - 54s - 631ms/step - loss: 0.5955 - val_loss: 0.2052 - learning_rate: 2.0000e-04\n",
      "Epoch 34/100\n",
      "86/86 - 52s - 609ms/step - loss: 0.5965 - val_loss: 0.2024 - learning_rate: 2.0000e-04\n",
      "Epoch 35/100\n",
      "86/86 - 55s - 640ms/step - loss: 0.5962 - val_loss: 0.2024 - learning_rate: 2.0000e-04\n",
      "Epoch 36/100\n",
      "86/86 - 53s - 616ms/step - loss: 0.5953 - val_loss: 0.2039 - learning_rate: 2.0000e-04\n",
      "Epoch 37/100\n",
      "86/86 - 54s - 624ms/step - loss: 0.5958 - val_loss: 0.2037 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "86/86 - 51s - 591ms/step - loss: 0.5968 - val_loss: 0.2066 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "86/86 - 53s - 619ms/step - loss: 0.5984 - val_loss: 0.2031 - learning_rate: 2.0000e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "86/86 - 55s - 634ms/step - loss: 0.5965 - val_loss: 0.2023 - learning_rate: 2.0000e-04\n",
      "Epoch 41/100\n",
      "86/86 - 58s - 674ms/step - loss: 0.5953 - val_loss: 0.2022 - learning_rate: 4.0000e-05\n",
      "Epoch 42/100\n",
      "86/86 - 53s - 621ms/step - loss: 0.5950 - val_loss: 0.2024 - learning_rate: 4.0000e-05\n",
      "Epoch 43/100\n",
      "86/86 - 52s - 607ms/step - loss: 0.5952 - val_loss: 0.2023 - learning_rate: 4.0000e-05\n",
      "Epoch 44/100\n",
      "86/86 - 51s - 590ms/step - loss: 0.5951 - val_loss: 0.2025 - learning_rate: 4.0000e-05\n",
      "Epoch 45/100\n",
      "86/86 - 56s - 650ms/step - loss: 0.5953 - val_loss: 0.2027 - learning_rate: 4.0000e-05\n",
      "Epoch 46/100\n",
      "86/86 - 55s - 638ms/step - loss: 0.5951 - val_loss: 0.2025 - learning_rate: 4.0000e-05\n",
      "Epoch 47/100\n",
      "86/86 - 55s - 645ms/step - loss: 0.5951 - val_loss: 0.2023 - learning_rate: 4.0000e-05\n",
      "Epoch 47: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 28s - 2s/step - loss: 293.0375 - val_loss: 196.5015 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "16/16 - 11s - 669ms/step - loss: 154.9049 - val_loss: 113.2679 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "16/16 - 11s - 692ms/step - loss: 95.2192 - val_loss: 77.3082 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "16/16 - 9s - 566ms/step - loss: 67.4893 - val_loss: 57.2991 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "16/16 - 10s - 608ms/step - loss: 54.0723 - val_loss: 50.4852 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "16/16 - 10s - 634ms/step - loss: 46.2079 - val_loss: 42.0475 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "16/16 - 10s - 613ms/step - loss: 41.4995 - val_loss: 40.6371 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "16/16 - 10s - 632ms/step - loss: 38.0937 - val_loss: 34.6249 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "16/16 - 9s - 579ms/step - loss: 35.3758 - val_loss: 33.8314 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "16/16 - 9s - 582ms/step - loss: 32.9441 - val_loss: 33.2696 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "16/16 - 9s - 578ms/step - loss: 30.6650 - val_loss: 30.1669 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "16/16 - 10s - 599ms/step - loss: 29.2916 - val_loss: 31.4458 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "16/16 - 9s - 586ms/step - loss: 29.1245 - val_loss: 26.7481 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "16/16 - 10s - 616ms/step - loss: 27.1662 - val_loss: 25.8505 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "16/16 - 10s - 621ms/step - loss: 25.9171 - val_loss: 26.5973 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "16/16 - 10s - 656ms/step - loss: 25.4259 - val_loss: 25.2679 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "16/16 - 11s - 707ms/step - loss: 24.7477 - val_loss: 22.2266 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "16/16 - 11s - 681ms/step - loss: 24.4770 - val_loss: 21.5750 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "16/16 - 11s - 665ms/step - loss: 23.6102 - val_loss: 23.6192 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "16/16 - 11s - 696ms/step - loss: 22.9148 - val_loss: 23.5807 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "16/16 - 11s - 694ms/step - loss: 22.1538 - val_loss: 22.5637 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "16/16 - 10s - 634ms/step - loss: 22.2813 - val_loss: 23.3895 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "16/16 - 10s - 624ms/step - loss: 21.4591 - val_loss: 22.7764 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "16/16 - 12s - 735ms/step - loss: 20.9821 - val_loss: 19.1052 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "16/16 - 9s - 592ms/step - loss: 20.5499 - val_loss: 21.4871 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "16/16 - 10s - 619ms/step - loss: 20.6220 - val_loss: 21.9177 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "16/16 - 10s - 609ms/step - loss: 20.0722 - val_loss: 18.5865 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "16/16 - 10s - 648ms/step - loss: 20.0872 - val_loss: 18.7958 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "16/16 - 10s - 638ms/step - loss: 19.8034 - val_loss: 18.9133 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "16/16 - 10s - 612ms/step - loss: 19.4784 - val_loss: 18.1391 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "16/16 - 10s - 648ms/step - loss: 19.0624 - val_loss: 18.0236 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "16/16 - 14s - 848ms/step - loss: 18.9750 - val_loss: 19.0441 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "16/16 - 14s - 866ms/step - loss: 18.6184 - val_loss: 20.2526 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "16/16 - 14s - 854ms/step - loss: 19.9417 - val_loss: 21.0786 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "16/16 - 12s - 765ms/step - loss: 19.2022 - val_loss: 18.5351 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "16/16 - 14s - 868ms/step - loss: 18.7956 - val_loss: 17.3356 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "16/16 - 11s - 715ms/step - loss: 18.4647 - val_loss: 17.8276 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "16/16 - 12s - 756ms/step - loss: 18.1662 - val_loss: 17.5782 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "16/16 - 12s - 766ms/step - loss: 18.0583 - val_loss: 17.4984 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "16/16 - 13s - 832ms/step - loss: 17.9547 - val_loss: 17.0623 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "16/16 - 12s - 759ms/step - loss: 17.6253 - val_loss: 16.2356 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "16/16 - 11s - 676ms/step - loss: 18.1019 - val_loss: 18.7120 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "16/16 - 11s - 703ms/step - loss: 17.4531 - val_loss: 17.3101 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "16/16 - 11s - 674ms/step - loss: 17.1816 - val_loss: 17.2439 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "16/16 - 10s - 615ms/step - loss: 17.1313 - val_loss: 16.8181 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "16/16 - 10s - 605ms/step - loss: 17.2267 - val_loss: 17.4219 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "16/16 - 11s - 665ms/step - loss: 17.3285 - val_loss: 16.2106 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "16/16 - 10s - 628ms/step - loss: 17.1651 - val_loss: 17.3190 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "16/16 - 9s - 592ms/step - loss: 16.9695 - val_loss: 16.0710 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "16/16 - 10s - 612ms/step - loss: 17.1636 - val_loss: 16.8307 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "16/16 - 10s - 614ms/step - loss: 16.9048 - val_loss: 17.1189 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "16/16 - 11s - 694ms/step - loss: 16.7226 - val_loss: 16.1909 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "16/16 - 10s - 604ms/step - loss: 16.8611 - val_loss: 16.1103 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "16/16 - 10s - 634ms/step - loss: 16.7926 - val_loss: 16.5333 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "16/16 - 10s - 602ms/step - loss: 16.9204 - val_loss: 18.7260 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "16/16 - 10s - 640ms/step - loss: 17.3032 - val_loss: 14.8147 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "16/16 - 10s - 632ms/step - loss: 16.9329 - val_loss: 18.0405 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "16/16 - 10s - 632ms/step - loss: 16.7198 - val_loss: 16.3924 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "16/16 - 10s - 649ms/step - loss: 16.6512 - val_loss: 16.4556 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "16/16 - 19s - 1s/step - loss: 17.1717 - val_loss: 18.1117 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "16/16 - 9s - 566ms/step - loss: 16.4828 - val_loss: 15.9293 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "16/16 - 9s - 566ms/step - loss: 16.2690 - val_loss: 16.2526 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "16/16 - 12s - 752ms/step - loss: 16.5479 - val_loss: 18.7840 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "16/16 - 12s - 764ms/step - loss: 16.8723 - val_loss: 15.0544 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "16/16 - 11s - 685ms/step - loss: 16.3377 - val_loss: 17.5081 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "16/16 - 11s - 675ms/step - loss: 16.2361 - val_loss: 15.6291 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "16/16 - 12s - 721ms/step - loss: 16.2562 - val_loss: 16.0098 - learning_rate: 2.0000e-04\n",
      "Epoch 68/100\n",
      "16/16 - 11s - 675ms/step - loss: 16.2122 - val_loss: 16.1053 - learning_rate: 2.0000e-04\n",
      "Epoch 69/100\n",
      "16/16 - 11s - 663ms/step - loss: 16.1081 - val_loss: 15.8716 - learning_rate: 2.0000e-04\n",
      "Epoch 70/100\n",
      "16/16 - 10s - 652ms/step - loss: 16.0779 - val_loss: 16.0907 - learning_rate: 2.0000e-04\n",
      "Epoch 71/100\n",
      "16/16 - 10s - 649ms/step - loss: 15.9452 - val_loss: 15.8028 - learning_rate: 2.0000e-04\n",
      "Epoch 72/100\n",
      "16/16 - 9s - 588ms/step - loss: 16.1664 - val_loss: 16.8939 - learning_rate: 2.0000e-04\n",
      "Epoch 73/100\n",
      "16/16 - 10s - 596ms/step - loss: 16.0719 - val_loss: 15.5676 - learning_rate: 2.0000e-04\n",
      "Epoch 74/100\n",
      "16/16 - 11s - 674ms/step - loss: 16.0739 - val_loss: 15.7467 - learning_rate: 2.0000e-04\n",
      "Epoch 75/100\n",
      "16/16 - 11s - 702ms/step - loss: 16.1599 - val_loss: 16.1736 - learning_rate: 2.0000e-04\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "16/16 - 10s - 653ms/step - loss: 16.1091 - val_loss: 16.0889 - learning_rate: 2.0000e-04\n",
      "Epoch 77/100\n",
      "16/16 - 11s - 696ms/step - loss: 15.8972 - val_loss: 16.0794 - learning_rate: 4.0000e-05\n",
      "Epoch 78/100\n",
      "16/16 - 11s - 678ms/step - loss: 16.1275 - val_loss: 16.0992 - learning_rate: 4.0000e-05\n",
      "Epoch 79/100\n",
      "16/16 - 11s - 676ms/step - loss: 16.0423 - val_loss: 16.0425 - learning_rate: 4.0000e-05\n",
      "Epoch 80/100\n",
      "16/16 - 14s - 849ms/step - loss: 16.1146 - val_loss: 15.9342 - learning_rate: 4.0000e-05\n",
      "Epoch 81/100\n",
      "16/16 - 11s - 698ms/step - loss: 16.0223 - val_loss: 16.1225 - learning_rate: 4.0000e-05\n",
      "Epoch 82/100\n",
      "16/16 - 10s - 654ms/step - loss: 16.1098 - val_loss: 15.9925 - learning_rate: 4.0000e-05\n",
      "Epoch 83/100\n",
      "16/16 - 11s - 707ms/step - loss: 16.1134 - val_loss: 16.0145 - learning_rate: 4.0000e-05\n",
      "Epoch 84/100\n",
      "16/16 - 11s - 662ms/step - loss: 16.0445 - val_loss: 15.9348 - learning_rate: 4.0000e-05\n",
      "Epoch 85/100\n",
      "16/16 - 11s - 690ms/step - loss: 15.9441 - val_loss: 15.9934 - learning_rate: 4.0000e-05\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "16/16 - 10s - 630ms/step - loss: 16.1165 - val_loss: 15.9941 - learning_rate: 4.0000e-05\n",
      "Epoch 87/100\n",
      "16/16 - 10s - 651ms/step - loss: 16.0273 - val_loss: 16.0104 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "16/16 - 10s - 635ms/step - loss: 16.1179 - val_loss: 15.9736 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "16/16 - 10s - 616ms/step - loss: 16.0005 - val_loss: 15.9472 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "16/16 - 10s - 614ms/step - loss: 15.9510 - val_loss: 15.9316 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "16/16 - 10s - 618ms/step - loss: 15.8744 - val_loss: 15.9311 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "16/16 - 10s - 602ms/step - loss: 15.9116 - val_loss: 15.8980 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "16/16 - 10s - 629ms/step - loss: 16.0868 - val_loss: 15.9269 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "16/16 - 10s - 638ms/step - loss: 16.0807 - val_loss: 15.9184 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "16/16 - 10s - 623ms/step - loss: 16.0522 - val_loss: 15.9315 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "16/16 - 10s - 642ms/step - loss: 16.1235 - val_loss: 15.9086 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "16/16 - 11s - 694ms/step - loss: 16.0972 - val_loss: 15.9417 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "16/16 - 11s - 674ms/step - loss: 16.0290 - val_loss: 15.9491 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "16/16 - 11s - 662ms/step - loss: 16.0229 - val_loss: 15.9598 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "16/16 - 11s - 694ms/step - loss: 16.1630 - val_loss: 15.9036 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 - 28s - 3s/step - loss: 329.7665 - val_loss: 241.5083 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "8/8 - 5s - 595ms/step - loss: 214.4448 - val_loss: 164.8615 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "8/8 - 4s - 553ms/step - loss: 144.0232 - val_loss: 112.7086 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "8/8 - 5s - 621ms/step - loss: 99.0094 - val_loss: 78.7072 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "8/8 - 6s - 729ms/step - loss: 70.3959 - val_loss: 57.5175 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "8/8 - 5s - 620ms/step - loss: 52.6257 - val_loss: 44.2754 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "8/8 - 6s - 783ms/step - loss: 41.3667 - val_loss: 35.8673 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "8/8 - 5s - 617ms/step - loss: 34.2341 - val_loss: 30.2961 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "8/8 - 5s - 618ms/step - loss: 29.4249 - val_loss: 26.3447 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "8/8 - 5s - 635ms/step - loss: 26.1395 - val_loss: 23.6969 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "8/8 - 6s - 702ms/step - loss: 23.4422 - val_loss: 21.4048 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "8/8 - 7s - 837ms/step - loss: 21.3066 - val_loss: 19.3312 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "8/8 - 4s - 558ms/step - loss: 19.6872 - val_loss: 17.8563 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "8/8 - 5s - 590ms/step - loss: 18.5945 - val_loss: 17.0341 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "8/8 - 4s - 529ms/step - loss: 17.2898 - val_loss: 15.5729 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "8/8 - 4s - 547ms/step - loss: 16.4197 - val_loss: 15.0106 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "8/8 - 5s - 591ms/step - loss: 15.1860 - val_loss: 13.8050 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "8/8 - 5s - 677ms/step - loss: 14.4454 - val_loss: 13.5525 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "8/8 - 5s - 642ms/step - loss: 13.7550 - val_loss: 12.4515 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "8/8 - 5s - 634ms/step - loss: 13.1940 - val_loss: 11.8911 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "8/8 - 5s - 615ms/step - loss: 12.5152 - val_loss: 11.3739 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "8/8 - 5s - 665ms/step - loss: 12.1129 - val_loss: 11.1202 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "8/8 - 5s - 652ms/step - loss: 11.6668 - val_loss: 10.5020 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "8/8 - 5s - 652ms/step - loss: 11.1413 - val_loss: 10.1829 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "8/8 - 5s - 655ms/step - loss: 10.7995 - val_loss: 9.7644 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "8/8 - 5s - 651ms/step - loss: 10.3161 - val_loss: 9.3821 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "8/8 - 5s - 610ms/step - loss: 10.0444 - val_loss: 8.9563 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "8/8 - 5s - 611ms/step - loss: 9.8165 - val_loss: 8.6938 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "8/8 - 5s - 645ms/step - loss: 9.3786 - val_loss: 8.4920 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "8/8 - 5s - 646ms/step - loss: 9.1951 - val_loss: 8.1371 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "8/8 - 5s - 618ms/step - loss: 8.9392 - val_loss: 8.1478 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "8/8 - 6s - 709ms/step - loss: 8.7659 - val_loss: 7.7064 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "8/8 - 5s - 621ms/step - loss: 8.7643 - val_loss: 7.8155 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "8/8 - 5s - 686ms/step - loss: 8.3771 - val_loss: 7.3147 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "8/8 - 6s - 741ms/step - loss: 8.1374 - val_loss: 7.2525 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "8/8 - 6s - 765ms/step - loss: 8.0401 - val_loss: 6.9806 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "8/8 - 6s - 770ms/step - loss: 7.7362 - val_loss: 6.8419 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "8/8 - 6s - 770ms/step - loss: 7.5711 - val_loss: 6.6360 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "8/8 - 6s - 769ms/step - loss: 7.4582 - val_loss: 6.6692 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "8/8 - 6s - 787ms/step - loss: 7.3232 - val_loss: 6.4266 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "8/8 - 7s - 847ms/step - loss: 7.2944 - val_loss: 6.7036 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "8/8 - 6s - 694ms/step - loss: 7.2562 - val_loss: 6.1147 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "8/8 - 5s - 671ms/step - loss: 7.0062 - val_loss: 6.0579 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "8/8 - 6s - 729ms/step - loss: 6.8384 - val_loss: 5.9369 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "8/8 - 5s - 623ms/step - loss: 6.6760 - val_loss: 5.8665 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "8/8 - 5s - 626ms/step - loss: 6.6058 - val_loss: 5.8456 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "8/8 - 5s - 582ms/step - loss: 6.5515 - val_loss: 5.6382 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "8/8 - 5s - 597ms/step - loss: 6.4535 - val_loss: 5.5719 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "8/8 - 6s - 691ms/step - loss: 6.4517 - val_loss: 5.4343 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "8/8 - 5s - 611ms/step - loss: 6.4254 - val_loss: 5.6162 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "8/8 - 5s - 621ms/step - loss: 6.3257 - val_loss: 5.2899 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "8/8 - 5s - 585ms/step - loss: 5.9882 - val_loss: 5.5486 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "8/8 - 5s - 618ms/step - loss: 6.0973 - val_loss: 5.1524 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "8/8 - 5s - 609ms/step - loss: 5.9917 - val_loss: 5.1453 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "8/8 - 5s - 620ms/step - loss: 5.8301 - val_loss: 5.0286 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "8/8 - 6s - 720ms/step - loss: 5.9086 - val_loss: 5.0757 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "8/8 - 6s - 740ms/step - loss: 5.8011 - val_loss: 5.0537 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "8/8 - 5s - 629ms/step - loss: 5.7916 - val_loss: 4.8605 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "8/8 - 5s - 680ms/step - loss: 5.6512 - val_loss: 4.8117 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "8/8 - 5s - 629ms/step - loss: 5.7246 - val_loss: 5.0782 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "8/8 - 5s - 608ms/step - loss: 5.6390 - val_loss: 4.7102 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "8/8 - 6s - 690ms/step - loss: 5.5676 - val_loss: 4.7519 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "8/8 - 5s - 620ms/step - loss: 5.5365 - val_loss: 4.6563 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "8/8 - 5s - 643ms/step - loss: 5.4336 - val_loss: 4.6536 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "8/8 - 5s - 643ms/step - loss: 5.4691 - val_loss: 4.6255 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "8/8 - 5s - 621ms/step - loss: 5.4225 - val_loss: 4.5687 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "8/8 - 5s - 655ms/step - loss: 5.3676 - val_loss: 4.5484 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "8/8 - 5s - 598ms/step - loss: 5.2091 - val_loss: 4.4717 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "8/8 - 5s - 613ms/step - loss: 5.3390 - val_loss: 4.4639 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "8/8 - 5s - 615ms/step - loss: 5.5645 - val_loss: 4.4071 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "8/8 - 6s - 692ms/step - loss: 5.3028 - val_loss: 4.4208 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "8/8 - 7s - 859ms/step - loss: 5.2329 - val_loss: 4.5270 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "8/8 - 5s - 653ms/step - loss: 5.1272 - val_loss: 4.2946 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "8/8 - 6s - 689ms/step - loss: 5.2814 - val_loss: 4.3202 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "8/8 - 5s - 614ms/step - loss: 5.2936 - val_loss: 4.2754 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "8/8 - 5s - 617ms/step - loss: 5.0562 - val_loss: 4.2711 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "8/8 - 5s - 589ms/step - loss: 5.0772 - val_loss: 4.2120 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "8/8 - 5s - 629ms/step - loss: 4.9891 - val_loss: 4.2587 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "8/8 - 5s - 593ms/step - loss: 5.0671 - val_loss: 4.2315 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "8/8 - 5s - 636ms/step - loss: 4.9284 - val_loss: 4.1100 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "8/8 - 5s - 669ms/step - loss: 5.5078 - val_loss: 4.1233 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "8/8 - 5s - 608ms/step - loss: 5.3364 - val_loss: 4.2052 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "8/8 - 5s - 588ms/step - loss: 5.2667 - val_loss: 4.1831 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "8/8 - 5s - 601ms/step - loss: 4.8906 - val_loss: 4.6626 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "8/8 - 5s - 582ms/step - loss: 4.9874 - val_loss: 4.0450 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "8/8 - 5s - 596ms/step - loss: 5.4050 - val_loss: 4.0340 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "8/8 - 5s - 599ms/step - loss: 4.9207 - val_loss: 4.3823 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "8/8 - 5s - 576ms/step - loss: 4.9283 - val_loss: 4.0055 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "8/8 - 5s - 643ms/step - loss: 4.8744 - val_loss: 4.1365 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "8/8 - 5s - 591ms/step - loss: 4.8273 - val_loss: 3.9646 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "8/8 - 5s - 614ms/step - loss: 4.7314 - val_loss: 4.0813 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "8/8 - 6s - 700ms/step - loss: 4.7790 - val_loss: 3.9221 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "8/8 - 5s - 687ms/step - loss: 4.7075 - val_loss: 4.0025 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "8/8 - 5s - 623ms/step - loss: 4.6992 - val_loss: 3.9495 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "8/8 - 5s - 588ms/step - loss: 4.6784 - val_loss: 3.8849 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "8/8 - 5s - 598ms/step - loss: 4.7238 - val_loss: 3.8779 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "8/8 - 5s - 592ms/step - loss: 4.7197 - val_loss: 3.9653 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "8/8 - 5s - 624ms/step - loss: 4.6170 - val_loss: 3.8365 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "8/8 - 5s - 601ms/step - loss: 4.6615 - val_loss: 3.9538 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "8/8 - 5s - 599ms/step - loss: 4.7059 - val_loss: 3.8200 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 31s - 1s/step - loss: 224.5757 - val_loss: 108.4395 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "24/24 - 15s - 609ms/step - loss: 72.0190 - val_loss: 38.2026 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "24/24 - 15s - 613ms/step - loss: 33.4071 - val_loss: 20.6699 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "24/24 - 14s - 592ms/step - loss: 22.5935 - val_loss: 14.2927 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "24/24 - 16s - 675ms/step - loss: 18.0864 - val_loss: 11.1862 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "24/24 - 17s - 696ms/step - loss: 14.9670 - val_loss: 8.8159 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "24/24 - 17s - 710ms/step - loss: 13.1015 - val_loss: 7.7601 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "24/24 - 17s - 703ms/step - loss: 12.0463 - val_loss: 6.4308 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "24/24 - 18s - 761ms/step - loss: 10.9095 - val_loss: 5.4857 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "24/24 - 16s - 658ms/step - loss: 10.2059 - val_loss: 5.1905 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "24/24 - 16s - 652ms/step - loss: 9.7795 - val_loss: 4.5851 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "24/24 - 16s - 663ms/step - loss: 9.2530 - val_loss: 3.9820 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "24/24 - 16s - 668ms/step - loss: 8.9773 - val_loss: 4.0443 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "24/24 - 17s - 728ms/step - loss: 8.7367 - val_loss: 3.5071 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "24/24 - 18s - 736ms/step - loss: 8.5830 - val_loss: 3.3443 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "24/24 - 17s - 710ms/step - loss: 8.3819 - val_loss: 3.2138 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "24/24 - 16s - 683ms/step - loss: 8.1958 - val_loss: 3.0457 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "24/24 - 17s - 695ms/step - loss: 8.0913 - val_loss: 2.9318 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "24/24 - 17s - 713ms/step - loss: 7.9901 - val_loss: 2.9181 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "24/24 - 19s - 804ms/step - loss: 7.8803 - val_loss: 2.7993 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "24/24 - 16s - 657ms/step - loss: 7.7987 - val_loss: 2.9117 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "24/24 - 20s - 847ms/step - loss: 7.7470 - val_loss: 2.6915 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "24/24 - 18s - 770ms/step - loss: 7.7461 - val_loss: 2.7691 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "24/24 - 16s - 687ms/step - loss: 7.7104 - val_loss: 2.7168 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "24/24 - 17s - 706ms/step - loss: 7.6290 - val_loss: 2.6305 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "24/24 - 18s - 753ms/step - loss: 7.6578 - val_loss: 2.7752 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "24/24 - 19s - 774ms/step - loss: 7.5923 - val_loss: 2.6341 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "24/24 - 18s - 742ms/step - loss: 7.5699 - val_loss: 2.5816 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "24/24 - 19s - 796ms/step - loss: 7.5594 - val_loss: 2.5775 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "24/24 - 18s - 758ms/step - loss: 7.5278 - val_loss: 2.5818 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "24/24 - 16s - 674ms/step - loss: 7.5138 - val_loss: 2.5685 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "24/24 - 15s - 642ms/step - loss: 7.4646 - val_loss: 2.3817 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "24/24 - 16s - 660ms/step - loss: 7.4910 - val_loss: 2.5055 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "24/24 - 15s - 630ms/step - loss: 7.5151 - val_loss: 2.4096 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "24/24 - 15s - 616ms/step - loss: 7.4656 - val_loss: 2.5619 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "24/24 - 15s - 636ms/step - loss: 7.4538 - val_loss: 2.3864 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "24/24 - 16s - 677ms/step - loss: 7.4472 - val_loss: 2.5608 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "24/24 - 15s - 623ms/step - loss: 7.4537 - val_loss: 2.4427 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "24/24 - 15s - 643ms/step - loss: 7.4260 - val_loss: 2.4498 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "24/24 - 15s - 607ms/step - loss: 7.4415 - val_loss: 2.4270 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "24/24 - 15s - 645ms/step - loss: 7.4149 - val_loss: 2.4582 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "24/24 - 17s - 692ms/step - loss: 7.4282 - val_loss: 2.4377 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "24/24 - 15s - 614ms/step - loss: 7.3948 - val_loss: 2.4261 - learning_rate: 2.0000e-04\n",
      "Epoch 44/100\n",
      "24/24 - 15s - 604ms/step - loss: 7.3939 - val_loss: 2.4047 - learning_rate: 2.0000e-04\n",
      "Epoch 45/100\n",
      "24/24 - 15s - 605ms/step - loss: 7.3958 - val_loss: 2.4192 - learning_rate: 2.0000e-04\n",
      "Epoch 46/100\n",
      "24/24 - 16s - 673ms/step - loss: 7.3919 - val_loss: 2.3973 - learning_rate: 2.0000e-04\n",
      "Epoch 47/100\n",
      "24/24 - 15s - 632ms/step - loss: 7.3952 - val_loss: 2.4203 - learning_rate: 2.0000e-04\n",
      "Epoch 48/100\n",
      "24/24 - 15s - 628ms/step - loss: 7.3869 - val_loss: 2.4209 - learning_rate: 2.0000e-04\n",
      "Epoch 49/100\n",
      "24/24 - 15s - 621ms/step - loss: 7.3964 - val_loss: 2.4129 - learning_rate: 2.0000e-04\n",
      "Epoch 50/100\n",
      "24/24 - 15s - 636ms/step - loss: 7.3932 - val_loss: 2.3889 - learning_rate: 2.0000e-04\n",
      "Epoch 51/100\n",
      "24/24 - 15s - 630ms/step - loss: 7.3823 - val_loss: 2.4169 - learning_rate: 2.0000e-04\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "24/24 - 16s - 681ms/step - loss: 7.3887 - val_loss: 2.4283 - learning_rate: 2.0000e-04\n",
      "Epoch 53/100\n",
      "24/24 - 15s - 642ms/step - loss: 7.3807 - val_loss: 2.4208 - learning_rate: 4.0000e-05\n",
      "Epoch 54/100\n",
      "24/24 - 16s - 657ms/step - loss: 7.3832 - val_loss: 2.4178 - learning_rate: 4.0000e-05\n",
      "Epoch 55/100\n",
      "24/24 - 16s - 646ms/step - loss: 7.3816 - val_loss: 2.4172 - learning_rate: 4.0000e-05\n",
      "Epoch 56/100\n",
      "24/24 - 16s - 664ms/step - loss: 7.3852 - val_loss: 2.4220 - learning_rate: 4.0000e-05\n",
      "Epoch 57/100\n",
      "24/24 - 17s - 701ms/step - loss: 7.3827 - val_loss: 2.4184 - learning_rate: 4.0000e-05\n",
      "Epoch 58/100\n",
      "24/24 - 20s - 836ms/step - loss: 7.3840 - val_loss: 2.4131 - learning_rate: 4.0000e-05\n",
      "Epoch 59/100\n",
      "24/24 - 22s - 933ms/step - loss: 7.3816 - val_loss: 2.4132 - learning_rate: 4.0000e-05\n",
      "Epoch 60/100\n",
      "24/24 - 24s - 989ms/step - loss: 7.3826 - val_loss: 2.4126 - learning_rate: 4.0000e-05\n",
      "Epoch 61/100\n",
      "24/24 - 21s - 854ms/step - loss: 7.3855 - val_loss: 2.4043 - learning_rate: 4.0000e-05\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "24/24 - 20s - 813ms/step - loss: 7.3811 - val_loss: 2.4129 - learning_rate: 4.0000e-05\n",
      "Epoch 63/100\n",
      "24/24 - 17s - 710ms/step - loss: 7.3826 - val_loss: 2.4128 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "24/24 - 18s - 748ms/step - loss: 7.3805 - val_loss: 2.4135 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "24/24 - 16s - 665ms/step - loss: 7.3810 - val_loss: 2.4134 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "24/24 - 18s - 752ms/step - loss: 7.3826 - val_loss: 2.4149 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "24/24 - 16s - 685ms/step - loss: 7.3839 - val_loss: 2.4119 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "24/24 - 17s - 692ms/step - loss: 7.3819 - val_loss: 2.4130 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "24/24 - 16s - 668ms/step - loss: 7.3804 - val_loss: 2.4143 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "24/24 - 16s - 674ms/step - loss: 7.3765 - val_loss: 2.4133 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "24/24 - 15s - 620ms/step - loss: 7.3828 - val_loss: 2.4124 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "24/24 - 16s - 666ms/step - loss: 7.3789 - val_loss: 2.4139 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "24/24 - 17s - 690ms/step - loss: 7.3794 - val_loss: 2.4129 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "24/24 - 16s - 669ms/step - loss: 7.3812 - val_loss: 2.4137 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "24/24 - 15s - 614ms/step - loss: 7.3804 - val_loss: 2.4139 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "24/24 - 15s - 613ms/step - loss: 7.3813 - val_loss: 2.4121 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "24/24 - 14s - 596ms/step - loss: 7.3817 - val_loss: 2.4129 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "24/24 - 14s - 593ms/step - loss: 7.3814 - val_loss: 2.4115 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "24/24 - 15s - 614ms/step - loss: 7.3826 - val_loss: 2.4109 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "24/24 - 15s - 619ms/step - loss: 7.3805 - val_loss: 2.4115 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "24/24 - 16s - 647ms/step - loss: 7.3818 - val_loss: 2.4114 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "24/24 - 17s - 695ms/step - loss: 7.3800 - val_loss: 2.4106 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "24/24 - 17s - 723ms/step - loss: 7.3784 - val_loss: 2.4109 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "24/24 - 15s - 613ms/step - loss: 7.3793 - val_loss: 2.4113 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "24/24 - 14s - 598ms/step - loss: 7.3820 - val_loss: 2.4104 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "24/24 - 14s - 592ms/step - loss: 7.3829 - val_loss: 2.4112 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "24/24 - 14s - 583ms/step - loss: 7.3814 - val_loss: 2.4104 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "24/24 - 14s - 591ms/step - loss: 7.3803 - val_loss: 2.4105 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "24/24 - 14s - 595ms/step - loss: 7.3837 - val_loss: 2.4113 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "24/24 - 14s - 592ms/step - loss: 7.3832 - val_loss: 2.4107 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "24/24 - 14s - 581ms/step - loss: 7.3779 - val_loss: 2.4098 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "24/24 - 14s - 567ms/step - loss: 7.3799 - val_loss: 2.4093 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "24/24 - 14s - 570ms/step - loss: 7.3805 - val_loss: 2.4098 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "24/24 - 14s - 592ms/step - loss: 7.3775 - val_loss: 2.4079 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "24/24 - 14s - 600ms/step - loss: 7.3790 - val_loss: 2.4091 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "24/24 - 14s - 603ms/step - loss: 7.3789 - val_loss: 2.4067 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "24/24 - 14s - 592ms/step - loss: 7.3794 - val_loss: 2.4089 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "24/24 - 14s - 588ms/step - loss: 7.3809 - val_loss: 2.4082 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "24/24 - 14s - 575ms/step - loss: 7.3809 - val_loss: 2.4076 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "24/24 - 15s - 634ms/step - loss: 7.3827 - val_loss: 2.4100 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 4'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 - 59s - 1s/step - loss: 125.3674 - val_loss: 23.5515 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "54/54 - 33s - 603ms/step - loss: 15.3402 - val_loss: 7.1475 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "54/54 - 32s - 594ms/step - loss: 7.6376 - val_loss: 3.4562 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "54/54 - 34s - 624ms/step - loss: 5.2713 - val_loss: 2.0472 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "54/54 - 36s - 662ms/step - loss: 4.3092 - val_loss: 1.4212 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "54/54 - 33s - 612ms/step - loss: 3.8043 - val_loss: 1.0566 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "54/54 - 32s - 598ms/step - loss: 3.5502 - val_loss: 1.0515 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "54/54 - 32s - 594ms/step - loss: 3.4011 - val_loss: 0.8033 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "54/54 - 34s - 628ms/step - loss: 3.3063 - val_loss: 0.7420 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "54/54 - 32s - 596ms/step - loss: 3.2422 - val_loss: 0.7444 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "54/54 - 34s - 634ms/step - loss: 3.2475 - val_loss: 0.6258 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "54/54 - 33s - 608ms/step - loss: 3.1832 - val_loss: 0.8328 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "54/54 - 33s - 604ms/step - loss: 3.1962 - val_loss: 0.5767 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "54/54 - 33s - 602ms/step - loss: 3.1592 - val_loss: 0.5923 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "54/54 - 33s - 608ms/step - loss: 3.1649 - val_loss: 0.6650 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "54/54 - 33s - 610ms/step - loss: 3.1173 - val_loss: 0.5595 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "54/54 - 32s - 591ms/step - loss: 3.1494 - val_loss: 0.6421 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "54/54 - 33s - 607ms/step - loss: 3.1240 - val_loss: 0.5466 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "54/54 - 33s - 602ms/step - loss: 3.1256 - val_loss: 0.5965 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "54/54 - 36s - 658ms/step - loss: 3.1080 - val_loss: 0.5394 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "54/54 - 34s - 621ms/step - loss: 3.1102 - val_loss: 0.5285 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "54/54 - 33s - 611ms/step - loss: 3.1097 - val_loss: 0.5474 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "54/54 - 33s - 611ms/step - loss: 3.1025 - val_loss: 0.5468 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "54/54 - 33s - 603ms/step - loss: 3.0984 - val_loss: 0.5796 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "54/54 - 34s - 633ms/step - loss: 3.0965 - val_loss: 0.5341 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "54/54 - 36s - 661ms/step - loss: 3.0917 - val_loss: 0.6114 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "54/54 - 49s - 902ms/step - loss: 3.1092 - val_loss: 0.5282 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "54/54 - 42s - 778ms/step - loss: 3.1127 - val_loss: 0.5381 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "54/54 - 39s - 730ms/step - loss: 3.1019 - val_loss: 0.5256 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "54/54 - 35s - 651ms/step - loss: 3.0815 - val_loss: 0.5624 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "54/54 - 36s - 671ms/step - loss: 3.0973 - val_loss: 0.5224 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "54/54 - 35s - 647ms/step - loss: 3.1068 - val_loss: 0.5407 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "54/54 - 35s - 640ms/step - loss: 3.1034 - val_loss: 0.5231 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "54/54 - 35s - 648ms/step - loss: 3.0905 - val_loss: 0.5654 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "54/54 - 34s - 639ms/step - loss: 3.0880 - val_loss: 0.5429 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "54/54 - 34s - 637ms/step - loss: 3.0928 - val_loss: 0.6765 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "54/54 - 36s - 661ms/step - loss: 3.1371 - val_loss: 0.5585 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "54/54 - 34s - 631ms/step - loss: 3.1073 - val_loss: 0.5797 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "54/54 - 34s - 627ms/step - loss: 3.1162 - val_loss: 0.5821 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "54/54 - 33s - 608ms/step - loss: 3.0940 - val_loss: 0.6249 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "54/54 - 32s - 595ms/step - loss: 3.1088 - val_loss: 0.5337 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "54/54 - 32s - 596ms/step - loss: 3.0864 - val_loss: 0.5288 - learning_rate: 2.0000e-04\n",
      "Epoch 43/100\n",
      "54/54 - 31s - 583ms/step - loss: 3.0885 - val_loss: 0.5312 - learning_rate: 2.0000e-04\n",
      "Epoch 44/100\n",
      "54/54 - 33s - 603ms/step - loss: 3.0874 - val_loss: 0.5264 - learning_rate: 2.0000e-04\n",
      "Epoch 45/100\n",
      "54/54 - 32s - 600ms/step - loss: 3.0855 - val_loss: 0.5396 - learning_rate: 2.0000e-04\n",
      "Epoch 46/100\n",
      "54/54 - 33s - 616ms/step - loss: 3.0901 - val_loss: 0.5375 - learning_rate: 2.0000e-04\n",
      "Epoch 47/100\n",
      "54/54 - 32s - 599ms/step - loss: 3.0865 - val_loss: 0.5373 - learning_rate: 2.0000e-04\n",
      "Epoch 48/100\n",
      "54/54 - 32s - 593ms/step - loss: 3.0882 - val_loss: 0.5299 - learning_rate: 2.0000e-04\n",
      "Epoch 49/100\n",
      "54/54 - 32s - 583ms/step - loss: 3.0864 - val_loss: 0.5305 - learning_rate: 2.0000e-04\n",
      "Epoch 50/100\n",
      "54/54 - 32s - 589ms/step - loss: 3.0871 - val_loss: 0.5314 - learning_rate: 2.0000e-04\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "54/54 - 33s - 602ms/step - loss: 3.0964 - val_loss: 0.5262 - learning_rate: 2.0000e-04\n",
      "Epoch 52/100\n",
      "54/54 - 33s - 613ms/step - loss: 3.0859 - val_loss: 0.5287 - learning_rate: 4.0000e-05\n",
      "Epoch 53/100\n",
      "54/54 - 32s - 591ms/step - loss: 3.0850 - val_loss: 0.5289 - learning_rate: 4.0000e-05\n",
      "Epoch 54/100\n",
      "54/54 - 32s - 585ms/step - loss: 3.0850 - val_loss: 0.5288 - learning_rate: 4.0000e-05\n",
      "Epoch 55/100\n",
      "54/54 - 34s - 628ms/step - loss: 3.0849 - val_loss: 0.5330 - learning_rate: 4.0000e-05\n",
      "Epoch 56/100\n",
      "54/54 - 36s - 671ms/step - loss: 3.0846 - val_loss: 0.5323 - learning_rate: 4.0000e-05\n",
      "Epoch 57/100\n",
      "54/54 - 33s - 617ms/step - loss: 3.0842 - val_loss: 0.5331 - learning_rate: 4.0000e-05\n",
      "Epoch 58/100\n",
      "54/54 - 32s - 599ms/step - loss: 3.0843 - val_loss: 0.5327 - learning_rate: 4.0000e-05\n",
      "Epoch 59/100\n",
      "54/54 - 33s - 619ms/step - loss: 3.0843 - val_loss: 0.5336 - learning_rate: 4.0000e-05\n",
      "Epoch 60/100\n",
      "54/54 - 32s - 597ms/step - loss: 3.0854 - val_loss: 0.5327 - learning_rate: 4.0000e-05\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "54/54 - 33s - 614ms/step - loss: 3.0841 - val_loss: 0.5341 - learning_rate: 4.0000e-05\n",
      "Epoch 62/100\n",
      "54/54 - 35s - 643ms/step - loss: 3.0839 - val_loss: 0.5340 - learning_rate: 1.0000e-05\n",
      "Epoch 63/100\n",
      "54/54 - 40s - 738ms/step - loss: 3.0841 - val_loss: 0.5341 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "54/54 - 39s - 725ms/step - loss: 3.0841 - val_loss: 0.5342 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "54/54 - 48s - 885ms/step - loss: 3.0840 - val_loss: 0.5336 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "54/54 - 38s - 696ms/step - loss: 3.0840 - val_loss: 0.5339 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "54/54 - 37s - 681ms/step - loss: 3.0839 - val_loss: 0.5336 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "54/54 - 41s - 766ms/step - loss: 3.0839 - val_loss: 0.5333 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "54/54 - 43s - 791ms/step - loss: 3.0843 - val_loss: 0.5337 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "54/54 - 36s - 660ms/step - loss: 3.0839 - val_loss: 0.5334 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "54/54 - 32s - 599ms/step - loss: 3.0840 - val_loss: 0.5334 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "54/54 - 32s - 591ms/step - loss: 3.0839 - val_loss: 0.5335 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "54/54 - 33s - 613ms/step - loss: 3.0841 - val_loss: 0.5338 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "54/54 - 32s - 589ms/step - loss: 3.0844 - val_loss: 0.5331 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "54/54 - 32s - 600ms/step - loss: 3.0839 - val_loss: 0.5337 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "54/54 - 32s - 598ms/step - loss: 3.0839 - val_loss: 0.5335 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "54/54 - 32s - 599ms/step - loss: 3.0840 - val_loss: 0.5338 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "54/54 - 31s - 575ms/step - loss: 3.0843 - val_loss: 0.5339 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "54/54 - 31s - 582ms/step - loss: 3.0839 - val_loss: 0.5333 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "54/54 - 32s - 589ms/step - loss: 3.0840 - val_loss: 0.5330 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "54/54 - 32s - 590ms/step - loss: 3.0841 - val_loss: 0.5341 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "54/54 - 31s - 582ms/step - loss: 3.0839 - val_loss: 0.5332 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "54/54 - 33s - 617ms/step - loss: 3.0839 - val_loss: 0.5336 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "54/54 - 31s - 576ms/step - loss: 3.0840 - val_loss: 0.5333 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "54/54 - 40s - 744ms/step - loss: 3.0841 - val_loss: 0.5335 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "54/54 - 40s - 742ms/step - loss: 3.0839 - val_loss: 0.5334 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "54/54 - 32s - 595ms/step - loss: 3.0840 - val_loss: 0.5334 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "54/54 - 32s - 593ms/step - loss: 3.0841 - val_loss: 0.5332 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "54/54 - 31s - 577ms/step - loss: 3.0843 - val_loss: 0.5340 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "54/54 - 31s - 579ms/step - loss: 3.0841 - val_loss: 0.5330 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "54/54 - 32s - 592ms/step - loss: 3.0842 - val_loss: 0.5334 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "54/54 - 33s - 606ms/step - loss: 3.0840 - val_loss: 0.5337 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "54/54 - 31s - 578ms/step - loss: 3.0840 - val_loss: 0.5328 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "54/54 - 31s - 575ms/step - loss: 3.0840 - val_loss: 0.5336 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "54/54 - 32s - 589ms/step - loss: 3.0842 - val_loss: 0.5331 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "54/54 - 32s - 593ms/step - loss: 3.0839 - val_loss: 0.5340 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "54/54 - 32s - 590ms/step - loss: 3.0840 - val_loss: 0.5332 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "54/54 - 31s - 573ms/step - loss: 3.0839 - val_loss: 0.5335 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "54/54 - 32s - 597ms/step - loss: 3.0842 - val_loss: 0.5341 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "54/54 - 31s - 583ms/step - loss: 3.0840 - val_loss: 0.5333 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 5'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 - 109s - 11s/step - loss: 320.3672 - val_loss: 251.5865 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "10/10 - 6s - 565ms/step - loss: 210.5238 - val_loss: 164.6131 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "10/10 - 5s - 542ms/step - loss: 142.2829 - val_loss: 113.9457 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "10/10 - 5s - 538ms/step - loss: 101.0589 - val_loss: 83.5769 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "10/10 - 5s - 520ms/step - loss: 76.0635 - val_loss: 66.9178 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "10/10 - 5s - 533ms/step - loss: 61.6419 - val_loss: 54.9026 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "10/10 - 5s - 511ms/step - loss: 52.0469 - val_loss: 47.1993 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "10/10 - 5s - 509ms/step - loss: 45.4901 - val_loss: 41.2924 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "10/10 - 5s - 505ms/step - loss: 40.8867 - val_loss: 37.4160 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "10/10 - 5s - 513ms/step - loss: 36.9080 - val_loss: 33.8822 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "10/10 - 5s - 520ms/step - loss: 33.6583 - val_loss: 32.5339 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "10/10 - 5s - 503ms/step - loss: 32.9210 - val_loss: 32.7573 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "10/10 - 5s - 502ms/step - loss: 31.1280 - val_loss: 27.3573 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "10/10 - 5s - 505ms/step - loss: 27.8497 - val_loss: 25.8383 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "10/10 - 5s - 504ms/step - loss: 26.4557 - val_loss: 24.8539 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "10/10 - 5s - 503ms/step - loss: 25.3024 - val_loss: 23.3566 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "10/10 - 5s - 520ms/step - loss: 24.1441 - val_loss: 22.4398 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "10/10 - 5s - 542ms/step - loss: 23.0920 - val_loss: 21.4607 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "10/10 - 5s - 533ms/step - loss: 22.4185 - val_loss: 21.4312 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "10/10 - 5s - 524ms/step - loss: 21.7142 - val_loss: 20.3300 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "10/10 - 5s - 522ms/step - loss: 20.9596 - val_loss: 19.6283 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "10/10 - 5s - 528ms/step - loss: 20.3753 - val_loss: 19.8184 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "10/10 - 5s - 526ms/step - loss: 20.4797 - val_loss: 18.9597 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "10/10 - 5s - 522ms/step - loss: 19.3158 - val_loss: 17.9317 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "10/10 - 5s - 514ms/step - loss: 18.7798 - val_loss: 17.6963 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "10/10 - 6s - 555ms/step - loss: 18.2763 - val_loss: 16.8893 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "10/10 - 5s - 514ms/step - loss: 17.9510 - val_loss: 16.5031 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "10/10 - 7s - 734ms/step - loss: 17.5381 - val_loss: 16.1593 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "10/10 - 9s - 851ms/step - loss: 17.8205 - val_loss: 15.9941 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "10/10 - 6s - 624ms/step - loss: 17.0087 - val_loss: 15.5545 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "10/10 - 7s - 721ms/step - loss: 16.6099 - val_loss: 15.2631 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "10/10 - 6s - 594ms/step - loss: 16.6818 - val_loss: 15.0026 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "10/10 - 6s - 577ms/step - loss: 16.0468 - val_loss: 14.9597 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "10/10 - 6s - 611ms/step - loss: 15.9856 - val_loss: 14.6005 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "10/10 - 6s - 584ms/step - loss: 15.6116 - val_loss: 14.4035 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "10/10 - 8s - 786ms/step - loss: 15.4437 - val_loss: 14.1655 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "10/10 - 8s - 784ms/step - loss: 15.3233 - val_loss: 13.9496 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "10/10 - 8s - 828ms/step - loss: 15.0881 - val_loss: 13.8467 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "10/10 - 10s - 1s/step - loss: 14.9395 - val_loss: 13.6538 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "10/10 - 9s - 892ms/step - loss: 14.6595 - val_loss: 13.4877 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "10/10 - 6s - 580ms/step - loss: 14.5727 - val_loss: 13.3169 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "10/10 - 5s - 541ms/step - loss: 14.2949 - val_loss: 13.3647 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "10/10 - 5s - 538ms/step - loss: 14.3303 - val_loss: 13.0462 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "10/10 - 6s - 591ms/step - loss: 14.2316 - val_loss: 12.9500 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "10/10 - 6s - 649ms/step - loss: 14.0624 - val_loss: 12.8237 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "10/10 - 6s - 624ms/step - loss: 13.8173 - val_loss: 12.7950 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "10/10 - 7s - 685ms/step - loss: 13.8896 - val_loss: 12.7437 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "10/10 - 7s - 655ms/step - loss: 13.6563 - val_loss: 12.5032 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "10/10 - 6s - 646ms/step - loss: 13.5329 - val_loss: 12.4622 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "10/10 - 6s - 600ms/step - loss: 13.6280 - val_loss: 12.3244 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "10/10 - 7s - 669ms/step - loss: 13.4662 - val_loss: 12.4232 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "10/10 - 6s - 629ms/step - loss: 13.4773 - val_loss: 12.1689 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "10/10 - 6s - 604ms/step - loss: 13.1159 - val_loss: 12.3115 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "10/10 - 6s - 601ms/step - loss: 13.2111 - val_loss: 12.0415 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "10/10 - 6s - 634ms/step - loss: 13.3384 - val_loss: 11.9359 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "10/10 - 7s - 670ms/step - loss: 12.9432 - val_loss: 12.0411 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "10/10 - 8s - 805ms/step - loss: 12.8947 - val_loss: 11.8325 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "10/10 - 9s - 894ms/step - loss: 12.9048 - val_loss: 11.7658 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "10/10 - 8s - 848ms/step - loss: 12.8422 - val_loss: 11.7931 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "10/10 - 9s - 922ms/step - loss: 12.8357 - val_loss: 11.6966 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "10/10 - 9s - 927ms/step - loss: 12.7552 - val_loss: 11.6137 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "10/10 - 10s - 959ms/step - loss: 12.8220 - val_loss: 11.6160 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "10/10 - 7s - 704ms/step - loss: 12.6908 - val_loss: 11.4894 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "10/10 - 7s - 664ms/step - loss: 12.5851 - val_loss: 11.5515 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "10/10 - 6s - 635ms/step - loss: 12.4891 - val_loss: 11.3991 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "10/10 - 6s - 616ms/step - loss: 12.5513 - val_loss: 11.5766 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "10/10 - 6s - 648ms/step - loss: 12.5498 - val_loss: 11.3088 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "10/10 - 7s - 667ms/step - loss: 12.3487 - val_loss: 11.3947 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "10/10 - 6s - 606ms/step - loss: 12.3173 - val_loss: 11.2530 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "10/10 - 7s - 652ms/step - loss: 12.3784 - val_loss: 11.1976 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "10/10 - 6s - 568ms/step - loss: 12.3331 - val_loss: 11.2556 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "10/10 - 7s - 674ms/step - loss: 12.2248 - val_loss: 11.1453 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "10/10 - 8s - 810ms/step - loss: 12.1409 - val_loss: 11.1661 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "10/10 - 7s - 684ms/step - loss: 12.2745 - val_loss: 11.0779 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "10/10 - 9s - 854ms/step - loss: 12.1534 - val_loss: 11.0756 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "10/10 - 9s - 941ms/step - loss: 12.1807 - val_loss: 11.0319 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "10/10 - 10s - 960ms/step - loss: 12.0802 - val_loss: 11.0555 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "10/10 - 9s - 928ms/step - loss: 12.0745 - val_loss: 10.9892 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "10/10 - 7s - 707ms/step - loss: 12.0634 - val_loss: 10.9859 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "10/10 - 7s - 692ms/step - loss: 12.0946 - val_loss: 10.9105 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "10/10 - 7s - 679ms/step - loss: 12.0292 - val_loss: 11.0173 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "10/10 - 6s - 648ms/step - loss: 11.9660 - val_loss: 10.8927 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "10/10 - 8s - 826ms/step - loss: 11.8983 - val_loss: 10.8432 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "10/10 - 7s - 654ms/step - loss: 11.9081 - val_loss: 10.8916 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "10/10 - 6s - 626ms/step - loss: 11.8813 - val_loss: 10.8838 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "10/10 - 7s - 740ms/step - loss: 11.8708 - val_loss: 10.7937 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "10/10 - 8s - 846ms/step - loss: 11.8254 - val_loss: 10.8008 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "10/10 - 10s - 970ms/step - loss: 11.8400 - val_loss: 10.7620 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "10/10 - 10s - 991ms/step - loss: 11.8430 - val_loss: 10.7278 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "10/10 - 10s - 1s/step - loss: 12.0795 - val_loss: 10.7877 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "10/10 - 8s - 850ms/step - loss: 11.8083 - val_loss: 10.6966 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "10/10 - 6s - 646ms/step - loss: 11.7426 - val_loss: 10.7842 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "10/10 - 6s - 635ms/step - loss: 11.8139 - val_loss: 10.6727 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "10/10 - 6s - 642ms/step - loss: 11.8906 - val_loss: 10.9499 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "10/10 - 7s - 651ms/step - loss: 11.6817 - val_loss: 10.6370 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "10/10 - 7s - 655ms/step - loss: 11.7523 - val_loss: 10.6956 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "10/10 - 7s - 660ms/step - loss: 11.6860 - val_loss: 10.6304 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "10/10 - 6s - 626ms/step - loss: 11.7229 - val_loss: 10.6435 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "10/10 - 7s - 689ms/step - loss: 11.6665 - val_loss: 10.6008 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "10/10 - 9s - 903ms/step - loss: 11.6442 - val_loss: 10.6260 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 6'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/21 - 33s - 2s/step - loss: 230.1975 - val_loss: 116.6685 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "21/21 - 12s - 581ms/step - loss: 73.1435 - val_loss: 38.1904 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "21/21 - 13s - 608ms/step - loss: 28.0126 - val_loss: 17.9857 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "21/21 - 12s - 572ms/step - loss: 15.9659 - val_loss: 11.6826 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "21/21 - 12s - 572ms/step - loss: 11.5151 - val_loss: 8.7563 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "21/21 - 12s - 590ms/step - loss: 9.3382 - val_loss: 7.1463 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "21/21 - 14s - 646ms/step - loss: 8.0432 - val_loss: 6.1599 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "21/21 - 13s - 602ms/step - loss: 7.2541 - val_loss: 5.5291 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "21/21 - 13s - 643ms/step - loss: 6.6896 - val_loss: 5.0842 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "21/21 - 13s - 633ms/step - loss: 6.3188 - val_loss: 4.8181 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "21/21 - 13s - 635ms/step - loss: 6.0712 - val_loss: 4.6095 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "21/21 - 15s - 731ms/step - loss: 5.9184 - val_loss: 4.4863 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "21/21 - 13s - 635ms/step - loss: 5.8099 - val_loss: 4.3911 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "21/21 - 14s - 667ms/step - loss: 5.7395 - val_loss: 4.3576 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "21/21 - 11s - 547ms/step - loss: 5.7098 - val_loss: 4.3122 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "21/21 - 14s - 654ms/step - loss: 5.6786 - val_loss: 4.2615 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "21/21 - 13s - 604ms/step - loss: 5.6464 - val_loss: 4.2482 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "21/21 - 13s - 638ms/step - loss: 5.5941 - val_loss: 4.2206 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "21/21 - 13s - 620ms/step - loss: 5.5849 - val_loss: 4.2049 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "21/21 - 13s - 619ms/step - loss: 5.5783 - val_loss: 4.1959 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "21/21 - 13s - 602ms/step - loss: 5.5734 - val_loss: 4.1831 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "21/21 - 13s - 636ms/step - loss: 5.5537 - val_loss: 4.1755 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "21/21 - 15s - 732ms/step - loss: 5.5446 - val_loss: 4.1803 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "21/21 - 14s - 685ms/step - loss: 5.5509 - val_loss: 4.1685 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "21/21 - 13s - 630ms/step - loss: 5.5461 - val_loss: 4.1769 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "21/21 - 12s - 564ms/step - loss: 5.5387 - val_loss: 4.1637 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "21/21 - 12s - 572ms/step - loss: 5.5382 - val_loss: 4.1706 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "21/21 - 12s - 553ms/step - loss: 5.5328 - val_loss: 4.1558 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "21/21 - 12s - 553ms/step - loss: 5.5295 - val_loss: 4.1576 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "21/21 - 12s - 566ms/step - loss: 5.5473 - val_loss: 4.1874 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "21/21 - 12s - 564ms/step - loss: 5.5302 - val_loss: 4.1525 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "21/21 - 12s - 573ms/step - loss: 5.5182 - val_loss: 4.1609 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "21/21 - 12s - 564ms/step - loss: 5.5882 - val_loss: 4.1992 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "21/21 - 11s - 539ms/step - loss: 5.5618 - val_loss: 4.1524 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "21/21 - 12s - 570ms/step - loss: 5.5572 - val_loss: 4.2056 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "21/21 - 13s - 621ms/step - loss: 5.5243 - val_loss: 4.1503 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "21/21 - 11s - 545ms/step - loss: 5.5243 - val_loss: 4.1544 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "21/21 - 12s - 552ms/step - loss: 5.5393 - val_loss: 4.1537 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "21/21 - 11s - 543ms/step - loss: 5.5229 - val_loss: 4.1557 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "21/21 - 11s - 539ms/step - loss: 5.5247 - val_loss: 4.1511 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "21/21 - 11s - 540ms/step - loss: 5.5207 - val_loss: 4.1592 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "21/21 - 11s - 534ms/step - loss: 5.5419 - val_loss: 4.1544 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "21/21 - 12s - 552ms/step - loss: 5.5206 - val_loss: 4.1644 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "21/21 - 12s - 556ms/step - loss: 5.5281 - val_loss: 4.1542 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "21/21 - 12s - 571ms/step - loss: 5.5225 - val_loss: 4.1488 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "21/21 - 12s - 562ms/step - loss: 5.5162 - val_loss: 4.1629 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "21/21 - 12s - 576ms/step - loss: 5.5322 - val_loss: 4.1512 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "21/21 - 12s - 590ms/step - loss: 5.5436 - val_loss: 4.1583 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "21/21 - 12s - 578ms/step - loss: 5.5448 - val_loss: 4.1540 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "21/21 - 13s - 620ms/step - loss: 5.5186 - val_loss: 4.1534 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "21/21 - 15s - 702ms/step - loss: 5.5384 - val_loss: 4.1562 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "21/21 - 13s - 618ms/step - loss: 5.5245 - val_loss: 4.1489 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "21/21 - 13s - 613ms/step - loss: 5.5220 - val_loss: 4.1653 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "21/21 - 14s - 677ms/step - loss: 5.5238 - val_loss: 4.1602 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 - 18s - 878ms/step - loss: 5.5202 - val_loss: 4.1658 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "21/21 - 18s - 878ms/step - loss: 5.5227 - val_loss: 4.1635 - learning_rate: 2.0000e-04\n",
      "Epoch 57/100\n",
      "21/21 - 16s - 773ms/step - loss: 5.5164 - val_loss: 4.1599 - learning_rate: 2.0000e-04\n",
      "Epoch 58/100\n",
      "21/21 - 17s - 795ms/step - loss: 5.5159 - val_loss: 4.1588 - learning_rate: 2.0000e-04\n",
      "Epoch 59/100\n",
      "21/21 - 17s - 811ms/step - loss: 5.5153 - val_loss: 4.1555 - learning_rate: 2.0000e-04\n",
      "Epoch 60/100\n",
      "21/21 - 14s - 667ms/step - loss: 5.5182 - val_loss: 4.1547 - learning_rate: 2.0000e-04\n",
      "Epoch 61/100\n",
      "21/21 - 13s - 608ms/step - loss: 5.5163 - val_loss: 4.1519 - learning_rate: 2.0000e-04\n",
      "Epoch 62/100\n",
      "21/21 - 13s - 617ms/step - loss: 5.5203 - val_loss: 4.1589 - learning_rate: 2.0000e-04\n",
      "Epoch 63/100\n",
      "21/21 - 13s - 611ms/step - loss: 5.5150 - val_loss: 4.1552 - learning_rate: 2.0000e-04\n",
      "Epoch 64/100\n",
      "21/21 - 13s - 632ms/step - loss: 5.5159 - val_loss: 4.1517 - learning_rate: 2.0000e-04\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 - 13s - 623ms/step - loss: 5.5202 - val_loss: 4.1579 - learning_rate: 2.0000e-04\n",
      "Epoch 66/100\n",
      "21/21 - 12s - 594ms/step - loss: 5.5148 - val_loss: 4.1568 - learning_rate: 4.0000e-05\n",
      "Epoch 67/100\n",
      "21/21 - 13s - 605ms/step - loss: 5.5148 - val_loss: 4.1569 - learning_rate: 4.0000e-05\n",
      "Epoch 68/100\n",
      "21/21 - 13s - 600ms/step - loss: 5.5150 - val_loss: 4.1567 - learning_rate: 4.0000e-05\n",
      "Epoch 69/100\n",
      "21/21 - 13s - 603ms/step - loss: 5.5154 - val_loss: 4.1550 - learning_rate: 4.0000e-05\n",
      "Epoch 70/100\n",
      "21/21 - 13s - 617ms/step - loss: 5.5155 - val_loss: 4.1545 - learning_rate: 4.0000e-05\n",
      "Epoch 71/100\n",
      "21/21 - 13s - 633ms/step - loss: 5.5158 - val_loss: 4.1540 - learning_rate: 4.0000e-05\n",
      "Epoch 72/100\n",
      "21/21 - 13s - 624ms/step - loss: 5.5141 - val_loss: 4.1546 - learning_rate: 4.0000e-05\n",
      "Epoch 73/100\n",
      "21/21 - 13s - 622ms/step - loss: 5.5145 - val_loss: 4.1554 - learning_rate: 4.0000e-05\n",
      "Epoch 74/100\n",
      "21/21 - 13s - 625ms/step - loss: 5.5147 - val_loss: 4.1546 - learning_rate: 4.0000e-05\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 - 14s - 666ms/step - loss: 5.5145 - val_loss: 4.1546 - learning_rate: 4.0000e-05\n",
      "Epoch 76/100\n",
      "21/21 - 13s - 623ms/step - loss: 5.5143 - val_loss: 4.1548 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "21/21 - 13s - 623ms/step - loss: 5.5147 - val_loss: 4.1549 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "21/21 - 13s - 619ms/step - loss: 5.5145 - val_loss: 4.1546 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "21/21 - 13s - 599ms/step - loss: 5.5145 - val_loss: 4.1547 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "21/21 - 13s - 605ms/step - loss: 5.5143 - val_loss: 4.1546 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "21/21 - 13s - 637ms/step - loss: 5.5143 - val_loss: 4.1545 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "21/21 - 13s - 614ms/step - loss: 5.5142 - val_loss: 4.1546 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "21/21 - 15s - 711ms/step - loss: 5.5143 - val_loss: 4.1547 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "21/21 - 13s - 624ms/step - loss: 5.5143 - val_loss: 4.1547 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "21/21 - 13s - 626ms/step - loss: 5.5143 - val_loss: 4.1546 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "21/21 - 13s - 602ms/step - loss: 5.5143 - val_loss: 4.1548 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "21/21 - 13s - 621ms/step - loss: 5.5143 - val_loss: 4.1547 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "21/21 - 13s - 606ms/step - loss: 5.5143 - val_loss: 4.1547 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "21/21 - 13s - 626ms/step - loss: 5.5142 - val_loss: 4.1548 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "21/21 - 14s - 657ms/step - loss: 5.5143 - val_loss: 4.1547 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "21/21 - 14s - 666ms/step - loss: 5.5143 - val_loss: 4.1548 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "21/21 - 13s - 606ms/step - loss: 5.5142 - val_loss: 4.1548 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "21/21 - 13s - 624ms/step - loss: 5.5144 - val_loss: 4.1549 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "21/21 - 13s - 598ms/step - loss: 5.5142 - val_loss: 4.1549 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "21/21 - 13s - 614ms/step - loss: 5.5144 - val_loss: 4.1549 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "21/21 - 13s - 603ms/step - loss: 5.5142 - val_loss: 4.1547 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "21/21 - 13s - 628ms/step - loss: 5.5143 - val_loss: 4.1548 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "21/21 - 13s - 640ms/step - loss: 5.5142 - val_loss: 4.1545 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "21/21 - 13s - 631ms/step - loss: 5.5142 - val_loss: 4.1545 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "21/21 - 13s - 612ms/step - loss: 5.5143 - val_loss: 4.1546 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 7'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 - 38s - 3s/step - loss: 312.1459 - val_loss: 204.6212 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "13/13 - 7s - 564ms/step - loss: 168.9151 - val_loss: 119.1957 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "13/13 - 7s - 570ms/step - loss: 98.8668 - val_loss: 72.1789 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "13/13 - 7s - 560ms/step - loss: 65.8781 - val_loss: 51.1796 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "13/13 - 9s - 712ms/step - loss: 50.1786 - val_loss: 40.6333 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "13/13 - 8s - 639ms/step - loss: 41.7219 - val_loss: 34.4962 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "13/13 - 7s - 571ms/step - loss: 36.4625 - val_loss: 30.4918 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "13/13 - 7s - 560ms/step - loss: 33.6188 - val_loss: 27.7227 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "13/13 - 7s - 568ms/step - loss: 30.9559 - val_loss: 25.5009 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "13/13 - 7s - 559ms/step - loss: 28.7232 - val_loss: 23.7109 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "13/13 - 8s - 589ms/step - loss: 27.8538 - val_loss: 22.2849 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "13/13 - 7s - 567ms/step - loss: 25.6568 - val_loss: 21.6970 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "13/13 - 7s - 556ms/step - loss: 25.2075 - val_loss: 20.0933 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "13/13 - 7s - 565ms/step - loss: 23.6480 - val_loss: 19.2486 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "13/13 - 7s - 562ms/step - loss: 23.2863 - val_loss: 18.3681 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "13/13 - 8s - 584ms/step - loss: 22.7302 - val_loss: 17.7919 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "13/13 - 8s - 614ms/step - loss: 21.3162 - val_loss: 17.0392 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "13/13 - 8s - 619ms/step - loss: 21.6481 - val_loss: 16.4639 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "13/13 - 8s - 625ms/step - loss: 20.4466 - val_loss: 16.4020 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "13/13 - 8s - 645ms/step - loss: 20.4499 - val_loss: 15.4854 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "13/13 - 9s - 673ms/step - loss: 19.1509 - val_loss: 15.3160 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "13/13 - 8s - 638ms/step - loss: 18.9673 - val_loss: 15.1614 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "13/13 - 8s - 615ms/step - loss: 18.7879 - val_loss: 15.1638 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "13/13 - 8s - 609ms/step - loss: 18.1184 - val_loss: 14.0166 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "13/13 - 8s - 610ms/step - loss: 18.0554 - val_loss: 13.6773 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "13/13 - 8s - 622ms/step - loss: 17.4679 - val_loss: 13.3025 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "13/13 - 8s - 626ms/step - loss: 17.3353 - val_loss: 13.1408 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "13/13 - 8s - 616ms/step - loss: 16.8243 - val_loss: 12.7214 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "13/13 - 8s - 601ms/step - loss: 16.6627 - val_loss: 12.5721 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "13/13 - 8s - 589ms/step - loss: 16.2772 - val_loss: 12.2733 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "13/13 - 8s - 611ms/step - loss: 16.1770 - val_loss: 12.0827 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "13/13 - 8s - 618ms/step - loss: 15.9593 - val_loss: 11.7849 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "13/13 - 8s - 615ms/step - loss: 15.7518 - val_loss: 12.4901 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "13/13 - 8s - 615ms/step - loss: 15.4194 - val_loss: 11.5042 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "13/13 - 8s - 609ms/step - loss: 15.6636 - val_loss: 11.9061 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "13/13 - 8s - 612ms/step - loss: 16.1041 - val_loss: 11.0235 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "13/13 - 8s - 601ms/step - loss: 14.7409 - val_loss: 11.0201 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "13/13 - 8s - 607ms/step - loss: 14.8922 - val_loss: 10.6689 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "13/13 - 8s - 591ms/step - loss: 14.5002 - val_loss: 10.5201 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "13/13 - 8s - 606ms/step - loss: 14.2723 - val_loss: 10.7579 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "13/13 - 8s - 591ms/step - loss: 14.2546 - val_loss: 10.8307 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "13/13 - 8s - 591ms/step - loss: 14.4827 - val_loss: 11.1119 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "13/13 - 8s - 628ms/step - loss: 14.4816 - val_loss: 10.0067 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "13/13 - 8s - 636ms/step - loss: 13.9061 - val_loss: 9.8615 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "13/13 - 10s - 762ms/step - loss: 13.7499 - val_loss: 9.9021 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "13/13 - 8s - 600ms/step - loss: 13.8440 - val_loss: 10.0220 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "13/13 - 8s - 601ms/step - loss: 14.0687 - val_loss: 9.5357 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "13/13 - 8s - 600ms/step - loss: 13.7191 - val_loss: 9.5679 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "13/13 - 8s - 603ms/step - loss: 13.3654 - val_loss: 9.8806 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "13/13 - 8s - 596ms/step - loss: 13.2498 - val_loss: 9.1994 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "13/13 - 8s - 596ms/step - loss: 13.0771 - val_loss: 9.1625 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "13/13 - 8s - 607ms/step - loss: 13.6291 - val_loss: 9.7875 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "13/13 - 8s - 608ms/step - loss: 13.1283 - val_loss: 8.9279 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "13/13 - 8s - 608ms/step - loss: 12.6829 - val_loss: 8.8959 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "13/13 - 8s - 588ms/step - loss: 12.6943 - val_loss: 8.7488 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "13/13 - 8s - 586ms/step - loss: 12.5666 - val_loss: 8.6645 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "13/13 - 8s - 585ms/step - loss: 12.7179 - val_loss: 8.6585 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "13/13 - 8s - 606ms/step - loss: 12.3766 - val_loss: 8.5317 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "13/13 - 8s - 594ms/step - loss: 12.2595 - val_loss: 8.5969 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "13/13 - 8s - 603ms/step - loss: 12.3423 - val_loss: 8.4575 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "13/13 - 8s - 626ms/step - loss: 12.3044 - val_loss: 8.3161 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "13/13 - 8s - 595ms/step - loss: 12.3085 - val_loss: 8.2470 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "13/13 - 8s - 606ms/step - loss: 12.3102 - val_loss: 8.1847 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "13/13 - 8s - 598ms/step - loss: 11.9594 - val_loss: 8.2045 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "13/13 - 8s - 581ms/step - loss: 11.7979 - val_loss: 8.0669 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "13/13 - 8s - 603ms/step - loss: 11.9373 - val_loss: 8.3960 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "13/13 - 8s - 608ms/step - loss: 11.7953 - val_loss: 7.9843 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "13/13 - 7s - 569ms/step - loss: 12.0628 - val_loss: 7.9301 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "13/13 - 7s - 548ms/step - loss: 12.0604 - val_loss: 8.1614 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "13/13 - 7s - 556ms/step - loss: 11.8317 - val_loss: 7.8221 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "13/13 - 7s - 549ms/step - loss: 11.6183 - val_loss: 7.9316 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "13/13 - 7s - 574ms/step - loss: 11.4116 - val_loss: 7.7210 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "13/13 - 7s - 543ms/step - loss: 11.8469 - val_loss: 7.6823 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "13/13 - 7s - 557ms/step - loss: 11.5457 - val_loss: 7.6607 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "13/13 - 7s - 552ms/step - loss: 11.4925 - val_loss: 7.7075 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "13/13 - 7s - 545ms/step - loss: 11.3823 - val_loss: 7.5872 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "13/13 - 7s - 559ms/step - loss: 11.7247 - val_loss: 7.5394 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "13/13 - 7s - 542ms/step - loss: 11.4200 - val_loss: 7.6158 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "13/13 - 7s - 531ms/step - loss: 11.3403 - val_loss: 7.4496 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "13/13 - 7s - 535ms/step - loss: 11.4698 - val_loss: 7.4204 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "13/13 - 7s - 552ms/step - loss: 11.2072 - val_loss: 7.4413 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "13/13 - 7s - 546ms/step - loss: 11.1698 - val_loss: 7.3782 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "13/13 - 7s - 554ms/step - loss: 11.2788 - val_loss: 7.3306 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "13/13 - 7s - 553ms/step - loss: 11.4805 - val_loss: 7.4593 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "13/13 - 7s - 563ms/step - loss: 11.1367 - val_loss: 7.2588 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "13/13 - 9s - 659ms/step - loss: 11.0666 - val_loss: 7.2577 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "13/13 - 7s - 560ms/step - loss: 11.0101 - val_loss: 7.2962 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "13/13 - 7s - 545ms/step - loss: 11.2615 - val_loss: 7.1792 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "13/13 - 7s - 538ms/step - loss: 11.0120 - val_loss: 7.1932 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "13/13 - 7s - 564ms/step - loss: 11.0695 - val_loss: 7.3384 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "13/13 - 7s - 554ms/step - loss: 11.0948 - val_loss: 7.0936 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "13/13 - 7s - 543ms/step - loss: 10.8090 - val_loss: 7.4456 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "13/13 - 7s - 529ms/step - loss: 10.8830 - val_loss: 7.0471 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "13/13 - 7s - 533ms/step - loss: 10.8465 - val_loss: 7.2102 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "13/13 - 7s - 525ms/step - loss: 10.8828 - val_loss: 7.0010 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "13/13 - 7s - 544ms/step - loss: 10.8546 - val_loss: 7.0222 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "13/13 - 7s - 547ms/step - loss: 11.0161 - val_loss: 7.0730 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "13/13 - 7s - 541ms/step - loss: 11.5008 - val_loss: 7.0963 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "13/13 - 7s - 544ms/step - loss: 10.7237 - val_loss: 6.9569 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "13/13 - 7s - 543ms/step - loss: 10.7958 - val_loss: 7.1686 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 8'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 - 37s - 1s/step - loss: 212.4025 - val_loss: 100.2751 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "29/29 - 16s - 535ms/step - loss: 65.0197 - val_loss: 39.6344 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "29/29 - 16s - 539ms/step - loss: 32.4028 - val_loss: 23.6674 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "29/29 - 16s - 541ms/step - loss: 22.0276 - val_loss: 17.2225 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "29/29 - 15s - 534ms/step - loss: 16.7533 - val_loss: 13.0388 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "29/29 - 16s - 542ms/step - loss: 13.5539 - val_loss: 10.4791 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "29/29 - 17s - 569ms/step - loss: 11.3028 - val_loss: 8.6032 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "29/29 - 15s - 533ms/step - loss: 9.7536 - val_loss: 7.2018 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "29/29 - 15s - 532ms/step - loss: 8.6720 - val_loss: 6.2292 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "29/29 - 16s - 554ms/step - loss: 7.4865 - val_loss: 5.5502 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "29/29 - 19s - 656ms/step - loss: 6.8589 - val_loss: 4.7030 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "29/29 - 17s - 602ms/step - loss: 6.2426 - val_loss: 4.1451 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "29/29 - 18s - 615ms/step - loss: 5.7038 - val_loss: 3.6879 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "29/29 - 19s - 666ms/step - loss: 5.4181 - val_loss: 3.4670 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "29/29 - 18s - 632ms/step - loss: 4.9515 - val_loss: 3.0790 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "29/29 - 17s - 591ms/step - loss: 4.7272 - val_loss: 3.0588 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "29/29 - 19s - 640ms/step - loss: 4.5180 - val_loss: 2.6590 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "29/29 - 17s - 603ms/step - loss: 4.2557 - val_loss: 2.4183 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "29/29 - 17s - 579ms/step - loss: 4.1229 - val_loss: 2.3884 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "29/29 - 16s - 569ms/step - loss: 4.0120 - val_loss: 2.2086 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "29/29 - 16s - 550ms/step - loss: 3.8314 - val_loss: 2.0684 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "29/29 - 16s - 555ms/step - loss: 3.7599 - val_loss: 2.0022 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "29/29 - 15s - 522ms/step - loss: 3.6475 - val_loss: 2.1496 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "29/29 - 15s - 526ms/step - loss: 3.6063 - val_loss: 1.8153 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "29/29 - 16s - 535ms/step - loss: 3.6491 - val_loss: 1.7567 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "29/29 - 16s - 557ms/step - loss: 3.4581 - val_loss: 1.7270 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "29/29 - 17s - 570ms/step - loss: 3.4543 - val_loss: 1.6925 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "29/29 - 16s - 568ms/step - loss: 3.3747 - val_loss: 1.6974 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "29/29 - 16s - 561ms/step - loss: 3.3474 - val_loss: 1.5866 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "29/29 - 18s - 609ms/step - loss: 3.3377 - val_loss: 1.6103 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "29/29 - 16s - 538ms/step - loss: 3.2980 - val_loss: 1.5304 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "29/29 - 15s - 518ms/step - loss: 3.2333 - val_loss: 1.5464 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "29/29 - 15s - 522ms/step - loss: 3.2172 - val_loss: 1.4838 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "29/29 - 16s - 539ms/step - loss: 3.1805 - val_loss: 1.4775 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "29/29 - 16s - 560ms/step - loss: 3.2072 - val_loss: 1.4615 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "29/29 - 16s - 546ms/step - loss: 3.2006 - val_loss: 1.4455 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "29/29 - 16s - 535ms/step - loss: 3.1587 - val_loss: 1.4588 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "29/29 - 15s - 523ms/step - loss: 3.1228 - val_loss: 1.4776 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "29/29 - 16s - 537ms/step - loss: 3.1349 - val_loss: 1.3954 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "29/29 - 15s - 530ms/step - loss: 3.1076 - val_loss: 1.3867 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "29/29 - 16s - 538ms/step - loss: 3.0871 - val_loss: 1.4283 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "29/29 - 15s - 523ms/step - loss: 3.0913 - val_loss: 1.4276 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "29/29 - 15s - 524ms/step - loss: 3.0646 - val_loss: 1.5392 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "29/29 - 16s - 541ms/step - loss: 3.0994 - val_loss: 1.4942 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "29/29 - 16s - 537ms/step - loss: 3.0612 - val_loss: 1.3509 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "29/29 - 17s - 599ms/step - loss: 3.0577 - val_loss: 1.3739 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "29/29 - 16s - 562ms/step - loss: 3.0530 - val_loss: 1.3861 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "29/29 - 16s - 553ms/step - loss: 3.0497 - val_loss: 1.3392 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "29/29 - 18s - 627ms/step - loss: 3.0568 - val_loss: 1.3277 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "29/29 - 17s - 595ms/step - loss: 3.0276 - val_loss: 1.3887 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "29/29 - 17s - 593ms/step - loss: 3.0226 - val_loss: 1.3555 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "29/29 - 17s - 577ms/step - loss: 3.0455 - val_loss: 1.3145 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "29/29 - 17s - 583ms/step - loss: 3.0251 - val_loss: 1.3221 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "29/29 - 17s - 583ms/step - loss: 3.0252 - val_loss: 1.3122 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "29/29 - 16s - 561ms/step - loss: 3.0149 - val_loss: 1.3676 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "29/29 - 17s - 592ms/step - loss: 2.9988 - val_loss: 1.4979 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "29/29 - 17s - 584ms/step - loss: 3.0550 - val_loss: 1.4978 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "29/29 - 16s - 559ms/step - loss: 3.0164 - val_loss: 1.3367 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "29/29 - 16s - 555ms/step - loss: 3.0164 - val_loss: 1.3204 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "29/29 - 16s - 545ms/step - loss: 3.0076 - val_loss: 1.3209 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "29/29 - 16s - 558ms/step - loss: 3.0086 - val_loss: 1.3700 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "29/29 - 17s - 577ms/step - loss: 3.0120 - val_loss: 1.3964 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "29/29 - 16s - 566ms/step - loss: 3.0028 - val_loss: 1.3576 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "29/29 - 17s - 580ms/step - loss: 2.9964 - val_loss: 1.3030 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "29/29 - 16s - 558ms/step - loss: 3.0260 - val_loss: 1.3203 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "29/29 - 16s - 560ms/step - loss: 2.9998 - val_loss: 1.3041 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "29/29 - 16s - 569ms/step - loss: 3.0278 - val_loss: 1.3003 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "29/29 - 19s - 648ms/step - loss: 3.0441 - val_loss: 1.2953 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "29/29 - 17s - 586ms/step - loss: 3.0560 - val_loss: 1.3088 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "29/29 - 20s - 700ms/step - loss: 2.9929 - val_loss: 1.4986 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "29/29 - 20s - 679ms/step - loss: 3.0151 - val_loss: 1.3190 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "29/29 - 19s - 654ms/step - loss: 3.0013 - val_loss: 1.3375 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "29/29 - 19s - 646ms/step - loss: 2.9852 - val_loss: 1.2993 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "29/29 - 17s - 589ms/step - loss: 3.0178 - val_loss: 1.2832 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "29/29 - 17s - 600ms/step - loss: 2.9807 - val_loss: 1.4959 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "29/29 - 17s - 573ms/step - loss: 3.0023 - val_loss: 1.2911 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "29/29 - 16s - 548ms/step - loss: 3.0017 - val_loss: 1.2914 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "29/29 - 17s - 573ms/step - loss: 2.9986 - val_loss: 1.2815 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "29/29 - 17s - 574ms/step - loss: 2.9879 - val_loss: 1.3843 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "29/29 - 17s - 573ms/step - loss: 3.0039 - val_loss: 1.2908 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "29/29 - 15s - 530ms/step - loss: 3.0030 - val_loss: 1.2923 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "29/29 - 17s - 580ms/step - loss: 2.9940 - val_loss: 1.3199 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "29/29 - 18s - 618ms/step - loss: 2.9918 - val_loss: 1.2865 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "29/29 - 19s - 647ms/step - loss: 3.0278 - val_loss: 1.2887 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "29/29 - 20s - 675ms/step - loss: 2.9835 - val_loss: 1.3734 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "29/29 - 17s - 600ms/step - loss: 2.9978 - val_loss: 1.3485 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "29/29 - 17s - 582ms/step - loss: 2.9893 - val_loss: 1.3527 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "29/29 - 21s - 741ms/step - loss: 2.9866 - val_loss: 1.3285 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "29/29 - 16s - 548ms/step - loss: 2.9802 - val_loss: 1.3054 - learning_rate: 2.0000e-04\n",
      "Epoch 90/100\n",
      "29/29 - 16s - 542ms/step - loss: 2.9791 - val_loss: 1.3194 - learning_rate: 2.0000e-04\n",
      "Epoch 91/100\n",
      "29/29 - 15s - 534ms/step - loss: 2.9840 - val_loss: 1.2991 - learning_rate: 2.0000e-04\n",
      "Epoch 92/100\n",
      "29/29 - 16s - 561ms/step - loss: 2.9791 - val_loss: 1.3086 - learning_rate: 2.0000e-04\n",
      "Epoch 93/100\n",
      "29/29 - 16s - 539ms/step - loss: 2.9780 - val_loss: 1.3220 - learning_rate: 2.0000e-04\n",
      "Epoch 94/100\n",
      "29/29 - 17s - 575ms/step - loss: 2.9746 - val_loss: 1.3013 - learning_rate: 2.0000e-04\n",
      "Epoch 95/100\n",
      "29/29 - 17s - 594ms/step - loss: 2.9786 - val_loss: 1.2971 - learning_rate: 2.0000e-04\n",
      "Epoch 96/100\n",
      "29/29 - 18s - 605ms/step - loss: 2.9747 - val_loss: 1.3151 - learning_rate: 2.0000e-04\n",
      "Epoch 97/100\n",
      "29/29 - 18s - 618ms/step - loss: 2.9783 - val_loss: 1.3029 - learning_rate: 2.0000e-04\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "29/29 - 17s - 585ms/step - loss: 2.9776 - val_loss: 1.3156 - learning_rate: 2.0000e-04\n",
      "Epoch 99/100\n",
      "29/29 - 18s - 620ms/step - loss: 2.9753 - val_loss: 1.3152 - learning_rate: 4.0000e-05\n",
      "Epoch 100/100\n",
      "29/29 - 18s - 604ms/step - loss: 2.9753 - val_loss: 1.3107 - learning_rate: 4.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 - 49s - 1s/step - loss: 170.7174 - val_loss: 57.3646 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "40/40 - 21s - 536ms/step - loss: 35.7593 - val_loss: 20.4789 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "40/40 - 22s - 553ms/step - loss: 17.2286 - val_loss: 12.0108 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "40/40 - 22s - 558ms/step - loss: 11.4891 - val_loss: 8.1408 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "40/40 - 22s - 539ms/step - loss: 8.6123 - val_loss: 6.0139 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "40/40 - 22s - 542ms/step - loss: 6.9542 - val_loss: 4.7378 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "40/40 - 24s - 598ms/step - loss: 5.8844 - val_loss: 4.0666 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "40/40 - 24s - 603ms/step - loss: 5.1846 - val_loss: 3.3126 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "40/40 - 25s - 613ms/step - loss: 4.6587 - val_loss: 2.9070 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "40/40 - 23s - 583ms/step - loss: 4.3178 - val_loss: 2.5866 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "40/40 - 24s - 589ms/step - loss: 4.0888 - val_loss: 2.4668 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "40/40 - 24s - 598ms/step - loss: 3.9166 - val_loss: 2.2621 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "40/40 - 24s - 600ms/step - loss: 3.7848 - val_loss: 2.1517 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "40/40 - 24s - 596ms/step - loss: 3.6899 - val_loss: 2.0511 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "40/40 - 25s - 615ms/step - loss: 3.6302 - val_loss: 2.0833 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "40/40 - 25s - 622ms/step - loss: 3.5800 - val_loss: 1.9840 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "40/40 - 26s - 650ms/step - loss: 3.5363 - val_loss: 1.9235 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "40/40 - 25s - 618ms/step - loss: 3.4950 - val_loss: 1.9027 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "40/40 - 26s - 638ms/step - loss: 3.4674 - val_loss: 1.9113 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "40/40 - 24s - 596ms/step - loss: 3.4481 - val_loss: 1.8481 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "40/40 - 24s - 595ms/step - loss: 3.4391 - val_loss: 1.9030 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "40/40 - 23s - 583ms/step - loss: 3.4232 - val_loss: 1.9441 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "40/40 - 22s - 560ms/step - loss: 3.4441 - val_loss: 1.8343 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "40/40 - 24s - 605ms/step - loss: 3.4010 - val_loss: 1.8161 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "40/40 - 24s - 594ms/step - loss: 3.4145 - val_loss: 1.8069 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "40/40 - 22s - 559ms/step - loss: 3.4023 - val_loss: 1.8189 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "40/40 - 27s - 665ms/step - loss: 3.4272 - val_loss: 1.8061 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "40/40 - 25s - 632ms/step - loss: 3.4184 - val_loss: 1.8341 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "40/40 - 25s - 626ms/step - loss: 3.3816 - val_loss: 1.7947 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "40/40 - 24s - 602ms/step - loss: 3.3850 - val_loss: 1.8085 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "40/40 - 24s - 606ms/step - loss: 3.4000 - val_loss: 1.8616 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "40/40 - 24s - 597ms/step - loss: 3.3922 - val_loss: 1.7919 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "40/40 - 22s - 554ms/step - loss: 3.3830 - val_loss: 1.8359 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "40/40 - 24s - 602ms/step - loss: 3.3870 - val_loss: 1.7953 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "40/40 - 24s - 600ms/step - loss: 3.3827 - val_loss: 1.7935 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "40/40 - 25s - 633ms/step - loss: 3.3829 - val_loss: 1.8470 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "40/40 - 25s - 619ms/step - loss: 3.3812 - val_loss: 1.8018 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "40/40 - 25s - 621ms/step - loss: 3.3883 - val_loss: 1.8185 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "40/40 - 24s - 604ms/step - loss: 3.3772 - val_loss: 1.7896 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "40/40 - 25s - 625ms/step - loss: 3.3780 - val_loss: 1.7927 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "40/40 - 24s - 605ms/step - loss: 3.3768 - val_loss: 1.8606 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "40/40 - 24s - 593ms/step - loss: 3.3755 - val_loss: 1.7951 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "40/40 - 24s - 591ms/step - loss: 3.3945 - val_loss: 1.8246 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "40/40 - 25s - 624ms/step - loss: 3.3791 - val_loss: 1.8456 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "40/40 - 25s - 618ms/step - loss: 3.3784 - val_loss: 1.7900 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "40/40 - 25s - 619ms/step - loss: 3.3973 - val_loss: 1.7863 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "40/40 - 23s - 563ms/step - loss: 3.3923 - val_loss: 1.7925 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "40/40 - 24s - 588ms/step - loss: 3.3966 - val_loss: 1.7880 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "40/40 - 23s - 586ms/step - loss: 3.3990 - val_loss: 1.7883 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "40/40 - 41s - 1s/step - loss: 3.3785 - val_loss: 1.8079 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "40/40 - 21s - 535ms/step - loss: 3.3782 - val_loss: 1.8531 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "40/40 - 26s - 643ms/step - loss: 3.3816 - val_loss: 1.8123 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "40/40 - 24s - 589ms/step - loss: 3.3945 - val_loss: 1.8195 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "40/40 - 25s - 625ms/step - loss: 3.3733 - val_loss: 1.8496 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "40/40 - 25s - 616ms/step - loss: 3.3794 - val_loss: 1.7867 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "40/40 - 25s - 623ms/step - loss: 3.3953 - val_loss: 1.7936 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "40/40 - 24s - 590ms/step - loss: 3.3737 - val_loss: 1.7887 - learning_rate: 2.0000e-04\n",
      "Epoch 58/100\n",
      "40/40 - 23s - 573ms/step - loss: 3.3658 - val_loss: 1.8054 - learning_rate: 2.0000e-04\n",
      "Epoch 59/100\n",
      "40/40 - 23s - 576ms/step - loss: 3.3679 - val_loss: 1.8002 - learning_rate: 2.0000e-04\n",
      "Epoch 60/100\n",
      "40/40 - 24s - 597ms/step - loss: 3.3718 - val_loss: 1.7925 - learning_rate: 2.0000e-04\n",
      "Epoch 61/100\n",
      "40/40 - 24s - 611ms/step - loss: 3.3670 - val_loss: 1.8026 - learning_rate: 2.0000e-04\n",
      "Epoch 62/100\n",
      "40/40 - 23s - 577ms/step - loss: 3.3670 - val_loss: 1.7971 - learning_rate: 2.0000e-04\n",
      "Epoch 63/100\n",
      "40/40 - 23s - 580ms/step - loss: 3.3690 - val_loss: 1.7965 - learning_rate: 2.0000e-04\n",
      "Epoch 64/100\n",
      "40/40 - 25s - 617ms/step - loss: 3.3682 - val_loss: 1.7948 - learning_rate: 2.0000e-04\n",
      "Epoch 65/100\n",
      "40/40 - 27s - 663ms/step - loss: 3.3683 - val_loss: 1.7963 - learning_rate: 2.0000e-04\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "40/40 - 24s - 609ms/step - loss: 3.3725 - val_loss: 1.7969 - learning_rate: 2.0000e-04\n",
      "Epoch 67/100\n",
      "40/40 - 22s - 562ms/step - loss: 3.3662 - val_loss: 1.7960 - learning_rate: 4.0000e-05\n",
      "Epoch 68/100\n",
      "40/40 - 23s - 563ms/step - loss: 3.3660 - val_loss: 1.7970 - learning_rate: 4.0000e-05\n",
      "Epoch 69/100\n",
      "40/40 - 24s - 599ms/step - loss: 3.3665 - val_loss: 1.7958 - learning_rate: 4.0000e-05\n",
      "Epoch 70/100\n",
      "40/40 - 23s - 572ms/step - loss: 3.3667 - val_loss: 1.7977 - learning_rate: 4.0000e-05\n",
      "Epoch 71/100\n",
      "40/40 - 24s - 600ms/step - loss: 3.3663 - val_loss: 1.7984 - learning_rate: 4.0000e-05\n",
      "Epoch 72/100\n",
      "40/40 - 23s - 586ms/step - loss: 3.3667 - val_loss: 1.7979 - learning_rate: 4.0000e-05\n",
      "Epoch 73/100\n",
      "40/40 - 23s - 579ms/step - loss: 3.3663 - val_loss: 1.7960 - learning_rate: 4.0000e-05\n",
      "Epoch 74/100\n",
      "40/40 - 23s - 587ms/step - loss: 3.3664 - val_loss: 1.7975 - learning_rate: 4.0000e-05\n",
      "Epoch 75/100\n",
      "40/40 - 23s - 586ms/step - loss: 3.3663 - val_loss: 1.7968 - learning_rate: 4.0000e-05\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "40/40 - 23s - 579ms/step - loss: 3.3663 - val_loss: 1.7972 - learning_rate: 4.0000e-05\n",
      "Epoch 77/100\n",
      "40/40 - 23s - 577ms/step - loss: 3.3661 - val_loss: 1.7968 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "40/40 - 24s - 599ms/step - loss: 3.3659 - val_loss: 1.7968 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "40/40 - 23s - 574ms/step - loss: 3.3662 - val_loss: 1.7964 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "40/40 - 23s - 570ms/step - loss: 3.3660 - val_loss: 1.7961 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "40/40 - 25s - 630ms/step - loss: 3.3658 - val_loss: 1.7969 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "40/40 - 23s - 567ms/step - loss: 3.3660 - val_loss: 1.7971 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "40/40 - 23s - 563ms/step - loss: 3.3661 - val_loss: 1.7970 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "40/40 - 23s - 576ms/step - loss: 3.3659 - val_loss: 1.7968 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "40/40 - 22s - 545ms/step - loss: 3.3660 - val_loss: 1.7970 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "40/40 - 23s - 573ms/step - loss: 3.3659 - val_loss: 1.7966 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "40/40 - 24s - 589ms/step - loss: 3.3660 - val_loss: 1.7959 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "40/40 - 23s - 568ms/step - loss: 3.3660 - val_loss: 1.7962 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "40/40 - 22s - 552ms/step - loss: 3.3662 - val_loss: 1.7965 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "40/40 - 22s - 552ms/step - loss: 3.3660 - val_loss: 1.7963 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "40/40 - 24s - 606ms/step - loss: 3.3661 - val_loss: 1.7967 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "40/40 - 23s - 570ms/step - loss: 3.3660 - val_loss: 1.7967 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "40/40 - 24s - 589ms/step - loss: 3.3660 - val_loss: 1.7968 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "40/40 - 23s - 565ms/step - loss: 3.3660 - val_loss: 1.7964 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "40/40 - 22s - 554ms/step - loss: 3.3661 - val_loss: 1.7967 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "40/40 - 23s - 586ms/step - loss: 3.3661 - val_loss: 1.7957 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "40/40 - 23s - 568ms/step - loss: 3.3660 - val_loss: 1.7956 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "40/40 - 22s - 548ms/step - loss: 3.3660 - val_loss: 1.7957 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "40/40 - 22s - 549ms/step - loss: 3.3662 - val_loss: 1.7965 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "40/40 - 23s - 565ms/step - loss: 3.3659 - val_loss: 1.7963 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 - 35s - 2s/step - loss: 257.4595 - val_loss: 154.9670 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "20/20 - 12s - 578ms/step - loss: 115.0353 - val_loss: 74.6170 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "20/20 - 11s - 566ms/step - loss: 63.5685 - val_loss: 47.1713 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "20/20 - 12s - 624ms/step - loss: 44.2376 - val_loss: 33.9551 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "20/20 - 13s - 655ms/step - loss: 34.8745 - val_loss: 27.1836 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "20/20 - 14s - 696ms/step - loss: 29.2331 - val_loss: 22.8930 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "20/20 - 11s - 565ms/step - loss: 25.7423 - val_loss: 20.2430 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "20/20 - 11s - 557ms/step - loss: 22.9574 - val_loss: 17.4161 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "20/20 - 13s - 626ms/step - loss: 20.8307 - val_loss: 15.7200 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "20/20 - 13s - 644ms/step - loss: 19.0224 - val_loss: 14.1002 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "20/20 - 11s - 541ms/step - loss: 17.8630 - val_loss: 13.0987 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "20/20 - 14s - 704ms/step - loss: 16.6588 - val_loss: 12.1550 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "20/20 - 13s - 662ms/step - loss: 15.6583 - val_loss: 11.0093 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "20/20 - 12s - 619ms/step - loss: 14.8021 - val_loss: 10.8853 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "20/20 - 12s - 622ms/step - loss: 14.2597 - val_loss: 9.6588 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "20/20 - 12s - 584ms/step - loss: 13.5076 - val_loss: 9.3700 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "20/20 - 12s - 619ms/step - loss: 13.1195 - val_loss: 8.6172 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "20/20 - 13s - 649ms/step - loss: 12.6563 - val_loss: 8.1732 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "20/20 - 11s - 562ms/step - loss: 12.3118 - val_loss: 7.8218 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "20/20 - 11s - 569ms/step - loss: 11.7959 - val_loss: 7.4746 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "20/20 - 11s - 560ms/step - loss: 11.5932 - val_loss: 7.2356 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "20/20 - 12s - 609ms/step - loss: 11.2686 - val_loss: 7.3210 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "20/20 - 11s - 560ms/step - loss: 11.2026 - val_loss: 6.6950 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "20/20 - 12s - 612ms/step - loss: 10.7678 - val_loss: 6.5003 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "20/20 - 13s - 645ms/step - loss: 10.6414 - val_loss: 6.3073 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "20/20 - 13s - 646ms/step - loss: 10.3494 - val_loss: 6.1500 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "20/20 - 13s - 628ms/step - loss: 10.2394 - val_loss: 6.0047 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "20/20 - 13s - 640ms/step - loss: 10.0832 - val_loss: 6.0601 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "20/20 - 12s - 615ms/step - loss: 10.0610 - val_loss: 5.8420 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "20/20 - 12s - 602ms/step - loss: 9.8791 - val_loss: 5.6402 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "20/20 - 13s - 664ms/step - loss: 9.7068 - val_loss: 5.5432 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "20/20 - 12s - 593ms/step - loss: 9.7453 - val_loss: 5.6560 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "20/20 - 12s - 579ms/step - loss: 9.7054 - val_loss: 5.3938 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "20/20 - 12s - 614ms/step - loss: 9.5721 - val_loss: 5.3040 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "20/20 - 13s - 655ms/step - loss: 9.4028 - val_loss: 5.2578 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "20/20 - 12s - 611ms/step - loss: 9.3781 - val_loss: 5.1814 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "20/20 - 12s - 588ms/step - loss: 9.2663 - val_loss: 5.1316 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "20/20 - 12s - 610ms/step - loss: 9.1963 - val_loss: 5.0741 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "20/20 - 12s - 613ms/step - loss: 9.1764 - val_loss: 5.0687 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "20/20 - 13s - 658ms/step - loss: 9.1530 - val_loss: 4.9949 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "20/20 - 13s - 665ms/step - loss: 9.1051 - val_loss: 4.9480 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "20/20 - 14s - 681ms/step - loss: 9.0719 - val_loss: 4.9150 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "20/20 - 11s - 563ms/step - loss: 9.0617 - val_loss: 4.8810 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "20/20 - 12s - 609ms/step - loss: 8.9906 - val_loss: 4.8978 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "20/20 - 12s - 577ms/step - loss: 8.9925 - val_loss: 4.8461 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "20/20 - 11s - 572ms/step - loss: 8.9335 - val_loss: 4.8355 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "20/20 - 12s - 611ms/step - loss: 8.9275 - val_loss: 4.7775 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "20/20 - 12s - 599ms/step - loss: 8.9019 - val_loss: 4.7594 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "20/20 - 12s - 608ms/step - loss: 8.9113 - val_loss: 4.7368 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "20/20 - 12s - 596ms/step - loss: 8.8584 - val_loss: 4.7210 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "20/20 - 13s - 632ms/step - loss: 8.8292 - val_loss: 4.7024 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "20/20 - 12s - 605ms/step - loss: 8.9288 - val_loss: 4.7260 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "20/20 - 12s - 580ms/step - loss: 8.7820 - val_loss: 4.6963 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "20/20 - 13s - 627ms/step - loss: 8.8381 - val_loss: 4.6800 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "20/20 - 13s - 639ms/step - loss: 8.7864 - val_loss: 4.6490 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "20/20 - 14s - 718ms/step - loss: 8.7612 - val_loss: 4.6377 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "20/20 - 13s - 629ms/step - loss: 8.7615 - val_loss: 4.6306 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "20/20 - 12s - 600ms/step - loss: 8.7455 - val_loss: 4.6222 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "20/20 - 12s - 617ms/step - loss: 8.7479 - val_loss: 4.6161 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "20/20 - 12s - 588ms/step - loss: 8.7448 - val_loss: 4.6013 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "20/20 - 11s - 574ms/step - loss: 8.7063 - val_loss: 4.5994 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "20/20 - 12s - 617ms/step - loss: 8.7143 - val_loss: 4.5859 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "20/20 - 13s - 641ms/step - loss: 8.7185 - val_loss: 4.5797 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "20/20 - 13s - 631ms/step - loss: 8.7336 - val_loss: 4.5769 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "20/20 - 13s - 650ms/step - loss: 8.7032 - val_loss: 4.5850 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "20/20 - 15s - 744ms/step - loss: 8.6874 - val_loss: 4.5686 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "20/20 - 13s - 658ms/step - loss: 8.7218 - val_loss: 4.5597 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "20/20 - 12s - 609ms/step - loss: 8.6806 - val_loss: 4.5805 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "20/20 - 12s - 594ms/step - loss: 8.6677 - val_loss: 4.5482 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "20/20 - 13s - 660ms/step - loss: 8.6892 - val_loss: 4.5442 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "20/20 - 13s - 675ms/step - loss: 8.6823 - val_loss: 4.5403 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "20/20 - 13s - 638ms/step - loss: 8.6725 - val_loss: 4.5613 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "20/20 - 12s - 610ms/step - loss: 8.7153 - val_loss: 4.5407 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "20/20 - 12s - 622ms/step - loss: 8.6368 - val_loss: 4.5564 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "20/20 - 12s - 595ms/step - loss: 8.6703 - val_loss: 4.5275 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "20/20 - 13s - 640ms/step - loss: 8.6441 - val_loss: 4.5254 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "20/20 - 14s - 679ms/step - loss: 8.6620 - val_loss: 4.5224 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "20/20 - 13s - 651ms/step - loss: 8.6439 - val_loss: 4.5297 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "20/20 - 13s - 642ms/step - loss: 8.6455 - val_loss: 4.5172 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "20/20 - 15s - 749ms/step - loss: 8.6594 - val_loss: 4.5223 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "20/20 - 12s - 623ms/step - loss: 8.6641 - val_loss: 4.5208 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "20/20 - 12s - 615ms/step - loss: 8.6495 - val_loss: 4.5233 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "20/20 - 13s - 652ms/step - loss: 8.6274 - val_loss: 4.5097 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "20/20 - 13s - 660ms/step - loss: 8.6645 - val_loss: 4.5120 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "20/20 - 13s - 668ms/step - loss: 8.7133 - val_loss: 4.5221 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "20/20 - 13s - 663ms/step - loss: 8.6172 - val_loss: 4.5420 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "20/20 - 14s - 678ms/step - loss: 8.6246 - val_loss: 4.5040 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "20/20 - 12s - 623ms/step - loss: 8.6468 - val_loss: 4.5076 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "20/20 - 12s - 614ms/step - loss: 8.7112 - val_loss: 4.5018 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "20/20 - 12s - 588ms/step - loss: 8.6510 - val_loss: 4.5082 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "20/20 - 12s - 622ms/step - loss: 8.6340 - val_loss: 4.4991 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "20/20 - 13s - 632ms/step - loss: 8.6192 - val_loss: 4.5093 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "20/20 - 12s - 615ms/step - loss: 8.6161 - val_loss: 4.4969 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "20/20 - 12s - 614ms/step - loss: 8.6212 - val_loss: 4.5008 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "20/20 - 12s - 615ms/step - loss: 8.6443 - val_loss: 4.4956 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "20/20 - 13s - 640ms/step - loss: 8.6391 - val_loss: 4.5023 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "20/20 - 13s - 652ms/step - loss: 8.5963 - val_loss: 4.5137 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "20/20 - 12s - 613ms/step - loss: 8.6147 - val_loss: 4.5035 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "20/20 - 12s - 611ms/step - loss: 8.6224 - val_loss: 4.4944 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "20/20 - 12s - 589ms/step - loss: 8.6234 - val_loss: 4.4926 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 11'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44/44 - 51s - 1s/step - loss: 156.6336 - val_loss: 44.7533 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "44/44 - 25s - 577ms/step - loss: 26.8513 - val_loss: 14.6373 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "44/44 - 26s - 601ms/step - loss: 12.2164 - val_loss: 7.8969 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "44/44 - 27s - 616ms/step - loss: 7.6465 - val_loss: 5.3657 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "44/44 - 28s - 639ms/step - loss: 5.4281 - val_loss: 3.4130 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "44/44 - 30s - 673ms/step - loss: 4.1903 - val_loss: 2.4477 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "44/44 - 28s - 636ms/step - loss: 3.4688 - val_loss: 1.8881 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "44/44 - 27s - 609ms/step - loss: 3.0779 - val_loss: 1.6374 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "44/44 - 28s - 630ms/step - loss: 2.7240 - val_loss: 1.3055 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "44/44 - 31s - 695ms/step - loss: 2.4974 - val_loss: 1.1255 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "44/44 - 29s - 664ms/step - loss: 2.3571 - val_loss: 1.1213 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "44/44 - 32s - 716ms/step - loss: 2.2719 - val_loss: 0.9578 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "44/44 - 29s - 649ms/step - loss: 2.1812 - val_loss: 0.8540 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "44/44 - 29s - 658ms/step - loss: 2.1108 - val_loss: 0.8247 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "44/44 - 29s - 653ms/step - loss: 2.0707 - val_loss: 0.7830 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "44/44 - 27s - 617ms/step - loss: 2.0471 - val_loss: 0.8838 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "44/44 - 28s - 634ms/step - loss: 2.0357 - val_loss: 0.7219 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "44/44 - 28s - 645ms/step - loss: 1.9999 - val_loss: 0.7299 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "44/44 - 27s - 602ms/step - loss: 2.0129 - val_loss: 0.6957 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "44/44 - 25s - 576ms/step - loss: 1.9829 - val_loss: 0.7106 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "44/44 - 29s - 649ms/step - loss: 1.9565 - val_loss: 0.6753 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "44/44 - 30s - 691ms/step - loss: 1.9548 - val_loss: 0.6942 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "44/44 - 30s - 680ms/step - loss: 1.9464 - val_loss: 0.6859 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "44/44 - 28s - 645ms/step - loss: 1.9651 - val_loss: 0.6556 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "44/44 - 26s - 594ms/step - loss: 1.9677 - val_loss: 0.6567 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "44/44 - 25s - 570ms/step - loss: 1.9508 - val_loss: 0.7796 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "44/44 - 28s - 638ms/step - loss: 1.9365 - val_loss: 0.6905 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "44/44 - 27s - 622ms/step - loss: 1.9688 - val_loss: 0.7039 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "44/44 - 28s - 633ms/step - loss: 1.9465 - val_loss: 0.6459 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "44/44 - 29s - 663ms/step - loss: 1.9252 - val_loss: 0.6424 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "44/44 - 28s - 631ms/step - loss: 1.9316 - val_loss: 0.6406 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "44/44 - 29s - 654ms/step - loss: 1.9431 - val_loss: 0.6664 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "44/44 - 29s - 667ms/step - loss: 1.9209 - val_loss: 0.6418 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "44/44 - 34s - 774ms/step - loss: 1.9374 - val_loss: 0.6617 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "44/44 - 28s - 627ms/step - loss: 1.9314 - val_loss: 0.6378 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "44/44 - 28s - 638ms/step - loss: 1.9230 - val_loss: 0.7144 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "44/44 - 29s - 666ms/step - loss: 1.9266 - val_loss: 0.6466 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "44/44 - 28s - 639ms/step - loss: 1.9147 - val_loss: 0.6997 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "44/44 - 44s - 1000ms/step - loss: 1.9110 - val_loss: 0.7077 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "44/44 - 30s - 676ms/step - loss: 1.9219 - val_loss: 0.7062 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "44/44 - 28s - 626ms/step - loss: 1.9196 - val_loss: 0.6437 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "44/44 - 26s - 588ms/step - loss: 1.9201 - val_loss: 0.6376 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "44/44 - 25s - 579ms/step - loss: 1.9297 - val_loss: 0.7870 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "44/44 - 26s - 585ms/step - loss: 1.9609 - val_loss: 0.7023 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "44/44 - 27s - 604ms/step - loss: 1.9200 - val_loss: 0.6663 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "44/44 - 25s - 563ms/step - loss: 1.9144 - val_loss: 0.6435 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "44/44 - 28s - 641ms/step - loss: 1.9149 - val_loss: 0.6457 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "44/44 - 26s - 593ms/step - loss: 1.9311 - val_loss: 0.6352 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "44/44 - 27s - 607ms/step - loss: 1.9101 - val_loss: 0.6987 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "44/44 - 29s - 660ms/step - loss: 1.9413 - val_loss: 0.7526 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "44/44 - 28s - 639ms/step - loss: 1.9145 - val_loss: 0.6617 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "44/44 - 33s - 754ms/step - loss: 1.9216 - val_loss: 0.6636 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "44/44 - 29s - 668ms/step - loss: 1.9155 - val_loss: 0.6437 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "44/44 - 29s - 660ms/step - loss: 1.9297 - val_loss: 0.6362 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "44/44 - 29s - 661ms/step - loss: 1.9340 - val_loss: 0.7065 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "44/44 - 28s - 627ms/step - loss: 1.9289 - val_loss: 0.6387 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "44/44 - 30s - 682ms/step - loss: 1.9214 - val_loss: 0.6349 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "44/44 - 28s - 633ms/step - loss: 1.9464 - val_loss: 0.6515 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "44/44 - 28s - 632ms/step - loss: 1.9313 - val_loss: 0.7027 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "44/44 - 27s - 622ms/step - loss: 1.9135 - val_loss: 0.6563 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "44/44 - 28s - 625ms/step - loss: 1.9355 - val_loss: 0.7569 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "44/44 - 28s - 626ms/step - loss: 1.9180 - val_loss: 0.6368 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "44/44 - 30s - 681ms/step - loss: 1.9213 - val_loss: 0.6471 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "44/44 - 29s - 664ms/step - loss: 1.9199 - val_loss: 0.6889 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "44/44 - 28s - 640ms/step - loss: 1.9164 - val_loss: 0.6534 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "44/44 - 30s - 676ms/step - loss: 1.9163 - val_loss: 0.6392 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "44/44 - 30s - 677ms/step - loss: 1.9176 - val_loss: 0.6379 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "44/44 - 29s - 663ms/step - loss: 1.9131 - val_loss: 0.6596 - learning_rate: 2.0000e-04\n",
      "Epoch 69/100\n",
      "44/44 - 28s - 640ms/step - loss: 1.9158 - val_loss: 0.6461 - learning_rate: 2.0000e-04\n",
      "Epoch 70/100\n",
      "44/44 - 34s - 778ms/step - loss: 1.9120 - val_loss: 0.6550 - learning_rate: 2.0000e-04\n",
      "Epoch 71/100\n",
      "44/44 - 37s - 840ms/step - loss: 1.9147 - val_loss: 0.6622 - learning_rate: 2.0000e-04\n",
      "Epoch 72/100\n",
      "44/44 - 39s - 891ms/step - loss: 1.9098 - val_loss: 0.6579 - learning_rate: 2.0000e-04\n",
      "Epoch 73/100\n",
      "44/44 - 36s - 824ms/step - loss: 1.9105 - val_loss: 0.6583 - learning_rate: 2.0000e-04\n",
      "Epoch 74/100\n",
      "44/44 - 37s - 838ms/step - loss: 1.9105 - val_loss: 0.6570 - learning_rate: 2.0000e-04\n",
      "Epoch 75/100\n",
      "44/44 - 41s - 941ms/step - loss: 1.9092 - val_loss: 0.6555 - learning_rate: 2.0000e-04\n",
      "Epoch 76/100\n",
      "44/44 - 37s - 834ms/step - loss: 1.9101 - val_loss: 0.6509 - learning_rate: 2.0000e-04\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "44/44 - 38s - 858ms/step - loss: 1.9108 - val_loss: 0.6617 - learning_rate: 2.0000e-04\n",
      "Epoch 78/100\n",
      "44/44 - 41s - 935ms/step - loss: 1.9081 - val_loss: 0.6591 - learning_rate: 4.0000e-05\n",
      "Epoch 79/100\n",
      "44/44 - 41s - 925ms/step - loss: 1.9084 - val_loss: 0.6606 - learning_rate: 4.0000e-05\n",
      "Epoch 80/100\n",
      "44/44 - 31s - 704ms/step - loss: 1.9092 - val_loss: 0.6570 - learning_rate: 4.0000e-05\n",
      "Epoch 81/100\n",
      "44/44 - 31s - 709ms/step - loss: 1.9080 - val_loss: 0.6591 - learning_rate: 4.0000e-05\n",
      "Epoch 82/100\n",
      "44/44 - 34s - 768ms/step - loss: 1.9080 - val_loss: 0.6551 - learning_rate: 4.0000e-05\n",
      "Epoch 83/100\n",
      "44/44 - 35s - 792ms/step - loss: 1.9078 - val_loss: 0.6565 - learning_rate: 4.0000e-05\n",
      "Epoch 84/100\n",
      "44/44 - 38s - 864ms/step - loss: 1.9087 - val_loss: 0.6583 - learning_rate: 4.0000e-05\n",
      "Epoch 85/100\n",
      "44/44 - 33s - 755ms/step - loss: 1.9086 - val_loss: 0.6566 - learning_rate: 4.0000e-05\n",
      "Epoch 86/100\n",
      "44/44 - 32s - 716ms/step - loss: 1.9082 - val_loss: 0.6558 - learning_rate: 4.0000e-05\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "44/44 - 34s - 773ms/step - loss: 1.9093 - val_loss: 0.6558 - learning_rate: 4.0000e-05\n",
      "Epoch 88/100\n",
      "44/44 - 33s - 739ms/step - loss: 1.9078 - val_loss: 0.6569 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "44/44 - 33s - 755ms/step - loss: 1.9078 - val_loss: 0.6560 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "44/44 - 35s - 792ms/step - loss: 1.9077 - val_loss: 0.6565 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "44/44 - 31s - 706ms/step - loss: 1.9077 - val_loss: 0.6568 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "44/44 - 32s - 721ms/step - loss: 1.9078 - val_loss: 0.6558 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "44/44 - 34s - 765ms/step - loss: 1.9077 - val_loss: 0.6557 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "44/44 - 34s - 769ms/step - loss: 1.9077 - val_loss: 0.6553 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "44/44 - 31s - 703ms/step - loss: 1.9078 - val_loss: 0.6548 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "44/44 - 30s - 686ms/step - loss: 1.9077 - val_loss: 0.6555 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "44/44 - 31s - 698ms/step - loss: 1.9079 - val_loss: 0.6559 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "44/44 - 34s - 780ms/step - loss: 1.9078 - val_loss: 0.6557 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "44/44 - 32s - 723ms/step - loss: 1.9080 - val_loss: 0.6556 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "44/44 - 32s - 738ms/step - loss: 1.9077 - val_loss: 0.6555 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 12'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 - 86s - 1s/step - loss: 84.4952 - val_loss: 7.8846 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "80/80 - 78s - 970ms/step - loss: 4.5189 - val_loss: 1.4900 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "80/80 - 72s - 898ms/step - loss: 1.6982 - val_loss: 0.4571 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "80/80 - 75s - 932ms/step - loss: 1.1510 - val_loss: 0.2390 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "80/80 - 68s - 847ms/step - loss: 1.0283 - val_loss: 0.1777 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "80/80 - 66s - 823ms/step - loss: 0.9947 - val_loss: 0.2199 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "80/80 - 71s - 889ms/step - loss: 0.9791 - val_loss: 0.1725 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "80/80 - 67s - 842ms/step - loss: 0.9722 - val_loss: 0.1424 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "80/80 - 59s - 733ms/step - loss: 0.9708 - val_loss: 0.2227 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "80/80 - 50s - 623ms/step - loss: 0.9663 - val_loss: 0.1395 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "80/80 - 45s - 561ms/step - loss: 0.9698 - val_loss: 0.1707 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "80/80 - 44s - 553ms/step - loss: 0.9648 - val_loss: 0.1417 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "80/80 - 46s - 573ms/step - loss: 0.9662 - val_loss: 0.1394 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "80/80 - 49s - 616ms/step - loss: 0.9703 - val_loss: 0.1642 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "80/80 - 46s - 572ms/step - loss: 0.9731 - val_loss: 0.1415 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "80/80 - 46s - 573ms/step - loss: 0.9640 - val_loss: 0.1429 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "80/80 - 48s - 603ms/step - loss: 0.9676 - val_loss: 0.1549 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "80/80 - 46s - 578ms/step - loss: 0.9711 - val_loss: 0.1462 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "80/80 - 45s - 564ms/step - loss: 0.9664 - val_loss: 0.1395 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "80/80 - 45s - 568ms/step - loss: 0.9684 - val_loss: 0.1459 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "80/80 - 47s - 582ms/step - loss: 0.9707 - val_loss: 0.1516 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "80/80 - 46s - 578ms/step - loss: 0.9771 - val_loss: 0.1427 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "80/80 - 45s - 567ms/step - loss: 0.9669 - val_loss: 0.1647 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "80/80 - 48s - 597ms/step - loss: 0.9621 - val_loss: 0.1565 - learning_rate: 2.0000e-04\n",
      "Epoch 25/100\n",
      "80/80 - 46s - 579ms/step - loss: 0.9623 - val_loss: 0.1484 - learning_rate: 2.0000e-04\n",
      "Epoch 26/100\n",
      "80/80 - 45s - 559ms/step - loss: 0.9630 - val_loss: 0.1504 - learning_rate: 2.0000e-04\n",
      "Epoch 27/100\n",
      "80/80 - 47s - 582ms/step - loss: 0.9625 - val_loss: 0.1449 - learning_rate: 2.0000e-04\n",
      "Epoch 28/100\n",
      "80/80 - 44s - 556ms/step - loss: 0.9630 - val_loss: 0.1515 - learning_rate: 2.0000e-04\n",
      "Epoch 29/100\n",
      "80/80 - 46s - 571ms/step - loss: 0.9636 - val_loss: 0.1447 - learning_rate: 2.0000e-04\n",
      "Epoch 30/100\n",
      "80/80 - 49s - 610ms/step - loss: 0.9630 - val_loss: 0.1572 - learning_rate: 2.0000e-04\n",
      "Epoch 31/100\n",
      "80/80 - 47s - 590ms/step - loss: 0.9623 - val_loss: 0.1512 - learning_rate: 2.0000e-04\n",
      "Epoch 32/100\n",
      "80/80 - 46s - 574ms/step - loss: 0.9620 - val_loss: 0.1528 - learning_rate: 2.0000e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "80/80 - 47s - 584ms/step - loss: 0.9619 - val_loss: 0.1532 - learning_rate: 2.0000e-04\n",
      "Epoch 34/100\n",
      "80/80 - 43s - 534ms/step - loss: 0.9613 - val_loss: 0.1526 - learning_rate: 4.0000e-05\n",
      "Epoch 35/100\n",
      "80/80 - 44s - 550ms/step - loss: 0.9616 - val_loss: 0.1505 - learning_rate: 4.0000e-05\n",
      "Epoch 36/100\n",
      "80/80 - 43s - 540ms/step - loss: 0.9612 - val_loss: 0.1518 - learning_rate: 4.0000e-05\n",
      "Epoch 37/100\n",
      "80/80 - 45s - 560ms/step - loss: 0.9614 - val_loss: 0.1511 - learning_rate: 4.0000e-05\n",
      "Epoch 38/100\n",
      "80/80 - 44s - 547ms/step - loss: 0.9613 - val_loss: 0.1511 - learning_rate: 4.0000e-05\n",
      "Epoch 38: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 13'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 44s - 1s/step - loss: 179.9214 - val_loss: 66.2080 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "36/36 - 19s - 518ms/step - loss: 42.2788 - val_loss: 25.6367 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "36/36 - 19s - 517ms/step - loss: 22.6824 - val_loss: 16.6287 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "36/36 - 19s - 529ms/step - loss: 16.5864 - val_loss: 12.6417 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "36/36 - 19s - 533ms/step - loss: 13.4380 - val_loss: 10.2270 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "36/36 - 20s - 544ms/step - loss: 11.1862 - val_loss: 8.3741 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "36/36 - 19s - 528ms/step - loss: 9.5316 - val_loss: 6.9766 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "36/36 - 19s - 528ms/step - loss: 8.6086 - val_loss: 6.0117 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "36/36 - 19s - 533ms/step - loss: 7.6831 - val_loss: 5.3815 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "36/36 - 19s - 526ms/step - loss: 7.2109 - val_loss: 4.8410 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "36/36 - 20s - 564ms/step - loss: 6.6932 - val_loss: 4.3835 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "36/36 - 21s - 594ms/step - loss: 6.1734 - val_loss: 4.0279 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "36/36 - 21s - 584ms/step - loss: 5.9401 - val_loss: 3.7592 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "36/36 - 20s - 549ms/step - loss: 5.8059 - val_loss: 3.9921 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "36/36 - 19s - 537ms/step - loss: 5.7276 - val_loss: 3.6485 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "36/36 - 19s - 535ms/step - loss: 5.3648 - val_loss: 3.3506 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "36/36 - 20s - 563ms/step - loss: 5.5034 - val_loss: 3.1634 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "36/36 - 22s - 600ms/step - loss: 5.2577 - val_loss: 3.1023 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "36/36 - 20s - 552ms/step - loss: 5.0728 - val_loss: 3.0019 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "36/36 - 20s - 542ms/step - loss: 5.0719 - val_loss: 3.1371 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "36/36 - 19s - 539ms/step - loss: 4.9234 - val_loss: 2.9490 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "36/36 - 20s - 548ms/step - loss: 4.8948 - val_loss: 2.8920 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "36/36 - 20s - 552ms/step - loss: 4.8940 - val_loss: 2.8443 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "36/36 - 19s - 540ms/step - loss: 4.9067 - val_loss: 2.7420 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "36/36 - 21s - 577ms/step - loss: 4.6933 - val_loss: 2.7102 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "36/36 - 26s - 726ms/step - loss: 4.7244 - val_loss: 2.6770 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "36/36 - 25s - 699ms/step - loss: 4.6841 - val_loss: 2.6541 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "36/36 - 22s - 624ms/step - loss: 4.6417 - val_loss: 2.6988 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "36/36 - 24s - 668ms/step - loss: 4.6117 - val_loss: 2.6138 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "36/36 - 22s - 608ms/step - loss: 4.6212 - val_loss: 2.7258 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "36/36 - 19s - 540ms/step - loss: 4.6161 - val_loss: 2.6422 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "36/36 - 20s - 549ms/step - loss: 4.5782 - val_loss: 2.5751 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "36/36 - 20s - 548ms/step - loss: 4.6057 - val_loss: 2.6074 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "36/36 - 20s - 559ms/step - loss: 4.5483 - val_loss: 2.5507 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "36/36 - 19s - 539ms/step - loss: 4.5651 - val_loss: 2.5716 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "36/36 - 19s - 536ms/step - loss: 4.5415 - val_loss: 2.5432 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "36/36 - 20s - 548ms/step - loss: 4.5314 - val_loss: 2.5569 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "36/36 - 20s - 543ms/step - loss: 4.4950 - val_loss: 2.5196 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "36/36 - 19s - 539ms/step - loss: 4.5151 - val_loss: 2.5212 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "36/36 - 20s - 563ms/step - loss: 4.4911 - val_loss: 2.5097 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "36/36 - 20s - 545ms/step - loss: 4.4702 - val_loss: 2.5281 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "36/36 - 21s - 588ms/step - loss: 4.4943 - val_loss: 2.7604 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "36/36 - 20s - 547ms/step - loss: 4.4574 - val_loss: 2.5183 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "36/36 - 19s - 541ms/step - loss: 4.4725 - val_loss: 2.5035 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "36/36 - 19s - 534ms/step - loss: 4.4800 - val_loss: 2.4991 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "36/36 - 20s - 549ms/step - loss: 4.5478 - val_loss: 2.4813 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "36/36 - 20s - 554ms/step - loss: 4.4520 - val_loss: 2.4922 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "36/36 - 19s - 541ms/step - loss: 4.5166 - val_loss: 2.4820 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "36/36 - 20s - 547ms/step - loss: 4.4486 - val_loss: 2.4785 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "36/36 - 19s - 540ms/step - loss: 4.4715 - val_loss: 2.5423 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "36/36 - 19s - 541ms/step - loss: 4.4577 - val_loss: 2.5808 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "36/36 - 19s - 537ms/step - loss: 4.4579 - val_loss: 2.4915 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "36/36 - 19s - 541ms/step - loss: 4.4389 - val_loss: 2.4603 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "36/36 - 19s - 534ms/step - loss: 4.4720 - val_loss: 2.4636 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "36/36 - 19s - 529ms/step - loss: 4.4584 - val_loss: 2.4580 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "36/36 - 19s - 534ms/step - loss: 4.4478 - val_loss: 2.4619 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "36/36 - 21s - 578ms/step - loss: 4.4465 - val_loss: 2.4579 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "36/36 - 20s - 551ms/step - loss: 4.4231 - val_loss: 2.4525 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "36/36 - 20s - 557ms/step - loss: 4.5834 - val_loss: 2.7301 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "36/36 - 19s - 542ms/step - loss: 4.4546 - val_loss: 2.4982 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "36/36 - 20s - 552ms/step - loss: 4.4257 - val_loss: 2.4503 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "36/36 - 20s - 555ms/step - loss: 4.4331 - val_loss: 2.4476 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "36/36 - 20s - 555ms/step - loss: 4.4348 - val_loss: 2.4539 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "36/36 - 21s - 580ms/step - loss: 4.4249 - val_loss: 2.4537 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "36/36 - 20s - 546ms/step - loss: 4.4451 - val_loss: 2.4772 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "36/36 - 19s - 534ms/step - loss: 4.4260 - val_loss: 2.4434 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "36/36 - 19s - 540ms/step - loss: 4.4157 - val_loss: 2.4714 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "36/36 - 19s - 540ms/step - loss: 4.4073 - val_loss: 2.4491 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "36/36 - 19s - 535ms/step - loss: 4.4089 - val_loss: 2.4463 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "36/36 - 19s - 538ms/step - loss: 4.4395 - val_loss: 2.4473 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "36/36 - 19s - 531ms/step - loss: 4.4335 - val_loss: 2.4393 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "36/36 - 19s - 539ms/step - loss: 4.4295 - val_loss: 2.4429 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "36/36 - 21s - 583ms/step - loss: 4.4182 - val_loss: 2.4604 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "36/36 - 19s - 538ms/step - loss: 4.4248 - val_loss: 2.4493 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "36/36 - 19s - 539ms/step - loss: 4.4225 - val_loss: 2.4370 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "36/36 - 20s - 546ms/step - loss: 4.4180 - val_loss: 2.4374 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "36/36 - 19s - 532ms/step - loss: 4.4384 - val_loss: 2.4416 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "36/36 - 20s - 545ms/step - loss: 4.4322 - val_loss: 2.4538 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "36/36 - 20s - 544ms/step - loss: 4.4139 - val_loss: 2.4728 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "36/36 - 19s - 534ms/step - loss: 4.4050 - val_loss: 2.4348 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "36/36 - 20s - 567ms/step - loss: 4.4339 - val_loss: 2.4368 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "36/36 - 21s - 572ms/step - loss: 4.4105 - val_loss: 2.4344 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "36/36 - 20s - 562ms/step - loss: 4.4174 - val_loss: 2.4365 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "36/36 - 20s - 555ms/step - loss: 4.4096 - val_loss: 2.4756 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "36/36 - 20s - 564ms/step - loss: 4.4388 - val_loss: 2.4408 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "36/36 - 20s - 553ms/step - loss: 4.4036 - val_loss: 2.4764 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "36/36 - 20s - 555ms/step - loss: 4.3979 - val_loss: 2.4366 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "36/36 - 23s - 641ms/step - loss: 4.4173 - val_loss: 2.4501 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "36/36 - 20s - 561ms/step - loss: 4.4020 - val_loss: 2.4391 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "36/36 - 21s - 574ms/step - loss: 4.4031 - val_loss: 2.4326 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "36/36 - 21s - 576ms/step - loss: 4.4082 - val_loss: 2.4337 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "36/36 - 21s - 579ms/step - loss: 4.4238 - val_loss: 2.4675 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "36/36 - 21s - 578ms/step - loss: 4.4346 - val_loss: 2.4994 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "36/36 - 20s - 562ms/step - loss: 4.4379 - val_loss: 2.4362 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "36/36 - 21s - 573ms/step - loss: 4.4234 - val_loss: 2.4348 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "36/36 - 21s - 577ms/step - loss: 4.4163 - val_loss: 2.4326 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "36/36 - 20s - 567ms/step - loss: 4.4144 - val_loss: 2.4447 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "36/36 - 20s - 568ms/step - loss: 4.4158 - val_loss: 2.4341 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "36/36 - 20s - 565ms/step - loss: 4.4041 - val_loss: 2.4346 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "36/36 - 20s - 553ms/step - loss: 4.4016 - val_loss: 2.4309 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 14'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 - 34s - 2s/step - loss: 302.7447 - val_loss: 213.5786 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "15/15 - 8s - 544ms/step - loss: 171.5744 - val_loss: 126.6361 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "15/15 - 9s - 603ms/step - loss: 107.3633 - val_loss: 85.9309 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "15/15 - 9s - 590ms/step - loss: 77.0463 - val_loss: 63.0949 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "15/15 - 8s - 542ms/step - loss: 60.0265 - val_loss: 51.1212 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "15/15 - 8s - 552ms/step - loss: 49.9731 - val_loss: 43.2077 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "15/15 - 8s - 530ms/step - loss: 43.6423 - val_loss: 37.7641 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "15/15 - 8s - 532ms/step - loss: 38.4502 - val_loss: 33.7625 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "15/15 - 8s - 525ms/step - loss: 33.8939 - val_loss: 29.5698 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "15/15 - 8s - 536ms/step - loss: 30.5397 - val_loss: 26.8688 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "15/15 - 8s - 561ms/step - loss: 28.1318 - val_loss: 24.7899 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "15/15 - 8s - 549ms/step - loss: 26.2471 - val_loss: 22.6617 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "15/15 - 8s - 544ms/step - loss: 24.4218 - val_loss: 21.2462 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "15/15 - 8s - 545ms/step - loss: 22.6857 - val_loss: 19.7016 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "15/15 - 8s - 551ms/step - loss: 21.4082 - val_loss: 18.7249 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "15/15 - 8s - 541ms/step - loss: 20.3953 - val_loss: 17.5687 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "15/15 - 8s - 542ms/step - loss: 19.2992 - val_loss: 16.6627 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "15/15 - 8s - 548ms/step - loss: 18.5750 - val_loss: 15.8851 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "15/15 - 8s - 531ms/step - loss: 17.7873 - val_loss: 15.2057 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "15/15 - 8s - 533ms/step - loss: 17.2481 - val_loss: 14.6375 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "15/15 - 8s - 557ms/step - loss: 16.6561 - val_loss: 14.0571 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "15/15 - 8s - 535ms/step - loss: 17.0341 - val_loss: 14.7037 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "15/15 - 8s - 547ms/step - loss: 16.1367 - val_loss: 13.1550 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "15/15 - 8s - 550ms/step - loss: 15.2448 - val_loss: 12.9845 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "15/15 - 8s - 549ms/step - loss: 14.8047 - val_loss: 12.3892 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "15/15 - 8s - 545ms/step - loss: 14.5416 - val_loss: 12.1382 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "15/15 - 8s - 526ms/step - loss: 14.2043 - val_loss: 11.7752 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "15/15 - 8s - 529ms/step - loss: 13.9291 - val_loss: 11.5235 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "15/15 - 8s - 548ms/step - loss: 13.5776 - val_loss: 11.2535 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "15/15 - 8s - 551ms/step - loss: 13.3830 - val_loss: 11.1434 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "15/15 - 8s - 545ms/step - loss: 13.2139 - val_loss: 10.8055 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "15/15 - 8s - 531ms/step - loss: 12.9321 - val_loss: 10.6932 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "15/15 - 8s - 536ms/step - loss: 12.8743 - val_loss: 10.4528 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "15/15 - 8s - 541ms/step - loss: 12.6800 - val_loss: 10.2658 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "15/15 - 8s - 534ms/step - loss: 12.4120 - val_loss: 10.2250 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "15/15 - 9s - 572ms/step - loss: 12.2664 - val_loss: 9.9756 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "15/15 - 8s - 554ms/step - loss: 12.1504 - val_loss: 9.8269 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "15/15 - 8s - 536ms/step - loss: 11.9566 - val_loss: 9.8113 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "15/15 - 8s - 545ms/step - loss: 11.8934 - val_loss: 9.5874 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "15/15 - 8s - 534ms/step - loss: 11.9716 - val_loss: 9.6368 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "15/15 - 10s - 648ms/step - loss: 11.7784 - val_loss: 9.4102 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "15/15 - 8s - 551ms/step - loss: 11.5581 - val_loss: 9.3091 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "15/15 - 8s - 556ms/step - loss: 11.8184 - val_loss: 9.2886 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "15/15 - 8s - 557ms/step - loss: 11.4342 - val_loss: 9.1544 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "15/15 - 8s - 562ms/step - loss: 11.2976 - val_loss: 9.0662 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "15/15 - 8s - 558ms/step - loss: 11.2266 - val_loss: 9.0550 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "15/15 - 8s - 536ms/step - loss: 11.2118 - val_loss: 8.9689 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "15/15 - 8s - 543ms/step - loss: 11.1590 - val_loss: 8.8859 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "15/15 - 9s - 567ms/step - loss: 11.0144 - val_loss: 8.8144 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "15/15 - 9s - 585ms/step - loss: 10.9392 - val_loss: 8.7650 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "15/15 - 8s - 559ms/step - loss: 10.9019 - val_loss: 8.7326 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "15/15 - 8s - 524ms/step - loss: 11.0244 - val_loss: 8.6251 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "15/15 - 8s - 539ms/step - loss: 11.1188 - val_loss: 8.7600 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "15/15 - 8s - 558ms/step - loss: 10.8441 - val_loss: 8.5397 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "15/15 - 8s - 539ms/step - loss: 10.7684 - val_loss: 8.5226 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "15/15 - 8s - 526ms/step - loss: 10.9103 - val_loss: 8.4860 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "15/15 - 8s - 531ms/step - loss: 10.6446 - val_loss: 8.4460 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "15/15 - 8s - 559ms/step - loss: 10.6054 - val_loss: 8.4468 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "15/15 - 8s - 553ms/step - loss: 10.7161 - val_loss: 8.3596 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "15/15 - 8s - 543ms/step - loss: 10.6236 - val_loss: 8.4028 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "15/15 - 8s - 548ms/step - loss: 10.5842 - val_loss: 8.3009 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "15/15 - 8s - 558ms/step - loss: 10.5011 - val_loss: 8.3200 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "15/15 - 8s - 533ms/step - loss: 10.4965 - val_loss: 8.2544 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "15/15 - 8s - 542ms/step - loss: 10.6378 - val_loss: 8.2701 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "15/15 - 8s - 563ms/step - loss: 10.3903 - val_loss: 8.1985 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "15/15 - 8s - 557ms/step - loss: 10.4075 - val_loss: 8.3196 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "15/15 - 8s - 563ms/step - loss: 10.4763 - val_loss: 8.2092 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "15/15 - 8s - 564ms/step - loss: 10.4163 - val_loss: 8.1995 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "15/15 - 8s - 546ms/step - loss: 10.5145 - val_loss: 8.1399 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "15/15 - 8s - 541ms/step - loss: 10.7310 - val_loss: 8.1178 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "15/15 - 8s - 532ms/step - loss: 10.3884 - val_loss: 8.0954 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "15/15 - 8s - 536ms/step - loss: 10.3107 - val_loss: 8.1571 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "15/15 - 8s - 544ms/step - loss: 10.2781 - val_loss: 8.1227 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "15/15 - 8s - 556ms/step - loss: 10.2607 - val_loss: 8.0790 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "15/15 - 8s - 558ms/step - loss: 10.2437 - val_loss: 8.0727 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "15/15 - 8s - 550ms/step - loss: 10.2149 - val_loss: 8.0249 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "15/15 - 9s - 616ms/step - loss: 10.2501 - val_loss: 8.1157 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "15/15 - 10s - 662ms/step - loss: 10.2468 - val_loss: 7.9952 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "15/15 - 8s - 561ms/step - loss: 10.2694 - val_loss: 7.9867 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "15/15 - 8s - 553ms/step - loss: 10.1926 - val_loss: 8.0041 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "15/15 - 8s - 526ms/step - loss: 10.2207 - val_loss: 7.9906 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "15/15 - 8s - 526ms/step - loss: 10.2689 - val_loss: 7.9730 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "15/15 - 8s - 530ms/step - loss: 10.1524 - val_loss: 7.9729 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "15/15 - 8s - 550ms/step - loss: 10.1560 - val_loss: 7.9334 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "15/15 - 8s - 546ms/step - loss: 10.1545 - val_loss: 7.9440 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "15/15 - 8s - 545ms/step - loss: 10.1531 - val_loss: 7.9149 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "15/15 - 8s - 537ms/step - loss: 10.2178 - val_loss: 7.9867 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "15/15 - 8s - 542ms/step - loss: 10.1539 - val_loss: 7.9126 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "15/15 - 8s - 548ms/step - loss: 10.1180 - val_loss: 7.9138 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "15/15 - 8s - 541ms/step - loss: 10.1416 - val_loss: 7.8864 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "15/15 - 9s - 576ms/step - loss: 10.1267 - val_loss: 7.8786 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "15/15 - 8s - 548ms/step - loss: 10.0870 - val_loss: 7.9139 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "15/15 - 8s - 545ms/step - loss: 10.1175 - val_loss: 7.9385 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "15/15 - 8s - 533ms/step - loss: 10.1293 - val_loss: 7.9010 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "15/15 - 8s - 529ms/step - loss: 10.1668 - val_loss: 7.8735 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "15/15 - 8s - 548ms/step - loss: 10.0750 - val_loss: 7.8612 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "15/15 - 8s - 560ms/step - loss: 10.0871 - val_loss: 7.8790 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "15/15 - 8s - 550ms/step - loss: 10.2815 - val_loss: 7.8685 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "15/15 - 9s - 567ms/step - loss: 10.1817 - val_loss: 7.9188 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "15/15 - 8s - 538ms/step - loss: 10.0333 - val_loss: 7.8302 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 15'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 - 24s - 2s/step - loss: 294.3513 - val_loss: 211.1915 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "14/14 - 7s - 530ms/step - loss: 160.6841 - val_loss: 121.8643 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "14/14 - 8s - 536ms/step - loss: 93.9333 - val_loss: 84.1056 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "14/14 - 8s - 549ms/step - loss: 63.0151 - val_loss: 59.4670 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "14/14 - 7s - 531ms/step - loss: 48.2982 - val_loss: 47.9900 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "14/14 - 8s - 546ms/step - loss: 40.8115 - val_loss: 41.9118 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "14/14 - 8s - 594ms/step - loss: 35.1613 - val_loss: 39.5361 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "14/14 - 8s - 540ms/step - loss: 31.8975 - val_loss: 35.1070 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "14/14 - 7s - 533ms/step - loss: 29.4755 - val_loss: 33.6070 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "14/14 - 8s - 550ms/step - loss: 27.3349 - val_loss: 31.2658 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "14/14 - 7s - 533ms/step - loss: 25.9447 - val_loss: 30.9176 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "14/14 - 8s - 539ms/step - loss: 24.6774 - val_loss: 30.1572 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "14/14 - 7s - 535ms/step - loss: 23.2567 - val_loss: 30.5548 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "14/14 - 9s - 667ms/step - loss: 22.3419 - val_loss: 29.5015 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "14/14 - 8s - 538ms/step - loss: 21.3464 - val_loss: 26.3490 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "14/14 - 7s - 535ms/step - loss: 20.9035 - val_loss: 25.9320 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "14/14 - 8s - 541ms/step - loss: 20.1833 - val_loss: 26.4667 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "14/14 - 8s - 545ms/step - loss: 19.6272 - val_loss: 27.9781 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "14/14 - 8s - 547ms/step - loss: 19.4456 - val_loss: 27.4873 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "14/14 - 8s - 542ms/step - loss: 18.7356 - val_loss: 23.6593 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "14/14 - 8s - 536ms/step - loss: 18.3544 - val_loss: 24.8008 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "14/14 - 7s - 531ms/step - loss: 18.0070 - val_loss: 24.2190 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "14/14 - 8s - 557ms/step - loss: 17.6010 - val_loss: 25.1089 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "14/14 - 8s - 567ms/step - loss: 17.2725 - val_loss: 22.5347 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "14/14 - 8s - 550ms/step - loss: 17.1300 - val_loss: 22.6749 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "14/14 - 8s - 555ms/step - loss: 17.1716 - val_loss: 22.9292 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "14/14 - 8s - 589ms/step - loss: 16.7006 - val_loss: 24.2191 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "14/14 - 8s - 576ms/step - loss: 16.5243 - val_loss: 22.2859 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "14/14 - 8s - 546ms/step - loss: 16.1993 - val_loss: 22.3296 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "14/14 - 8s - 537ms/step - loss: 16.0651 - val_loss: 23.3730 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "14/14 - 8s - 542ms/step - loss: 15.8790 - val_loss: 21.7457 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "14/14 - 8s - 544ms/step - loss: 15.6170 - val_loss: 24.5933 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "14/14 - 8s - 538ms/step - loss: 15.9797 - val_loss: 20.7089 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "14/14 - 8s - 542ms/step - loss: 15.8089 - val_loss: 22.8561 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "14/14 - 8s - 572ms/step - loss: 15.3057 - val_loss: 21.0628 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "14/14 - 8s - 539ms/step - loss: 15.4407 - val_loss: 21.3902 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "14/14 - 8s - 542ms/step - loss: 15.1489 - val_loss: 22.0035 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "14/14 - 8s - 556ms/step - loss: 15.2115 - val_loss: 21.6857 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "14/14 - 8s - 565ms/step - loss: 14.8740 - val_loss: 21.5655 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "14/14 - 8s - 566ms/step - loss: 14.9094 - val_loss: 22.2999 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "14/14 - 8s - 548ms/step - loss: 15.3557 - val_loss: 20.0331 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "14/14 - 8s - 557ms/step - loss: 14.6995 - val_loss: 23.0355 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "14/14 - 8s - 566ms/step - loss: 14.6662 - val_loss: 20.6202 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "14/14 - 8s - 569ms/step - loss: 14.7757 - val_loss: 21.3370 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "14/14 - 8s - 560ms/step - loss: 14.6111 - val_loss: 20.4882 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "14/14 - 8s - 558ms/step - loss: 14.5803 - val_loss: 21.5293 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "14/14 - 8s - 552ms/step - loss: 14.4154 - val_loss: 20.8053 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "14/14 - 7s - 533ms/step - loss: 14.4865 - val_loss: 21.5503 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "14/14 - 8s - 544ms/step - loss: 14.5133 - val_loss: 19.6252 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "14/14 - 8s - 541ms/step - loss: 14.9575 - val_loss: 21.6840 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "14/14 - 8s - 549ms/step - loss: 14.3833 - val_loss: 20.7002 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "14/14 - 8s - 564ms/step - loss: 14.2110 - val_loss: 20.6680 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "14/14 - 8s - 549ms/step - loss: 14.1894 - val_loss: 20.0757 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "14/14 - 10s - 700ms/step - loss: 14.0805 - val_loss: 20.8953 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "14/14 - 8s - 554ms/step - loss: 14.0762 - val_loss: 21.3982 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "14/14 - 8s - 554ms/step - loss: 14.1340 - val_loss: 20.1785 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "14/14 - 8s - 569ms/step - loss: 14.0198 - val_loss: 21.0064 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "14/14 - 8s - 574ms/step - loss: 14.2389 - val_loss: 21.1140 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "14/14 - 8s - 553ms/step - loss: 13.9242 - val_loss: 19.7739 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "14/14 - 8s - 551ms/step - loss: 14.0775 - val_loss: 20.1493 - learning_rate: 2.0000e-04\n",
      "Epoch 61/100\n",
      "14/14 - 8s - 556ms/step - loss: 13.8896 - val_loss: 20.6189 - learning_rate: 2.0000e-04\n",
      "Epoch 62/100\n",
      "14/14 - 8s - 551ms/step - loss: 13.8695 - val_loss: 20.7656 - learning_rate: 2.0000e-04\n",
      "Epoch 63/100\n",
      "14/14 - 8s - 542ms/step - loss: 13.9480 - val_loss: 20.6528 - learning_rate: 2.0000e-04\n",
      "Epoch 64/100\n",
      "14/14 - 8s - 542ms/step - loss: 13.9518 - val_loss: 20.7589 - learning_rate: 2.0000e-04\n",
      "Epoch 65/100\n",
      "14/14 - 8s - 553ms/step - loss: 13.8121 - val_loss: 20.5090 - learning_rate: 2.0000e-04\n",
      "Epoch 66/100\n",
      "14/14 - 8s - 565ms/step - loss: 13.9769 - val_loss: 20.2988 - learning_rate: 2.0000e-04\n",
      "Epoch 67/100\n",
      "14/14 - 8s - 560ms/step - loss: 13.8943 - val_loss: 20.3963 - learning_rate: 2.0000e-04\n",
      "Epoch 68/100\n",
      "14/14 - 8s - 560ms/step - loss: 13.9769 - val_loss: 21.0878 - learning_rate: 2.0000e-04\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "14/14 - 8s - 567ms/step - loss: 13.9751 - val_loss: 20.3596 - learning_rate: 2.0000e-04\n",
      "Epoch 70/100\n",
      "14/14 - 8s - 570ms/step - loss: 13.8374 - val_loss: 20.3662 - learning_rate: 4.0000e-05\n",
      "Epoch 71/100\n",
      "14/14 - 8s - 558ms/step - loss: 13.9382 - val_loss: 20.4091 - learning_rate: 4.0000e-05\n",
      "Epoch 72/100\n",
      "14/14 - 8s - 567ms/step - loss: 13.9138 - val_loss: 20.4180 - learning_rate: 4.0000e-05\n",
      "Epoch 73/100\n",
      "14/14 - 8s - 556ms/step - loss: 13.9193 - val_loss: 20.4158 - learning_rate: 4.0000e-05\n",
      "Epoch 74/100\n",
      "14/14 - 8s - 549ms/step - loss: 13.8737 - val_loss: 20.4392 - learning_rate: 4.0000e-05\n",
      "Epoch 75/100\n",
      "14/14 - 8s - 555ms/step - loss: 13.8814 - val_loss: 20.4748 - learning_rate: 4.0000e-05\n",
      "Epoch 76/100\n",
      "14/14 - 8s - 553ms/step - loss: 13.8535 - val_loss: 20.4106 - learning_rate: 4.0000e-05\n",
      "Epoch 77/100\n",
      "14/14 - 8s - 565ms/step - loss: 13.9148 - val_loss: 20.5233 - learning_rate: 4.0000e-05\n",
      "Epoch 78/100\n",
      "14/14 - 8s - 542ms/step - loss: 13.8918 - val_loss: 20.5246 - learning_rate: 4.0000e-05\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 - 8s - 552ms/step - loss: 13.8548 - val_loss: 20.5455 - learning_rate: 4.0000e-05\n",
      "Epoch 80/100\n",
      "14/14 - 8s - 542ms/step - loss: 13.8307 - val_loss: 20.5168 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "14/14 - 8s - 550ms/step - loss: 13.8597 - val_loss: 20.5167 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "14/14 - 8s - 566ms/step - loss: 13.9005 - val_loss: 20.5260 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "14/14 - 8s - 549ms/step - loss: 13.8712 - val_loss: 20.5186 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "14/14 - 8s - 587ms/step - loss: 13.7891 - val_loss: 20.5232 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "14/14 - 8s - 602ms/step - loss: 13.8817 - val_loss: 20.5329 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "14/14 - 8s - 557ms/step - loss: 13.8495 - val_loss: 20.5423 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "14/14 - 8s - 565ms/step - loss: 13.8833 - val_loss: 20.5317 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "14/14 - 8s - 584ms/step - loss: 13.8047 - val_loss: 20.5228 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "14/14 - 8s - 554ms/step - loss: 13.8173 - val_loss: 20.5312 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "14/14 - 8s - 560ms/step - loss: 13.7430 - val_loss: 20.5441 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "14/14 - 8s - 559ms/step - loss: 13.7668 - val_loss: 20.5366 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "14/14 - 10s - 687ms/step - loss: 13.7633 - val_loss: 20.5123 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "14/14 - 8s - 564ms/step - loss: 13.7941 - val_loss: 20.5241 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "14/14 - 8s - 541ms/step - loss: 13.8802 - val_loss: 20.5500 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "14/14 - 8s - 539ms/step - loss: 13.7822 - val_loss: 20.5188 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "14/14 - 8s - 549ms/step - loss: 13.8851 - val_loss: 20.5070 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "14/14 - 8s - 546ms/step - loss: 13.8388 - val_loss: 20.5036 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "14/14 - 8s - 553ms/step - loss: 13.8781 - val_loss: 20.4931 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "14/14 - 8s - 557ms/step - loss: 13.8681 - val_loss: 20.5036 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "14/14 - 8s - 561ms/step - loss: 13.8250 - val_loss: 20.5102 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 16'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 - 29s - 1s/step - loss: 243.5809 - val_loss: 132.9028 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 - 13s - 551ms/step - loss: 95.8763 - val_loss: 58.3216 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 - 13s - 558ms/step - loss: 52.0088 - val_loss: 39.2795 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 - 13s - 546ms/step - loss: 37.8984 - val_loss: 27.9965 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 - 13s - 545ms/step - loss: 30.7153 - val_loss: 22.1587 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "23/23 - 13s - 549ms/step - loss: 26.2905 - val_loss: 18.8327 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "23/23 - 12s - 543ms/step - loss: 23.4528 - val_loss: 16.4426 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "23/23 - 13s - 557ms/step - loss: 21.4784 - val_loss: 14.8277 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "23/23 - 12s - 543ms/step - loss: 19.7647 - val_loss: 13.3057 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "23/23 - 13s - 547ms/step - loss: 18.7130 - val_loss: 12.7143 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "23/23 - 13s - 546ms/step - loss: 17.9047 - val_loss: 11.2413 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "23/23 - 12s - 539ms/step - loss: 16.7571 - val_loss: 10.4290 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "23/23 - 12s - 538ms/step - loss: 15.8461 - val_loss: 9.8583 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "23/23 - 13s - 561ms/step - loss: 15.3775 - val_loss: 9.2500 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "23/23 - 13s - 549ms/step - loss: 14.8667 - val_loss: 8.7753 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "23/23 - 13s - 557ms/step - loss: 14.5377 - val_loss: 8.4254 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "23/23 - 13s - 550ms/step - loss: 13.9841 - val_loss: 8.5611 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "23/23 - 13s - 544ms/step - loss: 14.2264 - val_loss: 7.8365 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "23/23 - 16s - 679ms/step - loss: 13.5401 - val_loss: 7.4837 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "23/23 - 12s - 536ms/step - loss: 13.4866 - val_loss: 7.4787 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "23/23 - 12s - 535ms/step - loss: 13.0639 - val_loss: 7.5013 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "23/23 - 13s - 565ms/step - loss: 12.8576 - val_loss: 6.9474 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "23/23 - 13s - 557ms/step - loss: 12.6117 - val_loss: 6.8091 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "23/23 - 13s - 577ms/step - loss: 12.6565 - val_loss: 6.5632 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "23/23 - 13s - 581ms/step - loss: 12.4300 - val_loss: 6.5350 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "23/23 - 13s - 552ms/step - loss: 12.2864 - val_loss: 6.4964 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "23/23 - 13s - 556ms/step - loss: 12.0645 - val_loss: 6.2968 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "23/23 - 13s - 546ms/step - loss: 11.9880 - val_loss: 6.2665 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "23/23 - 12s - 543ms/step - loss: 11.9223 - val_loss: 6.0323 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "23/23 - 13s - 559ms/step - loss: 11.9741 - val_loss: 6.0130 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "23/23 - 14s - 590ms/step - loss: 11.7297 - val_loss: 5.8826 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "23/23 - 13s - 568ms/step - loss: 11.6918 - val_loss: 5.9881 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "23/23 - 13s - 570ms/step - loss: 11.6092 - val_loss: 5.8965 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "23/23 - 14s - 601ms/step - loss: 11.5330 - val_loss: 5.7580 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "23/23 - 13s - 582ms/step - loss: 11.5253 - val_loss: 5.8747 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "23/23 - 14s - 587ms/step - loss: 11.4429 - val_loss: 5.6651 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "23/23 - 13s - 565ms/step - loss: 11.4770 - val_loss: 5.8066 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "23/23 - 13s - 550ms/step - loss: 11.3002 - val_loss: 5.5504 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "23/23 - 13s - 556ms/step - loss: 11.5410 - val_loss: 5.7680 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "23/23 - 13s - 551ms/step - loss: 11.3728 - val_loss: 5.6958 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "23/23 - 13s - 552ms/step - loss: 11.2858 - val_loss: 5.4818 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "23/23 - 14s - 616ms/step - loss: 11.2672 - val_loss: 5.4429 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "23/23 - 13s - 562ms/step - loss: 11.1953 - val_loss: 5.5089 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "23/23 - 14s - 589ms/step - loss: 11.2421 - val_loss: 5.4856 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "23/23 - 13s - 558ms/step - loss: 11.2088 - val_loss: 5.3759 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "23/23 - 13s - 552ms/step - loss: 11.1737 - val_loss: 5.4271 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "23/23 - 13s - 562ms/step - loss: 11.1017 - val_loss: 5.3398 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "23/23 - 13s - 550ms/step - loss: 11.1689 - val_loss: 5.3548 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "23/23 - 13s - 546ms/step - loss: 11.0765 - val_loss: 5.3187 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "23/23 - 13s - 570ms/step - loss: 11.1096 - val_loss: 5.3974 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "23/23 - 13s - 557ms/step - loss: 11.0743 - val_loss: 5.3272 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "23/23 - 13s - 577ms/step - loss: 11.0396 - val_loss: 5.3454 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "23/23 - 13s - 587ms/step - loss: 11.0747 - val_loss: 5.4742 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "23/23 - 13s - 567ms/step - loss: 11.0113 - val_loss: 5.2521 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "23/23 - 13s - 553ms/step - loss: 11.0248 - val_loss: 5.4657 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "23/23 - 13s - 579ms/step - loss: 11.2206 - val_loss: 5.2358 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "23/23 - 13s - 577ms/step - loss: 11.0166 - val_loss: 5.2677 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "23/23 - 13s - 551ms/step - loss: 10.9767 - val_loss: 5.2504 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "23/23 - 12s - 543ms/step - loss: 11.0062 - val_loss: 5.2073 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "23/23 - 13s - 555ms/step - loss: 11.0137 - val_loss: 5.1990 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "23/23 - 13s - 561ms/step - loss: 11.0928 - val_loss: 5.5505 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "23/23 - 13s - 562ms/step - loss: 11.1710 - val_loss: 5.1997 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "23/23 - 13s - 560ms/step - loss: 10.9787 - val_loss: 5.1802 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "23/23 - 13s - 552ms/step - loss: 11.0300 - val_loss: 5.1819 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "23/23 - 13s - 561ms/step - loss: 10.9505 - val_loss: 5.2531 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "23/23 - 15s - 637ms/step - loss: 10.9369 - val_loss: 5.2221 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "23/23 - 13s - 568ms/step - loss: 10.9441 - val_loss: 5.2192 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "23/23 - 13s - 569ms/step - loss: 10.9072 - val_loss: 5.1578 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "23/23 - 13s - 557ms/step - loss: 10.9604 - val_loss: 5.3709 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "23/23 - 13s - 559ms/step - loss: 10.9666 - val_loss: 5.1708 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "23/23 - 13s - 547ms/step - loss: 10.9151 - val_loss: 5.2038 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "23/23 - 13s - 555ms/step - loss: 10.9423 - val_loss: 5.1691 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "23/23 - 14s - 596ms/step - loss: 10.9536 - val_loss: 5.1907 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "23/23 - 13s - 562ms/step - loss: 10.9349 - val_loss: 5.1716 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "23/23 - 13s - 568ms/step - loss: 10.9957 - val_loss: 5.2560 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "23/23 - 13s - 557ms/step - loss: 10.9868 - val_loss: 5.1727 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "23/23 - 13s - 563ms/step - loss: 11.0658 - val_loss: 5.1737 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "23/23 - 13s - 564ms/step - loss: 10.9888 - val_loss: 5.3933 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "23/23 - 13s - 545ms/step - loss: 10.9370 - val_loss: 5.2371 - learning_rate: 2.0000e-04\n",
      "Epoch 80/100\n",
      "23/23 - 13s - 549ms/step - loss: 10.8987 - val_loss: 5.1712 - learning_rate: 2.0000e-04\n",
      "Epoch 81/100\n",
      "23/23 - 13s - 544ms/step - loss: 10.9001 - val_loss: 5.1627 - learning_rate: 2.0000e-04\n",
      "Epoch 82/100\n",
      "23/23 - 13s - 557ms/step - loss: 10.8867 - val_loss: 5.1800 - learning_rate: 2.0000e-04\n",
      "Epoch 83/100\n",
      "23/23 - 13s - 576ms/step - loss: 10.8760 - val_loss: 5.1747 - learning_rate: 2.0000e-04\n",
      "Epoch 84/100\n",
      "23/23 - 13s - 585ms/step - loss: 10.8812 - val_loss: 5.1667 - learning_rate: 2.0000e-04\n",
      "Epoch 85/100\n",
      "23/23 - 13s - 585ms/step - loss: 10.8764 - val_loss: 5.1759 - learning_rate: 2.0000e-04\n",
      "Epoch 86/100\n",
      "23/23 - 13s - 570ms/step - loss: 10.8744 - val_loss: 5.1592 - learning_rate: 2.0000e-04\n",
      "Epoch 87/100\n",
      "23/23 - 13s - 574ms/step - loss: 10.8743 - val_loss: 5.1702 - learning_rate: 2.0000e-04\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "23/23 - 13s - 559ms/step - loss: 10.8754 - val_loss: 5.1661 - learning_rate: 2.0000e-04\n",
      "Epoch 89/100\n",
      "23/23 - 13s - 559ms/step - loss: 10.8725 - val_loss: 5.1665 - learning_rate: 4.0000e-05\n",
      "Epoch 90/100\n",
      "23/23 - 15s - 643ms/step - loss: 10.8698 - val_loss: 5.1666 - learning_rate: 4.0000e-05\n",
      "Epoch 91/100\n",
      "23/23 - 13s - 554ms/step - loss: 10.8724 - val_loss: 5.1669 - learning_rate: 4.0000e-05\n",
      "Epoch 92/100\n",
      "23/23 - 13s - 562ms/step - loss: 10.8731 - val_loss: 5.1675 - learning_rate: 4.0000e-05\n",
      "Epoch 93/100\n",
      "23/23 - 13s - 558ms/step - loss: 10.8716 - val_loss: 5.1685 - learning_rate: 4.0000e-05\n",
      "Epoch 94/100\n",
      "23/23 - 13s - 556ms/step - loss: 10.8714 - val_loss: 5.1700 - learning_rate: 4.0000e-05\n",
      "Epoch 95/100\n",
      "23/23 - 13s - 560ms/step - loss: 10.8751 - val_loss: 5.1729 - learning_rate: 4.0000e-05\n",
      "Epoch 96/100\n",
      "23/23 - 13s - 552ms/step - loss: 10.8697 - val_loss: 5.1673 - learning_rate: 4.0000e-05\n",
      "Epoch 97/100\n",
      "23/23 - 13s - 551ms/step - loss: 10.8705 - val_loss: 5.1654 - learning_rate: 4.0000e-05\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "23/23 - 12s - 543ms/step - loss: 10.8713 - val_loss: 5.1707 - learning_rate: 4.0000e-05\n",
      "Epoch 99/100\n",
      "23/23 - 13s - 548ms/step - loss: 10.8688 - val_loss: 5.1702 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "23/23 - 13s - 556ms/step - loss: 10.8692 - val_loss: 5.1710 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 17'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 - 36s - 2s/step - loss: 244.6139 - val_loss: 136.0779 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 - 12s - 540ms/step - loss: 96.8860 - val_loss: 60.5322 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 - 13s - 547ms/step - loss: 51.4444 - val_loss: 36.6732 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 - 13s - 556ms/step - loss: 35.4614 - val_loss: 26.7141 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 - 13s - 570ms/step - loss: 27.7348 - val_loss: 20.8911 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "23/23 - 13s - 582ms/step - loss: 23.3094 - val_loss: 18.6638 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "23/23 - 13s - 559ms/step - loss: 20.0274 - val_loss: 14.5520 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "23/23 - 13s - 544ms/step - loss: 17.9876 - val_loss: 12.6098 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "23/23 - 13s - 547ms/step - loss: 16.1061 - val_loss: 11.1878 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "23/23 - 13s - 551ms/step - loss: 14.8901 - val_loss: 10.3640 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "23/23 - 14s - 622ms/step - loss: 13.4693 - val_loss: 9.0288 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "23/23 - 13s - 550ms/step - loss: 12.6489 - val_loss: 8.0244 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "23/23 - 13s - 555ms/step - loss: 11.6677 - val_loss: 7.3173 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "23/23 - 13s - 548ms/step - loss: 11.7179 - val_loss: 7.1132 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "23/23 - 12s - 541ms/step - loss: 10.6059 - val_loss: 6.8018 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "23/23 - 12s - 543ms/step - loss: 10.1024 - val_loss: 5.7674 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "23/23 - 12s - 543ms/step - loss: 9.7317 - val_loss: 5.4371 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "23/23 - 13s - 578ms/step - loss: 9.3855 - val_loss: 5.3991 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "23/23 - 13s - 560ms/step - loss: 9.1012 - val_loss: 4.7526 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "23/23 - 13s - 555ms/step - loss: 8.6697 - val_loss: 5.9543 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "23/23 - 13s - 545ms/step - loss: 9.0794 - val_loss: 4.2873 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "23/23 - 13s - 565ms/step - loss: 8.3141 - val_loss: 4.1160 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "23/23 - 13s - 548ms/step - loss: 8.1381 - val_loss: 3.8671 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "23/23 - 13s - 545ms/step - loss: 7.8789 - val_loss: 3.8580 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "23/23 - 12s - 541ms/step - loss: 7.6661 - val_loss: 3.6449 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "23/23 - 12s - 542ms/step - loss: 7.5122 - val_loss: 3.6738 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "23/23 - 13s - 547ms/step - loss: 7.5724 - val_loss: 3.4380 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "23/23 - 13s - 546ms/step - loss: 7.2756 - val_loss: 3.1935 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "23/23 - 13s - 563ms/step - loss: 7.2281 - val_loss: 3.2382 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "23/23 - 13s - 552ms/step - loss: 7.1763 - val_loss: 2.9855 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "23/23 - 12s - 539ms/step - loss: 6.9902 - val_loss: 2.9697 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "23/23 - 13s - 559ms/step - loss: 6.8970 - val_loss: 2.8261 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "23/23 - 13s - 547ms/step - loss: 6.8516 - val_loss: 2.7570 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "23/23 - 13s - 563ms/step - loss: 6.7450 - val_loss: 2.7831 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "23/23 - 14s - 616ms/step - loss: 6.6779 - val_loss: 2.8087 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "23/23 - 14s - 590ms/step - loss: 6.5985 - val_loss: 2.5838 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "23/23 - 13s - 571ms/step - loss: 6.7488 - val_loss: 2.5621 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "23/23 - 13s - 565ms/step - loss: 6.5552 - val_loss: 2.5320 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "23/23 - 13s - 570ms/step - loss: 6.4942 - val_loss: 2.4470 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "23/23 - 13s - 572ms/step - loss: 6.5145 - val_loss: 2.5421 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "23/23 - 13s - 555ms/step - loss: 6.4447 - val_loss: 2.5656 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "23/23 - 13s - 548ms/step - loss: 6.2769 - val_loss: 2.3433 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "23/23 - 13s - 552ms/step - loss: 6.3280 - val_loss: 2.3102 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "23/23 - 13s - 551ms/step - loss: 6.3530 - val_loss: 2.3606 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "23/23 - 13s - 558ms/step - loss: 6.3412 - val_loss: 2.2597 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "23/23 - 13s - 566ms/step - loss: 6.2500 - val_loss: 2.4466 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "23/23 - 13s - 561ms/step - loss: 6.2814 - val_loss: 2.9437 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "23/23 - 13s - 556ms/step - loss: 6.2539 - val_loss: 2.4728 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "23/23 - 13s - 544ms/step - loss: 6.1971 - val_loss: 2.1535 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "23/23 - 12s - 543ms/step - loss: 6.2099 - val_loss: 2.2645 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "23/23 - 13s - 551ms/step - loss: 6.1265 - val_loss: 2.2780 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "23/23 - 13s - 562ms/step - loss: 6.1568 - val_loss: 2.1315 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "23/23 - 13s - 568ms/step - loss: 6.0921 - val_loss: 2.1959 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "23/23 - 14s - 609ms/step - loss: 6.1892 - val_loss: 2.0657 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "23/23 - 13s - 565ms/step - loss: 6.1840 - val_loss: 2.0536 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "23/23 - 13s - 556ms/step - loss: 6.0739 - val_loss: 2.2487 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "23/23 - 13s - 546ms/step - loss: 6.1292 - val_loss: 2.0292 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "23/23 - 13s - 546ms/step - loss: 5.9552 - val_loss: 2.2695 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "23/23 - 15s - 646ms/step - loss: 6.0694 - val_loss: 2.0649 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "23/23 - 13s - 568ms/step - loss: 6.0183 - val_loss: 2.0585 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "23/23 - 13s - 566ms/step - loss: 6.0738 - val_loss: 2.0143 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "23/23 - 13s - 569ms/step - loss: 5.9841 - val_loss: 1.9808 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "23/23 - 13s - 557ms/step - loss: 6.0965 - val_loss: 1.9813 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "23/23 - 13s - 560ms/step - loss: 6.1189 - val_loss: 2.1527 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "23/23 - 13s - 559ms/step - loss: 6.0550 - val_loss: 2.1894 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "23/23 - 13s - 558ms/step - loss: 5.8900 - val_loss: 2.0675 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "23/23 - 13s - 554ms/step - loss: 5.9196 - val_loss: 2.0015 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "23/23 - 13s - 560ms/step - loss: 5.9841 - val_loss: 1.9467 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "23/23 - 13s - 563ms/step - loss: 5.9216 - val_loss: 2.0758 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "23/23 - 13s - 559ms/step - loss: 5.9270 - val_loss: 2.0818 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "23/23 - 13s - 552ms/step - loss: 5.9092 - val_loss: 2.2346 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "23/23 - 13s - 555ms/step - loss: 5.9322 - val_loss: 1.9116 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "23/23 - 13s - 550ms/step - loss: 6.0211 - val_loss: 1.9040 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "23/23 - 14s - 608ms/step - loss: 5.8485 - val_loss: 1.9149 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "23/23 - 13s - 586ms/step - loss: 5.8547 - val_loss: 1.9176 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "23/23 - 14s - 590ms/step - loss: 5.9354 - val_loss: 1.8918 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "23/23 - 13s - 586ms/step - loss: 5.8957 - val_loss: 1.9728 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "23/23 - 13s - 578ms/step - loss: 5.9791 - val_loss: 2.2590 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "23/23 - 13s - 568ms/step - loss: 5.8366 - val_loss: 1.9733 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "23/23 - 14s - 588ms/step - loss: 5.8987 - val_loss: 2.0398 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "23/23 - 13s - 576ms/step - loss: 5.8247 - val_loss: 1.8919 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "23/23 - 13s - 565ms/step - loss: 5.8684 - val_loss: 1.8661 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "23/23 - 14s - 626ms/step - loss: 5.8552 - val_loss: 1.9034 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "23/23 - 13s - 560ms/step - loss: 5.8427 - val_loss: 1.9050 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "23/23 - 13s - 571ms/step - loss: 5.8283 - val_loss: 1.8758 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "23/23 - 13s - 557ms/step - loss: 5.7873 - val_loss: 1.9564 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "23/23 - 13s - 557ms/step - loss: 5.8059 - val_loss: 1.9395 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "23/23 - 13s - 550ms/step - loss: 5.7489 - val_loss: 1.8580 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "23/23 - 12s - 541ms/step - loss: 5.8865 - val_loss: 1.9511 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "23/23 - 13s - 561ms/step - loss: 5.9033 - val_loss: 2.0293 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "23/23 - 13s - 559ms/step - loss: 5.8916 - val_loss: 2.1089 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "23/23 - 13s - 572ms/step - loss: 5.8193 - val_loss: 1.8387 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "23/23 - 13s - 562ms/step - loss: 5.8069 - val_loss: 1.8406 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "23/23 - 13s - 560ms/step - loss: 5.7917 - val_loss: 2.0154 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "23/23 - 13s - 547ms/step - loss: 5.9153 - val_loss: 2.2896 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "23/23 - 13s - 546ms/step - loss: 5.7896 - val_loss: 1.9435 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "23/23 - 13s - 551ms/step - loss: 5.8085 - val_loss: 1.8677 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "23/23 - 13s - 554ms/step - loss: 5.8904 - val_loss: 1.9561 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "23/23 - 13s - 552ms/step - loss: 5.8265 - val_loss: 1.8252 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "23/23 - 13s - 563ms/step - loss: 5.8621 - val_loss: 2.1263 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 18'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 - 43s - 1s/step - loss: 197.1447 - val_loss: 78.3982 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "32/32 - 18s - 551ms/step - loss: 49.8863 - val_loss: 28.1984 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "32/32 - 19s - 599ms/step - loss: 25.0784 - val_loss: 17.1131 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "32/32 - 17s - 545ms/step - loss: 17.8720 - val_loss: 12.7040 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "32/32 - 18s - 553ms/step - loss: 14.0874 - val_loss: 10.3682 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "32/32 - 17s - 544ms/step - loss: 12.4197 - val_loss: 8.2656 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "32/32 - 17s - 538ms/step - loss: 10.5545 - val_loss: 7.0632 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "32/32 - 17s - 534ms/step - loss: 9.6364 - val_loss: 6.4223 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "32/32 - 18s - 549ms/step - loss: 8.8524 - val_loss: 5.5021 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "32/32 - 18s - 553ms/step - loss: 8.1618 - val_loss: 4.9982 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "32/32 - 17s - 537ms/step - loss: 7.7719 - val_loss: 5.3205 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "32/32 - 17s - 545ms/step - loss: 7.3465 - val_loss: 4.3082 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "32/32 - 18s - 553ms/step - loss: 7.0274 - val_loss: 4.1650 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "32/32 - 17s - 542ms/step - loss: 6.8223 - val_loss: 3.9891 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "32/32 - 17s - 543ms/step - loss: 6.5650 - val_loss: 3.7901 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "32/32 - 18s - 568ms/step - loss: 6.5277 - val_loss: 3.5013 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "32/32 - 17s - 543ms/step - loss: 6.3656 - val_loss: 3.3704 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "32/32 - 18s - 547ms/step - loss: 6.1900 - val_loss: 3.2752 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "32/32 - 17s - 546ms/step - loss: 6.1764 - val_loss: 3.1769 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "32/32 - 18s - 553ms/step - loss: 5.9263 - val_loss: 3.0975 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "32/32 - 19s - 593ms/step - loss: 5.9159 - val_loss: 3.0348 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "32/32 - 17s - 539ms/step - loss: 5.9543 - val_loss: 3.2772 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "32/32 - 17s - 540ms/step - loss: 5.7478 - val_loss: 3.1301 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "32/32 - 18s - 547ms/step - loss: 5.7418 - val_loss: 2.9903 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "32/32 - 18s - 555ms/step - loss: 5.6726 - val_loss: 3.1466 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "32/32 - 17s - 537ms/step - loss: 5.6964 - val_loss: 2.8052 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "32/32 - 17s - 538ms/step - loss: 5.6242 - val_loss: 2.7839 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "32/32 - 17s - 537ms/step - loss: 5.5445 - val_loss: 2.8143 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "32/32 - 17s - 539ms/step - loss: 5.5801 - val_loss: 2.7263 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "32/32 - 19s - 579ms/step - loss: 5.6024 - val_loss: 2.7170 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "32/32 - 18s - 573ms/step - loss: 5.4833 - val_loss: 2.8731 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "32/32 - 17s - 540ms/step - loss: 5.4852 - val_loss: 2.7699 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "32/32 - 18s - 567ms/step - loss: 5.4691 - val_loss: 2.7475 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "32/32 - 20s - 638ms/step - loss: 5.4735 - val_loss: 2.6939 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "32/32 - 18s - 566ms/step - loss: 5.4471 - val_loss: 2.6412 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "32/32 - 18s - 551ms/step - loss: 5.4143 - val_loss: 2.6175 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "32/32 - 18s - 551ms/step - loss: 5.4249 - val_loss: 2.6150 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "32/32 - 19s - 585ms/step - loss: 5.4297 - val_loss: 2.6161 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "32/32 - 17s - 516ms/step - loss: 5.4015 - val_loss: 2.6245 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "32/32 - 17s - 520ms/step - loss: 5.3867 - val_loss: 2.5870 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "32/32 - 17s - 538ms/step - loss: 5.3721 - val_loss: 2.5742 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "32/32 - 17s - 531ms/step - loss: 5.3597 - val_loss: 2.5735 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "32/32 - 17s - 516ms/step - loss: 5.3425 - val_loss: 2.5654 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "32/32 - 16s - 507ms/step - loss: 5.3391 - val_loss: 2.5811 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "32/32 - 17s - 519ms/step - loss: 5.3666 - val_loss: 2.7718 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "32/32 - 17s - 536ms/step - loss: 5.3713 - val_loss: 2.6182 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "32/32 - 17s - 530ms/step - loss: 5.3294 - val_loss: 2.6118 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "32/32 - 17s - 521ms/step - loss: 5.3454 - val_loss: 2.5698 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "32/32 - 16s - 514ms/step - loss: 5.3128 - val_loss: 2.5866 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "32/32 - 17s - 529ms/step - loss: 5.3308 - val_loss: 2.5365 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "32/32 - 17s - 530ms/step - loss: 5.3837 - val_loss: 2.6472 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "32/32 - 17s - 546ms/step - loss: 5.3218 - val_loss: 2.5492 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "32/32 - 17s - 533ms/step - loss: 5.3196 - val_loss: 2.7706 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "32/32 - 17s - 527ms/step - loss: 5.3300 - val_loss: 2.5508 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "32/32 - 17s - 516ms/step - loss: 5.2983 - val_loss: 2.5596 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "32/32 - 18s - 567ms/step - loss: 5.3326 - val_loss: 2.5185 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "32/32 - 17s - 535ms/step - loss: 5.2811 - val_loss: 2.6801 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "32/32 - 17s - 525ms/step - loss: 5.3104 - val_loss: 2.5220 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "32/32 - 17s - 518ms/step - loss: 5.3018 - val_loss: 2.5349 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "32/32 - 17s - 516ms/step - loss: 5.2963 - val_loss: 2.5181 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "32/32 - 16s - 511ms/step - loss: 5.3190 - val_loss: 2.8813 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "32/32 - 17s - 524ms/step - loss: 5.3812 - val_loss: 2.5530 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "32/32 - 17s - 522ms/step - loss: 5.3086 - val_loss: 2.5306 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "32/32 - 16s - 513ms/step - loss: 5.2777 - val_loss: 2.5375 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "32/32 - 16s - 511ms/step - loss: 5.3114 - val_loss: 2.5080 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "32/32 - 16s - 513ms/step - loss: 5.2811 - val_loss: 2.5492 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "32/32 - 16s - 513ms/step - loss: 5.2775 - val_loss: 2.5069 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "32/32 - 17s - 546ms/step - loss: 5.2831 - val_loss: 2.5679 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "32/32 - 17s - 522ms/step - loss: 5.2793 - val_loss: 2.5735 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "32/32 - 16s - 511ms/step - loss: 5.3241 - val_loss: 2.5205 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "32/32 - 17s - 517ms/step - loss: 5.3255 - val_loss: 2.5082 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "32/32 - 16s - 514ms/step - loss: 5.2843 - val_loss: 2.5022 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "32/32 - 17s - 520ms/step - loss: 5.2992 - val_loss: 2.5358 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "32/32 - 18s - 574ms/step - loss: 5.2747 - val_loss: 2.5152 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "32/32 - 16s - 515ms/step - loss: 5.2794 - val_loss: 2.4994 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "32/32 - 16s - 510ms/step - loss: 5.2854 - val_loss: 2.5087 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "32/32 - 16s - 511ms/step - loss: 5.2887 - val_loss: 2.4993 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "32/32 - 17s - 516ms/step - loss: 5.2783 - val_loss: 2.5299 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "32/32 - 16s - 510ms/step - loss: 5.2701 - val_loss: 2.5676 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "32/32 - 16s - 513ms/step - loss: 5.2843 - val_loss: 2.4973 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "32/32 - 17s - 545ms/step - loss: 5.3024 - val_loss: 2.5055 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "32/32 - 17s - 545ms/step - loss: 5.2964 - val_loss: 2.5406 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "32/32 - 17s - 527ms/step - loss: 5.2994 - val_loss: 2.5863 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "32/32 - 17s - 527ms/step - loss: 5.2744 - val_loss: 2.4981 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "32/32 - 16s - 509ms/step - loss: 5.2659 - val_loss: 2.5288 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "32/32 - 17s - 533ms/step - loss: 5.2718 - val_loss: 2.4992 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "32/32 - 17s - 521ms/step - loss: 5.2661 - val_loss: 2.6104 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "32/32 - 17s - 519ms/step - loss: 5.2983 - val_loss: 2.5099 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "32/32 - 17s - 530ms/step - loss: 5.2688 - val_loss: 2.5203 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "32/32 - 17s - 537ms/step - loss: 5.2794 - val_loss: 2.5163 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "32/32 - 17s - 522ms/step - loss: 5.2800 - val_loss: 2.5002 - learning_rate: 2.0000e-04\n",
      "Epoch 92/100\n",
      "32/32 - 16s - 508ms/step - loss: 5.2677 - val_loss: 2.5221 - learning_rate: 2.0000e-04\n",
      "Epoch 93/100\n",
      "32/32 - 18s - 560ms/step - loss: 5.2672 - val_loss: 2.5189 - learning_rate: 2.0000e-04\n",
      "Epoch 94/100\n",
      "32/32 - 16s - 509ms/step - loss: 5.2651 - val_loss: 2.5191 - learning_rate: 2.0000e-04\n",
      "Epoch 95/100\n",
      "32/32 - 17s - 540ms/step - loss: 5.2627 - val_loss: 2.5084 - learning_rate: 2.0000e-04\n",
      "Epoch 96/100\n",
      "32/32 - 17s - 545ms/step - loss: 5.2641 - val_loss: 2.5064 - learning_rate: 2.0000e-04\n",
      "Epoch 97/100\n",
      "32/32 - 17s - 541ms/step - loss: 5.2716 - val_loss: 2.5155 - learning_rate: 2.0000e-04\n",
      "Epoch 98/100\n",
      "32/32 - 17s - 520ms/step - loss: 5.2649 - val_loss: 2.5156 - learning_rate: 2.0000e-04\n",
      "Epoch 99/100\n",
      "32/32 - 16s - 513ms/step - loss: 5.2632 - val_loss: 2.5163 - learning_rate: 2.0000e-04\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "32/32 - 17s - 517ms/step - loss: 5.2647 - val_loss: 2.5168 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 19'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 - 29s - 1s/step - loss: 251.1724 - val_loss: 140.6481 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 - 12s - 508ms/step - loss: 100.3954 - val_loss: 68.8342 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 - 12s - 514ms/step - loss: 55.3321 - val_loss: 43.1895 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 - 12s - 519ms/step - loss: 39.6064 - val_loss: 33.5945 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 - 14s - 595ms/step - loss: 32.5078 - val_loss: 26.9594 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "23/23 - 13s - 568ms/step - loss: 28.3873 - val_loss: 23.6868 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "23/23 - 12s - 517ms/step - loss: 24.7424 - val_loss: 22.2939 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "23/23 - 12s - 521ms/step - loss: 22.4969 - val_loss: 19.3502 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "23/23 - 12s - 520ms/step - loss: 20.8053 - val_loss: 17.2627 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "23/23 - 12s - 523ms/step - loss: 19.4991 - val_loss: 18.3940 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "23/23 - 12s - 519ms/step - loss: 18.1251 - val_loss: 15.5991 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "23/23 - 12s - 520ms/step - loss: 17.3509 - val_loss: 14.7378 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "23/23 - 14s - 595ms/step - loss: 16.2823 - val_loss: 13.2309 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "23/23 - 12s - 517ms/step - loss: 15.8729 - val_loss: 13.3746 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "23/23 - 12s - 506ms/step - loss: 14.8659 - val_loss: 17.4976 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "23/23 - 12s - 513ms/step - loss: 14.5677 - val_loss: 11.5442 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "23/23 - 12s - 528ms/step - loss: 14.0510 - val_loss: 11.0521 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "23/23 - 12s - 533ms/step - loss: 13.8975 - val_loss: 14.1518 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "23/23 - 12s - 510ms/step - loss: 13.1785 - val_loss: 10.3149 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "23/23 - 12s - 514ms/step - loss: 13.0789 - val_loss: 11.2129 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "23/23 - 12s - 515ms/step - loss: 12.4161 - val_loss: 10.6193 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "23/23 - 12s - 508ms/step - loss: 12.0550 - val_loss: 10.5967 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "23/23 - 12s - 508ms/step - loss: 11.9251 - val_loss: 9.9354 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "23/23 - 12s - 507ms/step - loss: 11.6095 - val_loss: 9.5637 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "23/23 - 12s - 514ms/step - loss: 11.3937 - val_loss: 10.8629 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "23/23 - 12s - 517ms/step - loss: 11.2162 - val_loss: 9.1888 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "23/23 - 12s - 511ms/step - loss: 11.1118 - val_loss: 11.0788 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "23/23 - 12s - 528ms/step - loss: 10.9823 - val_loss: 8.7553 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "23/23 - 12s - 525ms/step - loss: 10.7728 - val_loss: 9.2041 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "23/23 - 12s - 526ms/step - loss: 10.5185 - val_loss: 8.0348 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "23/23 - 12s - 523ms/step - loss: 10.4730 - val_loss: 9.0848 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "23/23 - 12s - 515ms/step - loss: 10.3993 - val_loss: 8.6143 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "23/23 - 12s - 540ms/step - loss: 10.2473 - val_loss: 8.7668 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "23/23 - 12s - 529ms/step - loss: 10.3319 - val_loss: 7.7048 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "23/23 - 12s - 523ms/step - loss: 10.1258 - val_loss: 7.5151 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "23/23 - 12s - 508ms/step - loss: 10.0149 - val_loss: 9.8876 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "23/23 - 12s - 513ms/step - loss: 10.2016 - val_loss: 8.4001 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "23/23 - 12s - 511ms/step - loss: 9.7887 - val_loss: 7.2526 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "23/23 - 15s - 637ms/step - loss: 9.7638 - val_loss: 7.6865 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "23/23 - 12s - 524ms/step - loss: 9.7738 - val_loss: 7.8162 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "23/23 - 12s - 515ms/step - loss: 9.6511 - val_loss: 8.4125 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "23/23 - 12s - 513ms/step - loss: 9.5306 - val_loss: 7.1945 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "23/23 - 12s - 531ms/step - loss: 9.4792 - val_loss: 7.3213 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "23/23 - 12s - 513ms/step - loss: 9.6506 - val_loss: 6.8698 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "23/23 - 12s - 510ms/step - loss: 9.6206 - val_loss: 6.9469 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "23/23 - 12s - 506ms/step - loss: 9.3732 - val_loss: 7.7673 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "23/23 - 13s - 558ms/step - loss: 9.4010 - val_loss: 8.4177 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "23/23 - 16s - 691ms/step - loss: 9.3904 - val_loss: 7.6487 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "23/23 - 18s - 799ms/step - loss: 9.3173 - val_loss: 7.8141 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "23/23 - 13s - 578ms/step - loss: 9.3812 - val_loss: 7.7460 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "23/23 - 15s - 642ms/step - loss: 9.5457 - val_loss: 7.0165 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "23/23 - 14s - 599ms/step - loss: 9.1294 - val_loss: 7.6215 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "23/23 - 13s - 582ms/step - loss: 9.1657 - val_loss: 9.0340 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "23/23 - 13s - 580ms/step - loss: 9.1136 - val_loss: 6.7329 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "23/23 - 12s - 532ms/step - loss: 9.1213 - val_loss: 7.7386 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "23/23 - 12s - 528ms/step - loss: 9.1320 - val_loss: 7.2313 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "23/23 - 12s - 529ms/step - loss: 9.0838 - val_loss: 6.8718 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "23/23 - 13s - 554ms/step - loss: 9.0586 - val_loss: 7.2472 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "23/23 - 13s - 581ms/step - loss: 8.9701 - val_loss: 6.7457 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "23/23 - 13s - 564ms/step - loss: 9.0777 - val_loss: 8.0824 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "23/23 - 12s - 529ms/step - loss: 9.0866 - val_loss: 7.0430 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "23/23 - 13s - 585ms/step - loss: 9.0010 - val_loss: 7.0535 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "23/23 - 14s - 629ms/step - loss: 8.9094 - val_loss: 7.1763 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "23/23 - 13s - 556ms/step - loss: 8.9187 - val_loss: 6.8205 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "23/23 - 14s - 622ms/step - loss: 8.9033 - val_loss: 7.0569 - learning_rate: 2.0000e-04\n",
      "Epoch 66/100\n",
      "23/23 - 15s - 664ms/step - loss: 8.8677 - val_loss: 7.2763 - learning_rate: 2.0000e-04\n",
      "Epoch 67/100\n",
      "23/23 - 14s - 588ms/step - loss: 8.9062 - val_loss: 6.9070 - learning_rate: 2.0000e-04\n",
      "Epoch 68/100\n",
      "23/23 - 13s - 587ms/step - loss: 8.9823 - val_loss: 6.9651 - learning_rate: 2.0000e-04\n",
      "Epoch 69/100\n",
      "23/23 - 15s - 658ms/step - loss: 8.9567 - val_loss: 7.1043 - learning_rate: 2.0000e-04\n",
      "Epoch 70/100\n",
      "23/23 - 14s - 620ms/step - loss: 8.9239 - val_loss: 7.0897 - learning_rate: 2.0000e-04\n",
      "Epoch 71/100\n",
      "23/23 - 13s - 574ms/step - loss: 8.8653 - val_loss: 7.0294 - learning_rate: 2.0000e-04\n",
      "Epoch 72/100\n",
      "23/23 - 13s - 562ms/step - loss: 8.8964 - val_loss: 6.9815 - learning_rate: 2.0000e-04\n",
      "Epoch 73/100\n",
      "23/23 - 13s - 555ms/step - loss: 8.8438 - val_loss: 7.1652 - learning_rate: 2.0000e-04\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "23/23 - 13s - 546ms/step - loss: 8.9169 - val_loss: 6.9756 - learning_rate: 2.0000e-04\n",
      "Epoch 75/100\n",
      "23/23 - 12s - 543ms/step - loss: 8.8791 - val_loss: 6.9885 - learning_rate: 4.0000e-05\n",
      "Epoch 76/100\n",
      "23/23 - 13s - 560ms/step - loss: 8.8366 - val_loss: 6.9924 - learning_rate: 4.0000e-05\n",
      "Epoch 77/100\n",
      "23/23 - 13s - 568ms/step - loss: 8.8224 - val_loss: 7.0667 - learning_rate: 4.0000e-05\n",
      "Epoch 78/100\n",
      "23/23 - 13s - 553ms/step - loss: 8.8881 - val_loss: 7.0527 - learning_rate: 4.0000e-05\n",
      "Epoch 79/100\n",
      "23/23 - 13s - 556ms/step - loss: 8.8808 - val_loss: 7.0313 - learning_rate: 4.0000e-05\n",
      "Epoch 80/100\n",
      "23/23 - 13s - 571ms/step - loss: 8.8472 - val_loss: 7.0178 - learning_rate: 4.0000e-05\n",
      "Epoch 81/100\n",
      "23/23 - 13s - 562ms/step - loss: 8.9038 - val_loss: 7.0601 - learning_rate: 4.0000e-05\n",
      "Epoch 82/100\n",
      "23/23 - 13s - 554ms/step - loss: 8.8780 - val_loss: 7.0142 - learning_rate: 4.0000e-05\n",
      "Epoch 83/100\n",
      "23/23 - 13s - 545ms/step - loss: 8.8870 - val_loss: 7.0166 - learning_rate: 4.0000e-05\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "23/23 - 13s - 575ms/step - loss: 8.8554 - val_loss: 7.0152 - learning_rate: 4.0000e-05\n",
      "Epoch 85/100\n",
      "23/23 - 14s - 620ms/step - loss: 8.8212 - val_loss: 7.0102 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "23/23 - 13s - 549ms/step - loss: 8.8609 - val_loss: 7.0089 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "23/23 - 13s - 553ms/step - loss: 8.8402 - val_loss: 7.0252 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "23/23 - 13s - 563ms/step - loss: 8.8755 - val_loss: 7.0061 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "23/23 - 13s - 561ms/step - loss: 8.8927 - val_loss: 7.0052 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "23/23 - 13s - 557ms/step - loss: 8.8344 - val_loss: 7.0175 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "23/23 - 13s - 554ms/step - loss: 8.8984 - val_loss: 7.0329 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "23/23 - 13s - 545ms/step - loss: 8.8057 - val_loss: 7.0186 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "23/23 - 13s - 559ms/step - loss: 8.8372 - val_loss: 7.0325 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "23/23 - 14s - 588ms/step - loss: 8.8468 - val_loss: 7.0199 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "23/23 - 13s - 574ms/step - loss: 8.8614 - val_loss: 7.0079 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "23/23 - 13s - 566ms/step - loss: 8.8856 - val_loss: 7.0211 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "23/23 - 13s - 565ms/step - loss: 8.8687 - val_loss: 7.0260 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "23/23 - 13s - 562ms/step - loss: 8.8453 - val_loss: 7.0441 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "23/23 - 13s - 554ms/step - loss: 8.8674 - val_loss: 7.0151 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "23/23 - 13s - 558ms/step - loss: 8.8566 - val_loss: 7.0224 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 - 34s - 2s/step - loss: 315.0599 - val_loss: 209.8589 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "18/18 - 9s - 524ms/step - loss: 173.5293 - val_loss: 131.1723 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "18/18 - 10s - 530ms/step - loss: 118.9223 - val_loss: 95.7045 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "18/18 - 9s - 513ms/step - loss: 91.0563 - val_loss: 78.6340 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "18/18 - 10s - 533ms/step - loss: 76.2143 - val_loss: 70.4566 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "18/18 - 9s - 519ms/step - loss: 67.5351 - val_loss: 60.8821 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "18/18 - 9s - 527ms/step - loss: 61.3009 - val_loss: 56.9859 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "18/18 - 9s - 523ms/step - loss: 57.1805 - val_loss: 49.3910 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "18/18 - 11s - 625ms/step - loss: 51.2683 - val_loss: 49.7976 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "18/18 - 10s - 552ms/step - loss: 49.1056 - val_loss: 48.2411 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "18/18 - 10s - 558ms/step - loss: 46.2806 - val_loss: 45.5435 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "18/18 - 10s - 564ms/step - loss: 43.6844 - val_loss: 39.3225 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "18/18 - 10s - 531ms/step - loss: 42.7628 - val_loss: 37.4387 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "18/18 - 10s - 558ms/step - loss: 40.6946 - val_loss: 36.7352 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "18/18 - 10s - 573ms/step - loss: 39.3625 - val_loss: 35.0105 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "18/18 - 10s - 573ms/step - loss: 43.2497 - val_loss: 34.0610 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "18/18 - 10s - 571ms/step - loss: 40.5669 - val_loss: 34.1687 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "18/18 - 10s - 574ms/step - loss: 36.4539 - val_loss: 34.4911 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "18/18 - 10s - 576ms/step - loss: 35.8731 - val_loss: 33.2377 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "18/18 - 10s - 555ms/step - loss: 35.2486 - val_loss: 31.8906 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "18/18 - 10s - 572ms/step - loss: 34.6104 - val_loss: 30.0937 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "18/18 - 10s - 582ms/step - loss: 34.0508 - val_loss: 30.2035 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "18/18 - 11s - 584ms/step - loss: 33.8536 - val_loss: 30.2026 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "18/18 - 11s - 586ms/step - loss: 33.0284 - val_loss: 28.3640 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "18/18 - 10s - 578ms/step - loss: 35.8440 - val_loss: 30.9842 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "18/18 - 10s - 563ms/step - loss: 32.7478 - val_loss: 28.5653 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "18/18 - 10s - 556ms/step - loss: 33.7089 - val_loss: 31.5553 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "18/18 - 11s - 591ms/step - loss: 31.0676 - val_loss: 30.0905 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "18/18 - 10s - 572ms/step - loss: 31.4683 - val_loss: 28.0647 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "18/18 - 10s - 583ms/step - loss: 31.1300 - val_loss: 26.1095 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "18/18 - 10s - 558ms/step - loss: 30.5013 - val_loss: 26.7458 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "18/18 - 10s - 566ms/step - loss: 29.5293 - val_loss: 27.9407 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "18/18 - 10s - 551ms/step - loss: 29.1346 - val_loss: 30.9152 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "18/18 - 10s - 560ms/step - loss: 29.9169 - val_loss: 29.0983 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "18/18 - 10s - 555ms/step - loss: 29.1388 - val_loss: 25.3019 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "18/18 - 11s - 585ms/step - loss: 28.5766 - val_loss: 26.2318 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "18/18 - 11s - 585ms/step - loss: 28.2573 - val_loss: 26.4918 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "18/18 - 10s - 557ms/step - loss: 28.4821 - val_loss: 24.2630 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "18/18 - 12s - 693ms/step - loss: 27.3403 - val_loss: 27.8934 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "18/18 - 10s - 583ms/step - loss: 27.7132 - val_loss: 25.1290 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "18/18 - 10s - 582ms/step - loss: 26.9342 - val_loss: 23.9933 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "18/18 - 10s - 557ms/step - loss: 27.2856 - val_loss: 25.4784 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "18/18 - 10s - 563ms/step - loss: 26.7147 - val_loss: 23.4988 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "18/18 - 10s - 557ms/step - loss: 26.6346 - val_loss: 25.9771 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "18/18 - 10s - 558ms/step - loss: 27.5211 - val_loss: 28.7642 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "18/18 - 10s - 528ms/step - loss: 26.7594 - val_loss: 23.6740 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "18/18 - 9s - 525ms/step - loss: 26.4474 - val_loss: 23.6134 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "18/18 - 10s - 569ms/step - loss: 26.3759 - val_loss: 22.1844 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "18/18 - 10s - 544ms/step - loss: 36.6728 - val_loss: 27.3276 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "18/18 - 10s - 555ms/step - loss: 26.3727 - val_loss: 25.2229 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "18/18 - 10s - 564ms/step - loss: 26.6594 - val_loss: 26.9053 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "18/18 - 10s - 559ms/step - loss: 26.4756 - val_loss: 23.2425 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "18/18 - 10s - 556ms/step - loss: 25.5531 - val_loss: 24.2213 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "18/18 - 10s - 542ms/step - loss: 25.4340 - val_loss: 22.5423 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "18/18 - 10s - 528ms/step - loss: 25.2417 - val_loss: 24.9400 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "18/18 - 10s - 538ms/step - loss: 26.0105 - val_loss: 21.5027 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "18/18 - 10s - 545ms/step - loss: 27.2830 - val_loss: 26.5784 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "18/18 - 10s - 536ms/step - loss: 25.2679 - val_loss: 22.2442 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "18/18 - 10s - 535ms/step - loss: 25.0418 - val_loss: 22.7431 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "18/18 - 10s - 552ms/step - loss: 24.8423 - val_loss: 22.7931 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "18/18 - 10s - 556ms/step - loss: 24.8028 - val_loss: 22.5529 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "18/18 - 10s - 562ms/step - loss: 24.6409 - val_loss: 22.1357 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "18/18 - 10s - 545ms/step - loss: 24.5493 - val_loss: 22.3641 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "18/18 - 10s - 532ms/step - loss: 24.4894 - val_loss: 22.1012 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "18/18 - 10s - 548ms/step - loss: 24.6082 - val_loss: 22.5055 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "18/18 - 10s - 552ms/step - loss: 24.3256 - val_loss: 22.2460 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "18/18 - 10s - 565ms/step - loss: 24.1206 - val_loss: 22.6671 - learning_rate: 2.0000e-04\n",
      "Epoch 68/100\n",
      "18/18 - 10s - 555ms/step - loss: 24.3512 - val_loss: 22.4781 - learning_rate: 2.0000e-04\n",
      "Epoch 69/100\n",
      "18/18 - 10s - 549ms/step - loss: 24.3945 - val_loss: 23.0680 - learning_rate: 2.0000e-04\n",
      "Epoch 70/100\n",
      "18/18 - 12s - 645ms/step - loss: 24.3430 - val_loss: 22.2367 - learning_rate: 2.0000e-04\n",
      "Epoch 71/100\n",
      "18/18 - 10s - 531ms/step - loss: 24.2580 - val_loss: 22.1562 - learning_rate: 2.0000e-04\n",
      "Epoch 72/100\n",
      "18/18 - 9s - 521ms/step - loss: 24.3649 - val_loss: 22.2674 - learning_rate: 2.0000e-04\n",
      "Epoch 73/100\n",
      "18/18 - 9s - 527ms/step - loss: 24.0968 - val_loss: 21.9447 - learning_rate: 2.0000e-04\n",
      "Epoch 74/100\n",
      "18/18 - 10s - 537ms/step - loss: 24.2046 - val_loss: 21.9174 - learning_rate: 2.0000e-04\n",
      "Epoch 75/100\n",
      "18/18 - 10s - 552ms/step - loss: 24.2112 - val_loss: 22.5525 - learning_rate: 2.0000e-04\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "18/18 - 10s - 552ms/step - loss: 24.2411 - val_loss: 22.3012 - learning_rate: 2.0000e-04\n",
      "Epoch 77/100\n",
      "18/18 - 10s - 544ms/step - loss: 24.2371 - val_loss: 22.1974 - learning_rate: 4.0000e-05\n",
      "Epoch 78/100\n",
      "18/18 - 10s - 555ms/step - loss: 24.1656 - val_loss: 22.1253 - learning_rate: 4.0000e-05\n",
      "Epoch 79/100\n",
      "18/18 - 10s - 547ms/step - loss: 24.2124 - val_loss: 22.1750 - learning_rate: 4.0000e-05\n",
      "Epoch 80/100\n",
      "18/18 - 10s - 529ms/step - loss: 24.2670 - val_loss: 22.0897 - learning_rate: 4.0000e-05\n",
      "Epoch 81/100\n",
      "18/18 - 10s - 535ms/step - loss: 24.1236 - val_loss: 22.2155 - learning_rate: 4.0000e-05\n",
      "Epoch 82/100\n",
      "18/18 - 10s - 539ms/step - loss: 24.1624 - val_loss: 22.2467 - learning_rate: 4.0000e-05\n",
      "Epoch 83/100\n",
      "18/18 - 10s - 545ms/step - loss: 24.0557 - val_loss: 22.1662 - learning_rate: 4.0000e-05\n",
      "Epoch 84/100\n",
      "18/18 - 10s - 552ms/step - loss: 24.2365 - val_loss: 22.1674 - learning_rate: 4.0000e-05\n",
      "Epoch 85/100\n",
      "18/18 - 10s - 548ms/step - loss: 24.1047 - val_loss: 22.1212 - learning_rate: 4.0000e-05\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "18/18 - 10s - 550ms/step - loss: 24.0646 - val_loss: 22.1347 - learning_rate: 4.0000e-05\n",
      "Epoch 87/100\n",
      "18/18 - 10s - 532ms/step - loss: 23.9496 - val_loss: 22.1569 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "18/18 - 9s - 525ms/step - loss: 24.0221 - val_loss: 22.1293 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "18/18 - 10s - 537ms/step - loss: 24.0504 - val_loss: 22.0568 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "18/18 - 10s - 568ms/step - loss: 24.0389 - val_loss: 22.0611 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "18/18 - 10s - 555ms/step - loss: 23.9727 - val_loss: 22.1935 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "18/18 - 10s - 555ms/step - loss: 24.0705 - val_loss: 22.2106 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "18/18 - 10s - 572ms/step - loss: 24.1560 - val_loss: 22.1897 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "18/18 - 10s - 552ms/step - loss: 23.9813 - val_loss: 22.2330 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "18/18 - 10s - 541ms/step - loss: 23.9975 - val_loss: 22.2046 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "18/18 - 10s - 535ms/step - loss: 23.9816 - val_loss: 22.2086 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "18/18 - 10s - 539ms/step - loss: 24.1127 - val_loss: 22.1911 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "18/18 - 10s - 540ms/step - loss: 24.0584 - val_loss: 22.1662 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "18/18 - 10s - 549ms/step - loss: 24.1529 - val_loss: 22.1562 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "18/18 - 10s - 550ms/step - loss: 24.1717 - val_loss: 22.1587 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 21'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 - 26s - 2s/step - loss: 313.1308 - val_loss: 203.4508 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "15/15 - 8s - 525ms/step - loss: 164.9872 - val_loss: 116.6094 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "15/15 - 8s - 522ms/step - loss: 100.8119 - val_loss: 75.1241 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "15/15 - 8s - 519ms/step - loss: 71.3803 - val_loss: 56.8478 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "15/15 - 8s - 553ms/step - loss: 57.1321 - val_loss: 46.2267 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "15/15 - 8s - 555ms/step - loss: 47.4480 - val_loss: 38.9548 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "15/15 - 8s - 521ms/step - loss: 42.0531 - val_loss: 34.3829 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "15/15 - 8s - 521ms/step - loss: 38.7330 - val_loss: 32.1807 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "15/15 - 8s - 525ms/step - loss: 35.0498 - val_loss: 28.9236 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "15/15 - 8s - 555ms/step - loss: 32.8760 - val_loss: 26.6723 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "15/15 - 9s - 574ms/step - loss: 31.0442 - val_loss: 24.4457 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "15/15 - 9s - 597ms/step - loss: 29.8188 - val_loss: 22.9986 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "15/15 - 9s - 593ms/step - loss: 28.8196 - val_loss: 21.8437 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "15/15 - 9s - 596ms/step - loss: 27.3113 - val_loss: 21.1019 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "15/15 - 9s - 585ms/step - loss: 27.0429 - val_loss: 19.9953 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "15/15 - 9s - 574ms/step - loss: 26.1834 - val_loss: 19.4772 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "15/15 - 9s - 582ms/step - loss: 24.7963 - val_loss: 18.8832 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "15/15 - 9s - 581ms/step - loss: 24.3416 - val_loss: 18.0987 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "15/15 - 9s - 577ms/step - loss: 23.6405 - val_loss: 17.4374 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "15/15 - 9s - 586ms/step - loss: 23.2257 - val_loss: 16.8337 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "15/15 - 9s - 574ms/step - loss: 22.5877 - val_loss: 16.8500 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "15/15 - 9s - 568ms/step - loss: 22.7808 - val_loss: 15.9725 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "15/15 - 8s - 564ms/step - loss: 21.7719 - val_loss: 15.6053 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "15/15 - 9s - 584ms/step - loss: 21.3467 - val_loss: 15.1305 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "15/15 - 8s - 565ms/step - loss: 20.7404 - val_loss: 14.8373 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "15/15 - 8s - 567ms/step - loss: 20.4804 - val_loss: 14.6310 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "15/15 - 10s - 646ms/step - loss: 20.3584 - val_loss: 15.6953 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "15/15 - 9s - 610ms/step - loss: 20.1789 - val_loss: 13.9905 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "15/15 - 9s - 596ms/step - loss: 19.5455 - val_loss: 13.7851 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "15/15 - 9s - 610ms/step - loss: 19.7918 - val_loss: 13.3708 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "15/15 - 9s - 615ms/step - loss: 19.0616 - val_loss: 13.6941 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "15/15 - 10s - 659ms/step - loss: 19.4042 - val_loss: 13.1231 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "15/15 - 10s - 659ms/step - loss: 19.0888 - val_loss: 12.7535 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "15/15 - 11s - 713ms/step - loss: 18.4051 - val_loss: 12.5964 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "15/15 - 9s - 606ms/step - loss: 18.5328 - val_loss: 12.8102 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "15/15 - 9s - 595ms/step - loss: 18.9073 - val_loss: 12.2023 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "15/15 - 9s - 602ms/step - loss: 17.9129 - val_loss: 12.2483 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "15/15 - 9s - 580ms/step - loss: 17.8483 - val_loss: 11.9500 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "15/15 - 9s - 585ms/step - loss: 17.8200 - val_loss: 11.7343 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "15/15 - 9s - 595ms/step - loss: 17.3126 - val_loss: 11.9614 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "15/15 - 9s - 599ms/step - loss: 17.1643 - val_loss: 11.5102 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "15/15 - 9s - 599ms/step - loss: 17.1095 - val_loss: 11.6129 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "15/15 - 9s - 602ms/step - loss: 16.8989 - val_loss: 11.2324 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "15/15 - 9s - 606ms/step - loss: 16.9077 - val_loss: 11.1004 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "15/15 - 9s - 622ms/step - loss: 16.7821 - val_loss: 11.2870 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "15/15 - 9s - 598ms/step - loss: 16.9184 - val_loss: 11.2605 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "15/15 - 9s - 602ms/step - loss: 17.2785 - val_loss: 11.3699 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "15/15 - 10s - 634ms/step - loss: 16.8938 - val_loss: 10.6869 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "15/15 - 9s - 616ms/step - loss: 16.7950 - val_loss: 10.5600 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "15/15 - 9s - 621ms/step - loss: 16.4796 - val_loss: 10.4796 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "15/15 - 9s - 600ms/step - loss: 16.2884 - val_loss: 10.3721 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "15/15 - 9s - 626ms/step - loss: 16.2646 - val_loss: 10.8207 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "15/15 - 9s - 584ms/step - loss: 16.0274 - val_loss: 10.2372 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "15/15 - 9s - 602ms/step - loss: 15.8940 - val_loss: 10.1443 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "15/15 - 9s - 594ms/step - loss: 15.9963 - val_loss: 10.2769 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "15/15 - 9s - 602ms/step - loss: 15.6788 - val_loss: 10.0276 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "15/15 - 9s - 625ms/step - loss: 15.8075 - val_loss: 9.9972 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "15/15 - 9s - 595ms/step - loss: 15.5996 - val_loss: 10.3617 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "15/15 - 9s - 595ms/step - loss: 15.5635 - val_loss: 9.8927 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "15/15 - 9s - 596ms/step - loss: 15.4974 - val_loss: 9.8142 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "15/15 - 9s - 594ms/step - loss: 15.5421 - val_loss: 9.6541 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "15/15 - 9s - 621ms/step - loss: 15.4891 - val_loss: 10.0257 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "15/15 - 9s - 602ms/step - loss: 15.4178 - val_loss: 9.5488 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "15/15 - 9s - 593ms/step - loss: 15.2614 - val_loss: 9.5994 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "15/15 - 9s - 570ms/step - loss: 15.1832 - val_loss: 9.6805 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "15/15 - 9s - 573ms/step - loss: 15.2907 - val_loss: 9.4265 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "15/15 - 9s - 592ms/step - loss: 15.3231 - val_loss: 9.3896 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "15/15 - 9s - 594ms/step - loss: 15.2107 - val_loss: 9.4027 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "15/15 - 11s - 709ms/step - loss: 14.9255 - val_loss: 9.2917 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "15/15 - 9s - 594ms/step - loss: 15.0580 - val_loss: 9.5271 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "15/15 - 9s - 599ms/step - loss: 14.9285 - val_loss: 9.3444 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "15/15 - 10s - 648ms/step - loss: 14.8675 - val_loss: 9.2458 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "15/15 - 10s - 650ms/step - loss: 14.7802 - val_loss: 9.3992 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "15/15 - 9s - 597ms/step - loss: 14.9607 - val_loss: 9.3409 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "15/15 - 9s - 607ms/step - loss: 14.7789 - val_loss: 9.0451 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "15/15 - 9s - 595ms/step - loss: 14.7091 - val_loss: 9.0971 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "15/15 - 9s - 593ms/step - loss: 14.6392 - val_loss: 9.3559 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "15/15 - 9s - 584ms/step - loss: 14.6940 - val_loss: 8.9852 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "15/15 - 10s - 634ms/step - loss: 15.4994 - val_loss: 9.9370 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "15/15 - 10s - 690ms/step - loss: 14.5525 - val_loss: 8.9796 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "15/15 - 9s - 584ms/step - loss: 15.2869 - val_loss: 8.9551 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "15/15 - 10s - 688ms/step - loss: 14.6273 - val_loss: 8.8528 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "15/15 - 9s - 630ms/step - loss: 14.9179 - val_loss: 9.2608 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "15/15 - 9s - 606ms/step - loss: 14.5199 - val_loss: 8.7962 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "15/15 - 9s - 602ms/step - loss: 14.6311 - val_loss: 9.5115 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "15/15 - 9s - 621ms/step - loss: 14.4378 - val_loss: 8.7464 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "15/15 - 10s - 644ms/step - loss: 14.4093 - val_loss: 9.0079 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "15/15 - 10s - 639ms/step - loss: 14.7094 - val_loss: 9.2798 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "15/15 - 9s - 614ms/step - loss: 14.5959 - val_loss: 8.6913 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "15/15 - 9s - 583ms/step - loss: 14.6681 - val_loss: 8.6549 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "15/15 - 9s - 576ms/step - loss: 14.4065 - val_loss: 8.6422 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "15/15 - 10s - 686ms/step - loss: 14.4573 - val_loss: 8.9199 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "15/15 - 9s - 624ms/step - loss: 14.2963 - val_loss: 8.7132 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "15/15 - 10s - 649ms/step - loss: 14.4312 - val_loss: 9.2527 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "15/15 - 9s - 632ms/step - loss: 14.3686 - val_loss: 8.6575 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "15/15 - 9s - 579ms/step - loss: 14.2640 - val_loss: 8.5923 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "15/15 - 8s - 555ms/step - loss: 14.2458 - val_loss: 8.5662 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "15/15 - 8s - 563ms/step - loss: 14.2725 - val_loss: 8.6579 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "15/15 - 8s - 565ms/step - loss: 14.2188 - val_loss: 8.4861 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "15/15 - 9s - 572ms/step - loss: 14.2441 - val_loss: 8.5719 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 22'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 - 39s - 3s/step - loss: 316.6126 - val_loss: 215.0019 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "14/14 - 8s - 572ms/step - loss: 174.4536 - val_loss: 130.2061 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "14/14 - 8s - 548ms/step - loss: 110.8977 - val_loss: 86.9969 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "14/14 - 8s - 565ms/step - loss: 78.9551 - val_loss: 65.2856 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "14/14 - 8s - 553ms/step - loss: 62.2712 - val_loss: 53.0544 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "14/14 - 8s - 569ms/step - loss: 52.9074 - val_loss: 45.6548 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "14/14 - 8s - 555ms/step - loss: 46.2713 - val_loss: 40.0426 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "14/14 - 9s - 609ms/step - loss: 40.7901 - val_loss: 36.2546 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "14/14 - 8s - 570ms/step - loss: 38.0185 - val_loss: 32.9884 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "14/14 - 8s - 586ms/step - loss: 34.9589 - val_loss: 30.5881 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "14/14 - 9s - 610ms/step - loss: 32.8082 - val_loss: 28.8422 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "14/14 - 8s - 595ms/step - loss: 30.8864 - val_loss: 27.2173 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "14/14 - 9s - 620ms/step - loss: 29.5014 - val_loss: 25.7722 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "14/14 - 9s - 633ms/step - loss: 28.1858 - val_loss: 26.3895 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "14/14 - 10s - 723ms/step - loss: 27.4167 - val_loss: 23.5621 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "14/14 - 10s - 707ms/step - loss: 25.8437 - val_loss: 22.4763 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "14/14 - 10s - 680ms/step - loss: 25.1933 - val_loss: 21.9144 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "14/14 - 9s - 674ms/step - loss: 24.3979 - val_loss: 21.6819 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "14/14 - 10s - 685ms/step - loss: 23.4313 - val_loss: 20.2042 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "14/14 - 8s - 601ms/step - loss: 22.8908 - val_loss: 20.2046 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "14/14 - 8s - 572ms/step - loss: 22.3358 - val_loss: 19.4071 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "14/14 - 8s - 575ms/step - loss: 21.6386 - val_loss: 18.4378 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "14/14 - 8s - 580ms/step - loss: 21.1807 - val_loss: 17.9690 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "14/14 - 8s - 596ms/step - loss: 21.2555 - val_loss: 17.5161 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "14/14 - 8s - 586ms/step - loss: 20.4506 - val_loss: 17.0901 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "14/14 - 8s - 577ms/step - loss: 19.8969 - val_loss: 16.7805 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "14/14 - 8s - 572ms/step - loss: 19.6164 - val_loss: 18.3318 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "14/14 - 8s - 591ms/step - loss: 19.5376 - val_loss: 16.5424 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "14/14 - 9s - 616ms/step - loss: 18.8106 - val_loss: 15.7575 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "14/14 - 8s - 602ms/step - loss: 18.5983 - val_loss: 15.9267 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "14/14 - 8s - 595ms/step - loss: 18.1396 - val_loss: 16.0186 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "14/14 - 8s - 590ms/step - loss: 18.3892 - val_loss: 15.8502 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "14/14 - 8s - 586ms/step - loss: 18.7334 - val_loss: 14.7921 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "14/14 - 10s - 718ms/step - loss: 18.3494 - val_loss: 14.6452 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "14/14 - 8s - 566ms/step - loss: 17.2460 - val_loss: 14.2717 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "14/14 - 8s - 578ms/step - loss: 16.8577 - val_loss: 13.9318 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "14/14 - 9s - 633ms/step - loss: 16.4916 - val_loss: 13.7410 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "14/14 - 9s - 629ms/step - loss: 16.4248 - val_loss: 13.8543 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "14/14 - 8s - 593ms/step - loss: 16.2523 - val_loss: 13.3824 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "14/14 - 9s - 650ms/step - loss: 16.1184 - val_loss: 13.3393 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "14/14 - 9s - 640ms/step - loss: 16.0360 - val_loss: 13.2357 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "14/14 - 9s - 626ms/step - loss: 16.1958 - val_loss: 12.9887 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "14/14 - 8s - 605ms/step - loss: 16.1622 - val_loss: 12.6824 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "14/14 - 8s - 604ms/step - loss: 15.5701 - val_loss: 12.6843 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "14/14 - 8s - 596ms/step - loss: 15.6708 - val_loss: 12.5643 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "14/14 - 8s - 595ms/step - loss: 15.3664 - val_loss: 12.7530 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "14/14 - 8s - 595ms/step - loss: 15.2192 - val_loss: 12.5323 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "14/14 - 9s - 622ms/step - loss: 14.9283 - val_loss: 12.0422 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "14/14 - 9s - 621ms/step - loss: 14.5913 - val_loss: 12.4708 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "14/14 - 9s - 632ms/step - loss: 14.7418 - val_loss: 11.7611 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "14/14 - 9s - 638ms/step - loss: 14.6093 - val_loss: 11.7269 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "14/14 - 8s - 604ms/step - loss: 14.3499 - val_loss: 11.6149 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "14/14 - 8s - 603ms/step - loss: 14.1469 - val_loss: 11.5299 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "14/14 - 9s - 624ms/step - loss: 14.2121 - val_loss: 11.3488 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "14/14 - 9s - 616ms/step - loss: 14.2521 - val_loss: 12.1064 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "14/14 - 9s - 609ms/step - loss: 14.3458 - val_loss: 11.2756 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "14/14 - 8s - 603ms/step - loss: 14.1313 - val_loss: 11.5409 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "14/14 - 9s - 648ms/step - loss: 14.1449 - val_loss: 11.0113 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "14/14 - 9s - 632ms/step - loss: 13.8053 - val_loss: 11.1960 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "14/14 - 9s - 629ms/step - loss: 13.7463 - val_loss: 11.0663 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "14/14 - 9s - 641ms/step - loss: 13.4834 - val_loss: 10.8246 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "14/14 - 9s - 623ms/step - loss: 13.6163 - val_loss: 10.6770 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "14/14 - 9s - 629ms/step - loss: 13.6372 - val_loss: 10.7070 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "14/14 - 9s - 625ms/step - loss: 13.6129 - val_loss: 10.6053 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "14/14 - 9s - 617ms/step - loss: 13.4146 - val_loss: 10.4777 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "14/14 - 9s - 622ms/step - loss: 13.2259 - val_loss: 10.6280 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "14/14 - 8s - 598ms/step - loss: 13.2199 - val_loss: 10.4941 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "14/14 - 9s - 614ms/step - loss: 13.3530 - val_loss: 11.0381 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "14/14 - 9s - 611ms/step - loss: 13.6653 - val_loss: 10.3034 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "14/14 - 10s - 743ms/step - loss: 13.4190 - val_loss: 10.2046 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "14/14 - 10s - 693ms/step - loss: 13.0592 - val_loss: 10.2704 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "14/14 - 9s - 650ms/step - loss: 12.9859 - val_loss: 10.5938 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "14/14 - 9s - 632ms/step - loss: 12.7503 - val_loss: 10.1559 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "14/14 - 9s - 618ms/step - loss: 13.0388 - val_loss: 9.9860 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "14/14 - 9s - 620ms/step - loss: 12.6525 - val_loss: 9.9789 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "14/14 - 8s - 590ms/step - loss: 12.8401 - val_loss: 9.9533 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "14/14 - 9s - 626ms/step - loss: 12.7052 - val_loss: 9.9124 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "14/14 - 8s - 603ms/step - loss: 12.5291 - val_loss: 9.8819 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "14/14 - 9s - 611ms/step - loss: 12.5906 - val_loss: 9.9419 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "14/14 - 9s - 616ms/step - loss: 13.0054 - val_loss: 10.3537 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "14/14 - 9s - 637ms/step - loss: 12.9053 - val_loss: 10.0331 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "14/14 - 9s - 645ms/step - loss: 12.8605 - val_loss: 10.0497 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "14/14 - 9s - 637ms/step - loss: 12.8858 - val_loss: 9.6278 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "14/14 - 9s - 624ms/step - loss: 13.0147 - val_loss: 10.1747 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "14/14 - 9s - 611ms/step - loss: 12.8788 - val_loss: 9.6274 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "14/14 - 9s - 650ms/step - loss: 12.2983 - val_loss: 9.6430 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "14/14 - 9s - 619ms/step - loss: 12.2841 - val_loss: 9.5266 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "14/14 - 9s - 608ms/step - loss: 12.2586 - val_loss: 9.6000 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "14/14 - 8s - 603ms/step - loss: 12.2309 - val_loss: 9.4323 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "14/14 - 8s - 602ms/step - loss: 12.3725 - val_loss: 9.4064 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "14/14 - 8s - 592ms/step - loss: 12.2859 - val_loss: 9.3884 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "14/14 - 8s - 605ms/step - loss: 12.0526 - val_loss: 9.4133 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "14/14 - 8s - 599ms/step - loss: 12.0695 - val_loss: 9.4177 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "14/14 - 8s - 604ms/step - loss: 12.6841 - val_loss: 10.3774 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "14/14 - 8s - 597ms/step - loss: 12.5572 - val_loss: 9.2793 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "14/14 - 9s - 624ms/step - loss: 12.2116 - val_loss: 9.2766 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "14/14 - 8s - 598ms/step - loss: 12.1224 - val_loss: 9.6010 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "14/14 - 8s - 599ms/step - loss: 12.0507 - val_loss: 9.3019 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "14/14 - 8s - 600ms/step - loss: 11.9438 - val_loss: 9.2652 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "14/14 - 8s - 598ms/step - loss: 11.7620 - val_loss: 9.1724 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 23'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 - 36s - 1s/step - loss: 222.8528 - val_loss: 106.6733 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "25/25 - 14s - 575ms/step - loss: 70.5912 - val_loss: 40.0183 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "25/25 - 14s - 551ms/step - loss: 34.3163 - val_loss: 23.3731 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "25/25 - 14s - 543ms/step - loss: 24.2721 - val_loss: 17.5190 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "25/25 - 14s - 544ms/step - loss: 19.8548 - val_loss: 14.3136 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "25/25 - 14s - 567ms/step - loss: 17.2532 - val_loss: 12.1016 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "25/25 - 15s - 611ms/step - loss: 15.2902 - val_loss: 10.5885 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "25/25 - 16s - 636ms/step - loss: 14.1722 - val_loss: 9.4615 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "25/25 - 16s - 642ms/step - loss: 12.8947 - val_loss: 8.5538 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "25/25 - 15s - 620ms/step - loss: 12.2277 - val_loss: 7.8810 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "25/25 - 15s - 602ms/step - loss: 11.5132 - val_loss: 7.4626 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "25/25 - 16s - 637ms/step - loss: 10.9861 - val_loss: 6.9089 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "25/25 - 15s - 619ms/step - loss: 10.9596 - val_loss: 6.8856 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "25/25 - 15s - 602ms/step - loss: 10.4722 - val_loss: 6.2320 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "25/25 - 15s - 601ms/step - loss: 10.0896 - val_loss: 6.1332 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "25/25 - 15s - 595ms/step - loss: 9.6787 - val_loss: 6.0478 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "25/25 - 16s - 623ms/step - loss: 9.7060 - val_loss: 5.5932 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "25/25 - 16s - 646ms/step - loss: 9.2776 - val_loss: 5.3952 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "25/25 - 15s - 604ms/step - loss: 9.2764 - val_loss: 5.2666 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "25/25 - 15s - 593ms/step - loss: 9.0012 - val_loss: 5.4202 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "25/25 - 15s - 584ms/step - loss: 9.0929 - val_loss: 5.0566 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "25/25 - 16s - 657ms/step - loss: 8.8678 - val_loss: 4.9655 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "25/25 - 15s - 593ms/step - loss: 8.7103 - val_loss: 4.9300 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "25/25 - 15s - 606ms/step - loss: 8.7227 - val_loss: 4.8586 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "25/25 - 15s - 613ms/step - loss: 8.6713 - val_loss: 4.7584 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "25/25 - 15s - 602ms/step - loss: 8.5408 - val_loss: 4.6868 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "25/25 - 15s - 589ms/step - loss: 8.4587 - val_loss: 4.6741 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "25/25 - 15s - 599ms/step - loss: 8.4476 - val_loss: 4.5983 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "25/25 - 15s - 596ms/step - loss: 8.5088 - val_loss: 4.5661 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "25/25 - 16s - 632ms/step - loss: 8.3952 - val_loss: 4.5251 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "25/25 - 15s - 581ms/step - loss: 8.3554 - val_loss: 4.4978 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "25/25 - 15s - 589ms/step - loss: 8.3084 - val_loss: 4.5142 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "25/25 - 14s - 575ms/step - loss: 8.2853 - val_loss: 4.4437 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "25/25 - 15s - 586ms/step - loss: 8.2622 - val_loss: 4.4150 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "25/25 - 14s - 575ms/step - loss: 8.2411 - val_loss: 4.4183 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "25/25 - 15s - 594ms/step - loss: 8.2701 - val_loss: 4.4137 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "25/25 - 15s - 587ms/step - loss: 8.2392 - val_loss: 4.3654 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "25/25 - 14s - 559ms/step - loss: 8.1745 - val_loss: 4.3829 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "25/25 - 14s - 552ms/step - loss: 8.1826 - val_loss: 4.3492 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "25/25 - 14s - 566ms/step - loss: 8.1076 - val_loss: 4.3143 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "25/25 - 14s - 569ms/step - loss: 8.1183 - val_loss: 4.3005 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "25/25 - 14s - 571ms/step - loss: 8.0993 - val_loss: 4.2921 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "25/25 - 16s - 623ms/step - loss: 8.1066 - val_loss: 4.3367 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "25/25 - 14s - 569ms/step - loss: 8.1429 - val_loss: 4.2743 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "25/25 - 14s - 569ms/step - loss: 8.0813 - val_loss: 4.2700 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "25/25 - 15s - 585ms/step - loss: 8.1174 - val_loss: 4.3598 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "25/25 - 14s - 570ms/step - loss: 8.1126 - val_loss: 4.2485 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "25/25 - 14s - 562ms/step - loss: 8.0703 - val_loss: 4.2395 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "25/25 - 14s - 573ms/step - loss: 8.0412 - val_loss: 4.2349 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "25/25 - 14s - 550ms/step - loss: 8.0292 - val_loss: 4.2230 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "25/25 - 15s - 588ms/step - loss: 8.0149 - val_loss: 4.2288 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "25/25 - 16s - 622ms/step - loss: 8.0587 - val_loss: 4.2274 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "25/25 - 15s - 585ms/step - loss: 8.0721 - val_loss: 4.2298 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "25/25 - 14s - 580ms/step - loss: 8.0517 - val_loss: 4.2038 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "25/25 - 15s - 584ms/step - loss: 8.0193 - val_loss: 4.2243 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "25/25 - 14s - 557ms/step - loss: 8.0115 - val_loss: 4.1992 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "25/25 - 14s - 574ms/step - loss: 8.0240 - val_loss: 4.1906 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "25/25 - 14s - 568ms/step - loss: 8.0122 - val_loss: 4.2046 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "25/25 - 14s - 551ms/step - loss: 7.9896 - val_loss: 4.1838 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "25/25 - 14s - 548ms/step - loss: 7.9987 - val_loss: 4.1887 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "25/25 - 14s - 551ms/step - loss: 7.9841 - val_loss: 4.1784 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "25/25 - 14s - 563ms/step - loss: 7.9981 - val_loss: 4.1851 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "25/25 - 14s - 564ms/step - loss: 7.9984 - val_loss: 4.1847 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "25/25 - 16s - 657ms/step - loss: 8.0011 - val_loss: 4.1957 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "25/25 - 14s - 566ms/step - loss: 7.9863 - val_loss: 4.1943 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "25/25 - 14s - 567ms/step - loss: 8.0262 - val_loss: 4.1681 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "25/25 - 14s - 567ms/step - loss: 7.9653 - val_loss: 4.1659 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "25/25 - 15s - 591ms/step - loss: 7.9864 - val_loss: 4.1645 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "25/25 - 14s - 579ms/step - loss: 7.9732 - val_loss: 4.1628 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "25/25 - 15s - 585ms/step - loss: 7.9958 - val_loss: 4.1671 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "25/25 - 14s - 559ms/step - loss: 7.9556 - val_loss: 4.1606 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "25/25 - 14s - 572ms/step - loss: 7.9730 - val_loss: 4.1608 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "25/25 - 16s - 623ms/step - loss: 7.9614 - val_loss: 4.1628 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "25/25 - 16s - 635ms/step - loss: 7.9781 - val_loss: 4.1744 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "25/25 - 15s - 586ms/step - loss: 7.9722 - val_loss: 4.1740 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "25/25 - 15s - 583ms/step - loss: 7.9642 - val_loss: 4.1526 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "25/25 - 14s - 575ms/step - loss: 7.9497 - val_loss: 4.1537 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "25/25 - 14s - 578ms/step - loss: 7.9785 - val_loss: 4.1600 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "25/25 - 14s - 571ms/step - loss: 7.9747 - val_loss: 4.1522 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "25/25 - 15s - 591ms/step - loss: 7.9708 - val_loss: 4.1521 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "25/25 - 15s - 584ms/step - loss: 7.9788 - val_loss: 4.1528 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "25/25 - 15s - 593ms/step - loss: 8.0054 - val_loss: 4.1631 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "25/25 - 14s - 573ms/step - loss: 7.9633 - val_loss: 4.1468 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "25/25 - 14s - 568ms/step - loss: 7.9534 - val_loss: 4.1480 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "25/25 - 16s - 650ms/step - loss: 7.9597 - val_loss: 4.1465 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "25/25 - 14s - 570ms/step - loss: 7.9556 - val_loss: 4.1450 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "25/25 - 14s - 561ms/step - loss: 7.9622 - val_loss: 4.1444 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "25/25 - 14s - 580ms/step - loss: 7.9553 - val_loss: 4.1440 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "25/25 - 15s - 588ms/step - loss: 7.9530 - val_loss: 4.1492 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "25/25 - 14s - 570ms/step - loss: 7.9664 - val_loss: 4.1508 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "25/25 - 14s - 559ms/step - loss: 7.9565 - val_loss: 4.1431 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "25/25 - 14s - 567ms/step - loss: 7.9452 - val_loss: 4.1432 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "25/25 - 15s - 592ms/step - loss: 7.9527 - val_loss: 4.1489 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "25/25 - 15s - 605ms/step - loss: 7.9640 - val_loss: 4.1455 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "25/25 - 15s - 603ms/step - loss: 7.9796 - val_loss: 4.1552 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "25/25 - 15s - 584ms/step - loss: 7.9537 - val_loss: 4.1437 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "25/25 - 15s - 592ms/step - loss: 7.9522 - val_loss: 4.1583 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "25/25 - 14s - 579ms/step - loss: 7.9631 - val_loss: 4.1498 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "25/25 - 15s - 593ms/step - loss: 7.9379 - val_loss: 4.1442 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "25/25 - 15s - 597ms/step - loss: 7.9526 - val_loss: 4.1414 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 24'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57/57 - 47s - 827ms/step - loss: 123.3347 - val_loss: 26.2545 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 - 33s - 582ms/step - loss: 15.8475 - val_loss: 9.2146 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 - 31s - 546ms/step - loss: 7.3575 - val_loss: 4.8624 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 - 32s - 557ms/step - loss: 4.5736 - val_loss: 3.0886 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 - 34s - 594ms/step - loss: 3.2581 - val_loss: 2.1437 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 - 34s - 596ms/step - loss: 2.5690 - val_loss: 1.6820 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 - 33s - 583ms/step - loss: 2.1289 - val_loss: 1.3075 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 - 32s - 566ms/step - loss: 1.9024 - val_loss: 1.1271 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 - 33s - 573ms/step - loss: 1.7644 - val_loss: 0.9772 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 - 33s - 573ms/step - loss: 1.6060 - val_loss: 0.8883 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 - 33s - 571ms/step - loss: 1.5962 - val_loss: 0.8379 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 - 36s - 623ms/step - loss: 1.4813 - val_loss: 0.7830 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 - 33s - 579ms/step - loss: 1.4673 - val_loss: 0.8901 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 - 33s - 575ms/step - loss: 1.4147 - val_loss: 0.7287 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 - 33s - 572ms/step - loss: 1.3999 - val_loss: 0.7161 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 - 34s - 589ms/step - loss: 1.3722 - val_loss: 0.7207 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 - 32s - 563ms/step - loss: 1.3780 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 - 32s - 566ms/step - loss: 1.3554 - val_loss: 0.6729 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 - 32s - 568ms/step - loss: 1.3483 - val_loss: 0.6647 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 - 33s - 571ms/step - loss: 1.3362 - val_loss: 0.6711 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 - 34s - 605ms/step - loss: 1.3352 - val_loss: 0.6599 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 - 32s - 564ms/step - loss: 1.3383 - val_loss: 0.6548 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 - 32s - 569ms/step - loss: 1.3300 - val_loss: 0.6725 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 - 35s - 607ms/step - loss: 1.3286 - val_loss: 0.6516 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 - 33s - 572ms/step - loss: 1.3204 - val_loss: 0.6486 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 - 33s - 574ms/step - loss: 1.3208 - val_loss: 0.6491 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 - 34s - 590ms/step - loss: 1.3239 - val_loss: 0.6714 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 - 33s - 587ms/step - loss: 1.3275 - val_loss: 0.6825 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 - 33s - 576ms/step - loss: 1.3342 - val_loss: 0.6533 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 - 32s - 559ms/step - loss: 1.3160 - val_loss: 0.6568 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 - 34s - 600ms/step - loss: 1.3200 - val_loss: 0.6511 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 - 32s - 567ms/step - loss: 1.3355 - val_loss: 0.6524 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 - 35s - 620ms/step - loss: 1.3345 - val_loss: 0.6460 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 - 33s - 584ms/step - loss: 1.3300 - val_loss: 0.6437 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "57/57 - 32s - 558ms/step - loss: 1.3181 - val_loss: 0.6443 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "57/57 - 33s - 582ms/step - loss: 1.3232 - val_loss: 0.6434 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "57/57 - 33s - 571ms/step - loss: 1.3129 - val_loss: 0.6885 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "57/57 - 33s - 584ms/step - loss: 1.3274 - val_loss: 0.6432 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "57/57 - 32s - 561ms/step - loss: 1.3104 - val_loss: 0.6856 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "57/57 - 33s - 586ms/step - loss: 1.3173 - val_loss: 0.7451 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "57/57 - 32s - 569ms/step - loss: 1.3326 - val_loss: 0.6655 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "57/57 - 33s - 570ms/step - loss: 1.3223 - val_loss: 0.6479 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "57/57 - 32s - 565ms/step - loss: 1.3147 - val_loss: 0.6442 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "57/57 - 31s - 552ms/step - loss: 1.3155 - val_loss: 0.6446 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "57/57 - 32s - 566ms/step - loss: 1.3232 - val_loss: 0.6471 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "57/57 - 33s - 575ms/step - loss: 1.3169 - val_loss: 0.6434 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "57/57 - 32s - 562ms/step - loss: 1.3165 - val_loss: 0.6480 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "57/57 - 33s - 575ms/step - loss: 1.3141 - val_loss: 0.6431 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "57/57 - 34s - 593ms/step - loss: 1.3157 - val_loss: 0.6445 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "57/57 - 32s - 565ms/step - loss: 1.3162 - val_loss: 0.6590 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "57/57 - 33s - 570ms/step - loss: 1.3240 - val_loss: 0.6434 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "57/57 - 33s - 581ms/step - loss: 1.3181 - val_loss: 0.6433 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "57/57 - 33s - 571ms/step - loss: 1.3161 - val_loss: 0.6429 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "57/57 - 32s - 568ms/step - loss: 1.3192 - val_loss: 0.6583 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "57/57 - 32s - 562ms/step - loss: 1.3354 - val_loss: 0.6842 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "57/57 - 34s - 594ms/step - loss: 1.3272 - val_loss: 0.6439 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "57/57 - 34s - 596ms/step - loss: 1.3204 - val_loss: 0.6510 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "57/57 - 36s - 623ms/step - loss: 1.3173 - val_loss: 0.6521 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "57/57 - 33s - 584ms/step - loss: 1.3180 - val_loss: 0.6429 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "57/57 - 33s - 576ms/step - loss: 1.3176 - val_loss: 0.6441 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "57/57 - 33s - 580ms/step - loss: 1.3140 - val_loss: 0.6489 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "57/57 - 33s - 577ms/step - loss: 1.3140 - val_loss: 0.6460 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "57/57 - 33s - 581ms/step - loss: 1.3283 - val_loss: 0.6435 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "57/57 - 33s - 579ms/step - loss: 1.3118 - val_loss: 0.6430 - learning_rate: 2.0000e-04\n",
      "Epoch 65/100\n",
      "57/57 - 32s - 558ms/step - loss: 1.3111 - val_loss: 0.6429 - learning_rate: 2.0000e-04\n",
      "Epoch 66/100\n",
      "57/57 - 32s - 568ms/step - loss: 1.3113 - val_loss: 0.6429 - learning_rate: 2.0000e-04\n",
      "Epoch 67/100\n",
      "57/57 - 33s - 577ms/step - loss: 1.3121 - val_loss: 0.6429 - learning_rate: 2.0000e-04\n",
      "Epoch 68/100\n",
      "57/57 - 35s - 622ms/step - loss: 1.3107 - val_loss: 0.6442 - learning_rate: 2.0000e-04\n",
      "Epoch 69/100\n",
      "57/57 - 33s - 573ms/step - loss: 1.3122 - val_loss: 0.6434 - learning_rate: 2.0000e-04\n",
      "Epoch 70/100\n",
      "57/57 - 33s - 571ms/step - loss: 1.3116 - val_loss: 0.6429 - learning_rate: 2.0000e-04\n",
      "Epoch 71/100\n",
      "57/57 - 33s - 572ms/step - loss: 1.3114 - val_loss: 0.6430 - learning_rate: 2.0000e-04\n",
      "Epoch 72/100\n",
      "57/57 - 31s - 552ms/step - loss: 1.3124 - val_loss: 0.6444 - learning_rate: 2.0000e-04\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "57/57 - 33s - 575ms/step - loss: 1.3116 - val_loss: 0.6436 - learning_rate: 2.0000e-04\n",
      "Epoch 74/100\n",
      "57/57 - 33s - 579ms/step - loss: 1.3108 - val_loss: 0.6434 - learning_rate: 4.0000e-05\n",
      "Epoch 75/100\n",
      "57/57 - 32s - 560ms/step - loss: 1.3103 - val_loss: 0.6430 - learning_rate: 4.0000e-05\n",
      "Epoch 76/100\n",
      "57/57 - 32s - 568ms/step - loss: 1.3103 - val_loss: 0.6430 - learning_rate: 4.0000e-05\n",
      "Epoch 77/100\n",
      "57/57 - 34s - 601ms/step - loss: 1.3099 - val_loss: 0.6429 - learning_rate: 4.0000e-05\n",
      "Epoch 78/100\n",
      "57/57 - 32s - 559ms/step - loss: 1.3102 - val_loss: 0.6429 - learning_rate: 4.0000e-05\n",
      "Epoch 79/100\n",
      "57/57 - 32s - 564ms/step - loss: 1.3101 - val_loss: 0.6429 - learning_rate: 4.0000e-05\n",
      "Epoch 80/100\n",
      "57/57 - 33s - 571ms/step - loss: 1.3102 - val_loss: 0.6429 - learning_rate: 4.0000e-05\n",
      "Epoch 81/100\n",
      "57/57 - 33s - 586ms/step - loss: 1.3104 - val_loss: 0.6429 - learning_rate: 4.0000e-05\n",
      "Epoch 82/100\n",
      "57/57 - 33s - 572ms/step - loss: 1.3105 - val_loss: 0.6429 - learning_rate: 4.0000e-05\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "57/57 - 32s - 563ms/step - loss: 1.3102 - val_loss: 0.6429 - learning_rate: 4.0000e-05\n",
      "Epoch 84/100\n",
      "57/57 - 32s - 568ms/step - loss: 1.3098 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "57/57 - 34s - 589ms/step - loss: 1.3100 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "57/57 - 32s - 554ms/step - loss: 1.3102 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "57/57 - 35s - 618ms/step - loss: 1.3098 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "57/57 - 32s - 568ms/step - loss: 1.3100 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "57/57 - 32s - 562ms/step - loss: 1.3099 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "57/57 - 32s - 568ms/step - loss: 1.3099 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "57/57 - 33s - 573ms/step - loss: 1.3100 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "57/57 - 31s - 552ms/step - loss: 1.3100 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "57/57 - 32s - 566ms/step - loss: 1.3101 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "57/57 - 34s - 600ms/step - loss: 1.3099 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "57/57 - 33s - 582ms/step - loss: 1.3099 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "57/57 - 35s - 606ms/step - loss: 1.3100 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "57/57 - 32s - 559ms/step - loss: 1.3099 - val_loss: 0.6428 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "57/57 - 32s - 570ms/step - loss: 1.3099 - val_loss: 0.6428 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "57/57 - 33s - 578ms/step - loss: 1.3101 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "57/57 - 32s - 564ms/step - loss: 1.3098 - val_loss: 0.6429 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 25'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 - 35s - 4s/step - loss: 324.1472 - val_loss: 247.5965 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "10/10 - 6s - 558ms/step - loss: 208.0448 - val_loss: 162.3151 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "10/10 - 5s - 547ms/step - loss: 136.1794 - val_loss: 105.1780 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "10/10 - 6s - 557ms/step - loss: 94.6282 - val_loss: 76.8247 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "10/10 - 6s - 552ms/step - loss: 71.6602 - val_loss: 61.6583 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "10/10 - 5s - 548ms/step - loss: 58.7738 - val_loss: 51.5382 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "10/10 - 6s - 556ms/step - loss: 51.6373 - val_loss: 46.3759 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "10/10 - 6s - 556ms/step - loss: 47.1087 - val_loss: 42.6551 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "10/10 - 6s - 556ms/step - loss: 44.0470 - val_loss: 40.6288 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "10/10 - 6s - 561ms/step - loss: 41.8618 - val_loss: 39.0208 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "10/10 - 5s - 545ms/step - loss: 40.4464 - val_loss: 36.9316 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "10/10 - 6s - 553ms/step - loss: 38.9214 - val_loss: 35.2205 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "10/10 - 6s - 566ms/step - loss: 37.9744 - val_loss: 34.4230 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "10/10 - 6s - 553ms/step - loss: 37.0171 - val_loss: 34.6884 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "10/10 - 6s - 552ms/step - loss: 36.2237 - val_loss: 33.8763 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "10/10 - 6s - 555ms/step - loss: 35.5614 - val_loss: 32.6903 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "10/10 - 6s - 558ms/step - loss: 35.0753 - val_loss: 32.4020 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "10/10 - 6s - 555ms/step - loss: 34.5134 - val_loss: 32.0229 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "10/10 - 6s - 559ms/step - loss: 34.1174 - val_loss: 31.6344 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "10/10 - 6s - 567ms/step - loss: 33.7596 - val_loss: 31.2780 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "10/10 - 6s - 575ms/step - loss: 33.4865 - val_loss: 30.8872 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "10/10 - 6s - 551ms/step - loss: 33.1984 - val_loss: 30.9891 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "10/10 - 6s - 557ms/step - loss: 32.9797 - val_loss: 30.8904 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "10/10 - 7s - 709ms/step - loss: 32.7867 - val_loss: 30.1403 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "10/10 - 6s - 553ms/step - loss: 32.6040 - val_loss: 30.2154 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "10/10 - 6s - 563ms/step - loss: 32.4138 - val_loss: 29.8250 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "10/10 - 6s - 556ms/step - loss: 32.2836 - val_loss: 29.7375 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "10/10 - 6s - 561ms/step - loss: 32.2015 - val_loss: 30.2127 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "10/10 - 6s - 595ms/step - loss: 32.0953 - val_loss: 29.5832 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "10/10 - 6s - 601ms/step - loss: 31.9667 - val_loss: 29.3026 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "10/10 - 6s - 602ms/step - loss: 31.8591 - val_loss: 29.6803 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "10/10 - 6s - 606ms/step - loss: 31.7832 - val_loss: 29.5072 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "10/10 - 6s - 611ms/step - loss: 31.6632 - val_loss: 29.3253 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "10/10 - 7s - 650ms/step - loss: 31.6323 - val_loss: 29.1616 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "10/10 - 6s - 622ms/step - loss: 31.5731 - val_loss: 29.1101 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "10/10 - 6s - 610ms/step - loss: 31.5925 - val_loss: 29.6194 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "10/10 - 6s - 588ms/step - loss: 31.4591 - val_loss: 29.3269 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "10/10 - 6s - 599ms/step - loss: 31.4083 - val_loss: 29.0461 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "10/10 - 6s - 580ms/step - loss: 31.3790 - val_loss: 29.0741 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "10/10 - 6s - 590ms/step - loss: 31.3558 - val_loss: 28.8313 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "10/10 - 6s - 584ms/step - loss: 31.2896 - val_loss: 28.8789 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "10/10 - 6s - 591ms/step - loss: 31.2634 - val_loss: 29.0368 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "10/10 - 6s - 592ms/step - loss: 31.2602 - val_loss: 28.9628 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "10/10 - 6s - 594ms/step - loss: 31.2060 - val_loss: 29.0527 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "10/10 - 6s - 562ms/step - loss: 31.1972 - val_loss: 29.0586 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "10/10 - 6s - 582ms/step - loss: 31.1688 - val_loss: 29.0015 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "10/10 - 6s - 557ms/step - loss: 31.2041 - val_loss: 28.6122 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "10/10 - 6s - 558ms/step - loss: 31.1797 - val_loss: 28.9582 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "10/10 - 6s - 566ms/step - loss: 31.1268 - val_loss: 28.7566 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "10/10 - 6s - 581ms/step - loss: 31.1181 - val_loss: 28.7400 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "10/10 - 6s - 586ms/step - loss: 31.0848 - val_loss: 28.9550 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "10/10 - 6s - 579ms/step - loss: 31.0754 - val_loss: 28.9717 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "10/10 - 6s - 592ms/step - loss: 31.0614 - val_loss: 28.7815 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "10/10 - 6s - 605ms/step - loss: 31.0847 - val_loss: 28.7285 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "10/10 - 6s - 584ms/step - loss: 31.0390 - val_loss: 28.8274 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "10/10 - 6s - 585ms/step - loss: 31.0372 - val_loss: 28.9421 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "10/10 - 6s - 576ms/step - loss: 31.0354 - val_loss: 28.7085 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "10/10 - 6s - 585ms/step - loss: 31.0211 - val_loss: 28.6724 - learning_rate: 2.0000e-04\n",
      "Epoch 59/100\n",
      "10/10 - 6s - 595ms/step - loss: 31.0040 - val_loss: 28.7408 - learning_rate: 2.0000e-04\n",
      "Epoch 60/100\n",
      "10/10 - 6s - 623ms/step - loss: 31.0133 - val_loss: 28.8089 - learning_rate: 2.0000e-04\n",
      "Epoch 61/100\n",
      "10/10 - 6s - 610ms/step - loss: 30.9980 - val_loss: 28.7697 - learning_rate: 2.0000e-04\n",
      "Epoch 62/100\n",
      "10/10 - 6s - 632ms/step - loss: 30.9984 - val_loss: 28.7378 - learning_rate: 2.0000e-04\n",
      "Epoch 63/100\n",
      "10/10 - 6s - 613ms/step - loss: 30.9959 - val_loss: 28.7309 - learning_rate: 2.0000e-04\n",
      "Epoch 64/100\n",
      "10/10 - 6s - 597ms/step - loss: 30.9884 - val_loss: 28.7496 - learning_rate: 2.0000e-04\n",
      "Epoch 65/100\n",
      "10/10 - 6s - 598ms/step - loss: 30.9935 - val_loss: 28.7797 - learning_rate: 2.0000e-04\n",
      "Epoch 66/100\n",
      "10/10 - 6s - 606ms/step - loss: 30.9919 - val_loss: 28.7724 - learning_rate: 2.0000e-04\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "10/10 - 6s - 583ms/step - loss: 30.9887 - val_loss: 28.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 68/100\n",
      "10/10 - 6s - 598ms/step - loss: 30.9823 - val_loss: 28.7904 - learning_rate: 4.0000e-05\n",
      "Epoch 69/100\n",
      "10/10 - 6s - 625ms/step - loss: 30.9864 - val_loss: 28.7881 - learning_rate: 4.0000e-05\n",
      "Epoch 70/100\n",
      "10/10 - 6s - 613ms/step - loss: 30.9848 - val_loss: 28.7919 - learning_rate: 4.0000e-05\n",
      "Epoch 71/100\n",
      "10/10 - 6s - 616ms/step - loss: 30.9867 - val_loss: 28.7974 - learning_rate: 4.0000e-05\n",
      "Epoch 72/100\n",
      "10/10 - 6s - 585ms/step - loss: 30.9852 - val_loss: 28.7918 - learning_rate: 4.0000e-05\n",
      "Epoch 73/100\n",
      "10/10 - 6s - 576ms/step - loss: 30.9825 - val_loss: 28.7905 - learning_rate: 4.0000e-05\n",
      "Epoch 74/100\n",
      "10/10 - 6s - 593ms/step - loss: 30.9876 - val_loss: 28.8078 - learning_rate: 4.0000e-05\n",
      "Epoch 75/100\n",
      "10/10 - 8s - 757ms/step - loss: 30.9862 - val_loss: 28.7956 - learning_rate: 4.0000e-05\n",
      "Epoch 76/100\n",
      "10/10 - 6s - 605ms/step - loss: 30.9845 - val_loss: 28.7851 - learning_rate: 4.0000e-05\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "10/10 - 6s - 590ms/step - loss: 30.9813 - val_loss: 28.7894 - learning_rate: 4.0000e-05\n",
      "Epoch 78/100\n",
      "10/10 - 6s - 598ms/step - loss: 30.9820 - val_loss: 28.7943 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "10/10 - 6s - 599ms/step - loss: 30.9841 - val_loss: 28.7897 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "10/10 - 6s - 588ms/step - loss: 30.9828 - val_loss: 28.7903 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "10/10 - 6s - 589ms/step - loss: 30.9835 - val_loss: 28.7925 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "10/10 - 6s - 575ms/step - loss: 30.9826 - val_loss: 28.7922 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "10/10 - 6s - 576ms/step - loss: 30.9807 - val_loss: 28.7925 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "10/10 - 6s - 573ms/step - loss: 30.9843 - val_loss: 28.7906 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "10/10 - 6s - 592ms/step - loss: 30.9858 - val_loss: 28.7943 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "10/10 - 6s - 601ms/step - loss: 30.9843 - val_loss: 28.7903 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "10/10 - 6s - 590ms/step - loss: 30.9843 - val_loss: 28.7900 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "10/10 - 6s - 590ms/step - loss: 30.9830 - val_loss: 28.7918 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "10/10 - 6s - 590ms/step - loss: 30.9848 - val_loss: 28.7887 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "10/10 - 6s - 611ms/step - loss: 30.9752 - val_loss: 28.7891 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "10/10 - 6s - 587ms/step - loss: 30.9823 - val_loss: 28.7882 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "10/10 - 6s - 581ms/step - loss: 30.9817 - val_loss: 28.7896 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "10/10 - 6s - 580ms/step - loss: 30.9796 - val_loss: 28.7900 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "10/10 - 6s - 578ms/step - loss: 30.9841 - val_loss: 28.7856 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "10/10 - 6s - 590ms/step - loss: 30.9786 - val_loss: 28.7875 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "10/10 - 6s - 583ms/step - loss: 30.9785 - val_loss: 28.7891 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "10/10 - 6s - 601ms/step - loss: 30.9793 - val_loss: 28.7874 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "10/10 - 6s - 607ms/step - loss: 30.9843 - val_loss: 28.7848 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "10/10 - 6s - 584ms/step - loss: 30.9845 - val_loss: 28.7849 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "10/10 - 6s - 597ms/step - loss: 30.9806 - val_loss: 28.7858 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 26'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 - 25s - 4s/step - loss: 356.4106 - val_loss: 297.1639 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "6/6 - 3s - 538ms/step - loss: 261.0659 - val_loss: 233.1504 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "6/6 - 3s - 561ms/step - loss: 200.1579 - val_loss: 176.4245 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "6/6 - 3s - 548ms/step - loss: 154.0292 - val_loss: 137.3974 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "6/6 - 3s - 540ms/step - loss: 118.8407 - val_loss: 109.3603 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "6/6 - 3s - 537ms/step - loss: 94.4599 - val_loss: 91.5355 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "6/6 - 3s - 539ms/step - loss: 77.9701 - val_loss: 75.8228 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "6/6 - 3s - 549ms/step - loss: 65.0041 - val_loss: 67.2756 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "6/6 - 3s - 530ms/step - loss: 57.5364 - val_loss: 59.3662 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "6/6 - 3s - 549ms/step - loss: 51.1009 - val_loss: 55.1067 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "6/6 - 3s - 537ms/step - loss: 46.6719 - val_loss: 50.9451 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "6/6 - 3s - 547ms/step - loss: 43.5245 - val_loss: 48.4857 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "6/6 - 3s - 543ms/step - loss: 40.7559 - val_loss: 45.9482 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "6/6 - 3s - 536ms/step - loss: 38.8071 - val_loss: 45.1378 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "6/6 - 3s - 535ms/step - loss: 37.3228 - val_loss: 42.5741 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "6/6 - 3s - 555ms/step - loss: 35.3891 - val_loss: 41.9146 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "6/6 - 3s - 534ms/step - loss: 34.5423 - val_loss: 41.0280 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "6/6 - 3s - 529ms/step - loss: 33.9161 - val_loss: 39.3695 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "6/6 - 3s - 535ms/step - loss: 32.5315 - val_loss: 39.5138 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "6/6 - 3s - 543ms/step - loss: 31.5835 - val_loss: 38.0758 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "6/6 - 3s - 535ms/step - loss: 30.7009 - val_loss: 37.0590 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "6/6 - 3s - 540ms/step - loss: 30.4922 - val_loss: 37.3307 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "6/6 - 3s - 562ms/step - loss: 29.5951 - val_loss: 35.9679 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "6/6 - 3s - 538ms/step - loss: 29.0655 - val_loss: 35.2540 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "6/6 - 3s - 540ms/step - loss: 28.4463 - val_loss: 35.2823 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "6/6 - 3s - 537ms/step - loss: 28.3863 - val_loss: 34.4748 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "6/6 - 3s - 543ms/step - loss: 27.5864 - val_loss: 34.0865 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "6/6 - 3s - 542ms/step - loss: 27.4425 - val_loss: 33.7619 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "6/6 - 4s - 590ms/step - loss: 26.9159 - val_loss: 33.1754 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "6/6 - 3s - 551ms/step - loss: 26.5311 - val_loss: 33.0781 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "6/6 - 3s - 553ms/step - loss: 26.2499 - val_loss: 33.3470 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "6/6 - 3s - 549ms/step - loss: 26.1656 - val_loss: 32.1646 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "6/6 - 3s - 531ms/step - loss: 25.4818 - val_loss: 32.5110 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "6/6 - 3s - 544ms/step - loss: 25.6950 - val_loss: 32.2649 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "6/6 - 3s - 561ms/step - loss: 25.0040 - val_loss: 31.3540 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "6/6 - 3s - 547ms/step - loss: 24.8307 - val_loss: 31.2994 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "6/6 - 3s - 542ms/step - loss: 24.9811 - val_loss: 31.6456 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "6/6 - 3s - 564ms/step - loss: 24.3106 - val_loss: 30.6742 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "6/6 - 3s - 542ms/step - loss: 23.9983 - val_loss: 30.4788 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "6/6 - 5s - 799ms/step - loss: 23.7861 - val_loss: 30.7671 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "6/6 - 3s - 542ms/step - loss: 23.5753 - val_loss: 30.4104 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "6/6 - 3s - 553ms/step - loss: 23.3149 - val_loss: 29.9310 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "6/6 - 3s - 535ms/step - loss: 23.0361 - val_loss: 29.7632 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "6/6 - 3s - 540ms/step - loss: 22.8601 - val_loss: 29.7137 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "6/6 - 3s - 563ms/step - loss: 22.7643 - val_loss: 29.7168 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "6/6 - 3s - 581ms/step - loss: 22.7706 - val_loss: 29.3523 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "6/6 - 3s - 566ms/step - loss: 22.4208 - val_loss: 28.9583 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "6/6 - 4s - 593ms/step - loss: 22.6609 - val_loss: 29.4790 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "6/6 - 3s - 578ms/step - loss: 22.2265 - val_loss: 28.9807 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "6/6 - 4s - 586ms/step - loss: 22.1163 - val_loss: 28.6080 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "6/6 - 4s - 592ms/step - loss: 22.0104 - val_loss: 28.6624 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "6/6 - 4s - 584ms/step - loss: 21.8880 - val_loss: 29.1543 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "6/6 - 4s - 595ms/step - loss: 21.7811 - val_loss: 28.2024 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "6/6 - 4s - 614ms/step - loss: 21.7835 - val_loss: 28.6632 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "6/6 - 4s - 627ms/step - loss: 21.1131 - val_loss: 27.9539 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "6/6 - 4s - 611ms/step - loss: 21.5125 - val_loss: 27.8418 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "6/6 - 4s - 589ms/step - loss: 21.0660 - val_loss: 28.4251 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "6/6 - 4s - 606ms/step - loss: 21.1473 - val_loss: 28.1398 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "6/6 - 4s - 630ms/step - loss: 20.9032 - val_loss: 27.5907 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "6/6 - 4s - 620ms/step - loss: 20.8540 - val_loss: 27.4007 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "6/6 - 4s - 629ms/step - loss: 20.7777 - val_loss: 27.6053 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "6/6 - 4s - 628ms/step - loss: 20.7031 - val_loss: 27.6219 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "6/6 - 4s - 606ms/step - loss: 20.7164 - val_loss: 27.2904 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "6/6 - 4s - 586ms/step - loss: 20.5056 - val_loss: 27.3004 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "6/6 - 3s - 569ms/step - loss: 20.4062 - val_loss: 26.9197 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "6/6 - 3s - 567ms/step - loss: 20.4737 - val_loss: 27.1862 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "6/6 - 3s - 569ms/step - loss: 20.5205 - val_loss: 27.5373 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "6/6 - 3s - 577ms/step - loss: 20.0381 - val_loss: 26.8102 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "6/6 - 4s - 588ms/step - loss: 20.0504 - val_loss: 27.0365 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "6/6 - 4s - 603ms/step - loss: 20.0867 - val_loss: 26.5613 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "6/6 - 3s - 561ms/step - loss: 19.8516 - val_loss: 27.0733 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "6/6 - 3s - 579ms/step - loss: 19.7736 - val_loss: 26.9778 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "6/6 - 3s - 562ms/step - loss: 19.6510 - val_loss: 26.3361 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "6/6 - 3s - 571ms/step - loss: 19.6183 - val_loss: 26.4988 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "6/6 - 3s - 562ms/step - loss: 19.8914 - val_loss: 26.7879 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "6/6 - 3s - 580ms/step - loss: 19.7569 - val_loss: 26.1537 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "6/6 - 3s - 565ms/step - loss: 19.3930 - val_loss: 26.3853 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "6/6 - 3s - 565ms/step - loss: 19.3974 - val_loss: 26.6499 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "6/6 - 3s - 567ms/step - loss: 19.6034 - val_loss: 26.2495 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "6/6 - 3s - 563ms/step - loss: 20.2027 - val_loss: 25.9261 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "6/6 - 4s - 591ms/step - loss: 19.5738 - val_loss: 27.1395 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "6/6 - 4s - 593ms/step - loss: 19.4580 - val_loss: 25.9557 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "6/6 - 3s - 555ms/step - loss: 19.2641 - val_loss: 26.0461 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "6/6 - 3s - 559ms/step - loss: 19.1338 - val_loss: 25.8480 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "6/6 - 3s - 555ms/step - loss: 19.2457 - val_loss: 26.1431 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "6/6 - 3s - 548ms/step - loss: 19.2242 - val_loss: 25.7648 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "6/6 - 3s - 552ms/step - loss: 19.0213 - val_loss: 25.8525 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "6/6 - 3s - 570ms/step - loss: 19.0224 - val_loss: 26.2221 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "6/6 - 3s - 546ms/step - loss: 18.8063 - val_loss: 25.8447 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "6/6 - 3s - 550ms/step - loss: 18.9114 - val_loss: 25.7627 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "6/6 - 3s - 544ms/step - loss: 19.1478 - val_loss: 25.5115 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "6/6 - 3s - 564ms/step - loss: 18.7434 - val_loss: 26.3526 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "6/6 - 3s - 572ms/step - loss: 18.6271 - val_loss: 25.6482 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "6/6 - 3s - 579ms/step - loss: 18.6745 - val_loss: 25.5020 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "6/6 - 3s - 565ms/step - loss: 18.8151 - val_loss: 25.4059 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "6/6 - 3s - 573ms/step - loss: 18.7774 - val_loss: 25.6123 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "6/6 - 3s - 561ms/step - loss: 18.6176 - val_loss: 25.6688 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "6/6 - 3s - 577ms/step - loss: 18.5580 - val_loss: 25.1628 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "6/6 - 3s - 569ms/step - loss: 18.5640 - val_loss: 25.2684 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "6/6 - 4s - 587ms/step - loss: 18.7452 - val_loss: 25.8798 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 27'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 - 30s - 1s/step - loss: 222.6974 - val_loss: 105.5706 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "26/26 - 14s - 529ms/step - loss: 69.1883 - val_loss: 41.0742 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "26/26 - 14s - 530ms/step - loss: 34.0287 - val_loss: 23.8990 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "26/26 - 14s - 521ms/step - loss: 23.2625 - val_loss: 17.5368 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "26/26 - 14s - 527ms/step - loss: 18.5607 - val_loss: 14.1014 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "26/26 - 15s - 595ms/step - loss: 15.9214 - val_loss: 11.8154 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "26/26 - 14s - 534ms/step - loss: 13.7721 - val_loss: 10.2142 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "26/26 - 14s - 524ms/step - loss: 12.2001 - val_loss: 8.8455 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "26/26 - 14s - 552ms/step - loss: 11.1031 - val_loss: 8.0975 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "26/26 - 15s - 580ms/step - loss: 10.7378 - val_loss: 7.8388 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "26/26 - 15s - 592ms/step - loss: 9.6927 - val_loss: 6.5301 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "26/26 - 15s - 573ms/step - loss: 8.9186 - val_loss: 5.9465 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "26/26 - 15s - 593ms/step - loss: 8.3564 - val_loss: 5.5039 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "26/26 - 15s - 560ms/step - loss: 7.9256 - val_loss: 5.0984 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "26/26 - 15s - 561ms/step - loss: 7.6211 - val_loss: 5.1235 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "26/26 - 15s - 592ms/step - loss: 7.4717 - val_loss: 4.5236 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "26/26 - 14s - 555ms/step - loss: 7.0984 - val_loss: 4.3775 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "26/26 - 14s - 554ms/step - loss: 6.8917 - val_loss: 4.0993 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "26/26 - 15s - 580ms/step - loss: 6.7286 - val_loss: 3.9150 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "26/26 - 15s - 574ms/step - loss: 6.5714 - val_loss: 3.9460 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "26/26 - 14s - 552ms/step - loss: 6.4189 - val_loss: 3.9001 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "26/26 - 15s - 559ms/step - loss: 6.3218 - val_loss: 3.5370 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "26/26 - 14s - 553ms/step - loss: 6.1664 - val_loss: 3.4125 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "26/26 - 15s - 561ms/step - loss: 6.0675 - val_loss: 3.8811 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "26/26 - 15s - 560ms/step - loss: 6.2934 - val_loss: 3.6449 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "26/26 - 15s - 585ms/step - loss: 6.1683 - val_loss: 3.2259 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "26/26 - 17s - 647ms/step - loss: 5.7682 - val_loss: 3.1416 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "26/26 - 15s - 567ms/step - loss: 5.7721 - val_loss: 3.1373 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "26/26 - 14s - 555ms/step - loss: 5.7016 - val_loss: 3.0696 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "26/26 - 15s - 573ms/step - loss: 5.7330 - val_loss: 3.3514 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "26/26 - 15s - 571ms/step - loss: 5.8571 - val_loss: 3.0781 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "26/26 - 15s - 570ms/step - loss: 5.5752 - val_loss: 3.0871 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "26/26 - 14s - 552ms/step - loss: 5.5381 - val_loss: 3.0064 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "26/26 - 15s - 569ms/step - loss: 5.5321 - val_loss: 2.8162 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "26/26 - 15s - 567ms/step - loss: 5.4197 - val_loss: 3.0288 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "26/26 - 15s - 570ms/step - loss: 5.5002 - val_loss: 2.7628 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "26/26 - 15s - 576ms/step - loss: 5.4092 - val_loss: 2.7857 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "26/26 - 15s - 582ms/step - loss: 5.3965 - val_loss: 2.7235 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "26/26 - 16s - 598ms/step - loss: 5.3937 - val_loss: 3.0035 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "26/26 - 14s - 556ms/step - loss: 5.4011 - val_loss: 2.7486 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "26/26 - 14s - 553ms/step - loss: 5.4095 - val_loss: 2.7269 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "26/26 - 14s - 556ms/step - loss: 5.2913 - val_loss: 2.6886 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "26/26 - 14s - 551ms/step - loss: 5.3105 - val_loss: 2.6375 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "26/26 - 15s - 567ms/step - loss: 5.3204 - val_loss: 2.6698 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "26/26 - 15s - 566ms/step - loss: 5.3108 - val_loss: 2.6249 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "26/26 - 15s - 571ms/step - loss: 5.2630 - val_loss: 2.6101 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "26/26 - 15s - 563ms/step - loss: 5.3199 - val_loss: 2.6400 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "26/26 - 17s - 636ms/step - loss: 5.2798 - val_loss: 2.6075 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "26/26 - 15s - 581ms/step - loss: 5.2132 - val_loss: 2.5708 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "26/26 - 15s - 567ms/step - loss: 5.2870 - val_loss: 2.6724 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "26/26 - 15s - 575ms/step - loss: 5.2004 - val_loss: 2.5545 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "26/26 - 15s - 570ms/step - loss: 5.2145 - val_loss: 2.5636 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "26/26 - 15s - 561ms/step - loss: 5.1471 - val_loss: 2.5641 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "26/26 - 15s - 560ms/step - loss: 5.2863 - val_loss: 2.5855 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "26/26 - 15s - 564ms/step - loss: 5.3453 - val_loss: 2.5303 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "26/26 - 15s - 567ms/step - loss: 5.2138 - val_loss: 2.5244 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "26/26 - 15s - 565ms/step - loss: 5.2183 - val_loss: 2.5262 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "26/26 - 15s - 568ms/step - loss: 5.1347 - val_loss: 2.5336 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "26/26 - 15s - 562ms/step - loss: 5.1622 - val_loss: 2.5651 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "26/26 - 15s - 560ms/step - loss: 5.1658 - val_loss: 2.6141 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "26/26 - 14s - 556ms/step - loss: 5.1474 - val_loss: 2.4946 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "26/26 - 15s - 564ms/step - loss: 5.0987 - val_loss: 3.0393 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "26/26 - 15s - 562ms/step - loss: 5.1873 - val_loss: 2.5568 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "26/26 - 15s - 564ms/step - loss: 5.1561 - val_loss: 2.4861 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "26/26 - 15s - 564ms/step - loss: 5.0871 - val_loss: 2.5006 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "26/26 - 15s - 563ms/step - loss: 5.1150 - val_loss: 2.4932 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "26/26 - 15s - 561ms/step - loss: 5.1152 - val_loss: 2.5161 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "26/26 - 16s - 629ms/step - loss: 5.1263 - val_loss: 2.5266 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "26/26 - 14s - 557ms/step - loss: 5.0841 - val_loss: 2.4766 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "26/26 - 15s - 582ms/step - loss: 5.1178 - val_loss: 2.4782 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "26/26 - 15s - 595ms/step - loss: 5.1163 - val_loss: 2.4648 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "26/26 - 15s - 595ms/step - loss: 5.0971 - val_loss: 2.5000 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "26/26 - 14s - 557ms/step - loss: 5.0457 - val_loss: 2.5166 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "26/26 - 15s - 560ms/step - loss: 5.0884 - val_loss: 2.5011 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "26/26 - 14s - 552ms/step - loss: 5.1104 - val_loss: 2.5239 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "26/26 - 15s - 559ms/step - loss: 5.0817 - val_loss: 2.4530 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "26/26 - 15s - 559ms/step - loss: 5.0930 - val_loss: 2.7986 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "26/26 - 15s - 561ms/step - loss: 5.1320 - val_loss: 2.4506 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "26/26 - 15s - 569ms/step - loss: 5.0957 - val_loss: 2.4697 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "26/26 - 15s - 573ms/step - loss: 5.0690 - val_loss: 2.5371 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "26/26 - 15s - 595ms/step - loss: 5.0584 - val_loss: 2.4513 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "26/26 - 15s - 582ms/step - loss: 5.0684 - val_loss: 2.5319 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "26/26 - 15s - 569ms/step - loss: 5.0985 - val_loss: 2.4466 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "26/26 - 15s - 561ms/step - loss: 5.0967 - val_loss: 2.5352 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "26/26 - 15s - 561ms/step - loss: 5.0700 - val_loss: 2.4981 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "26/26 - 15s - 560ms/step - loss: 5.0690 - val_loss: 2.5603 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "26/26 - 15s - 579ms/step - loss: 5.1284 - val_loss: 2.4563 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "26/26 - 15s - 561ms/step - loss: 5.1254 - val_loss: 2.4714 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "26/26 - 16s - 624ms/step - loss: 5.1300 - val_loss: 2.5158 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "26/26 - 15s - 586ms/step - loss: 5.0579 - val_loss: 2.4502 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "26/26 - 16s - 613ms/step - loss: 5.1141 - val_loss: 2.4456 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "26/26 - 15s - 559ms/step - loss: 5.0586 - val_loss: 2.4500 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "26/26 - 15s - 569ms/step - loss: 5.0478 - val_loss: 2.5030 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "26/26 - 15s - 562ms/step - loss: 5.1286 - val_loss: 2.4596 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "26/26 - 15s - 566ms/step - loss: 5.0470 - val_loss: 2.4755 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "26/26 - 15s - 560ms/step - loss: 5.0478 - val_loss: 2.4827 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "26/26 - 15s - 564ms/step - loss: 5.0619 - val_loss: 2.4993 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "26/26 - 15s - 562ms/step - loss: 5.1256 - val_loss: 2.4295 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "26/26 - 15s - 569ms/step - loss: 5.0393 - val_loss: 2.4340 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "26/26 - 15s - 573ms/step - loss: 5.0451 - val_loss: 2.4279 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 28'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 - 25s - 1s/step - loss: 279.5316 - val_loss: 185.1600 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "18/18 - 10s - 543ms/step - loss: 144.5239 - val_loss: 102.1147 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "18/18 - 10s - 538ms/step - loss: 86.7310 - val_loss: 66.6358 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "18/18 - 10s - 542ms/step - loss: 61.1332 - val_loss: 49.5257 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "18/18 - 10s - 550ms/step - loss: 48.2080 - val_loss: 39.7219 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "18/18 - 10s - 567ms/step - loss: 40.1262 - val_loss: 34.7225 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "18/18 - 10s - 561ms/step - loss: 34.8426 - val_loss: 28.4844 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "18/18 - 10s - 534ms/step - loss: 30.0448 - val_loss: 24.9125 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "18/18 - 10s - 529ms/step - loss: 26.7983 - val_loss: 22.3020 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "18/18 - 10s - 543ms/step - loss: 24.4126 - val_loss: 20.0363 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "18/18 - 11s - 597ms/step - loss: 22.5139 - val_loss: 18.3097 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "18/18 - 13s - 747ms/step - loss: 20.5901 - val_loss: 16.5774 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "18/18 - 11s - 631ms/step - loss: 19.3164 - val_loss: 15.8450 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "18/18 - 10s - 577ms/step - loss: 18.0174 - val_loss: 14.5231 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "18/18 - 11s - 604ms/step - loss: 17.0170 - val_loss: 13.7561 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "18/18 - 13s - 706ms/step - loss: 16.1967 - val_loss: 12.9062 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "18/18 - 17s - 951ms/step - loss: 15.3355 - val_loss: 11.6169 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "18/18 - 13s - 714ms/step - loss: 14.6443 - val_loss: 11.0842 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "18/18 - 14s - 782ms/step - loss: 13.9214 - val_loss: 10.2576 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "18/18 - 13s - 735ms/step - loss: 13.6687 - val_loss: 9.8320 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "18/18 - 11s - 627ms/step - loss: 12.8840 - val_loss: 9.4529 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "18/18 - 11s - 607ms/step - loss: 12.3343 - val_loss: 9.1740 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "18/18 - 10s - 577ms/step - loss: 12.0269 - val_loss: 9.6315 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "18/18 - 10s - 566ms/step - loss: 11.6477 - val_loss: 9.1024 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "18/18 - 10s - 560ms/step - loss: 11.5944 - val_loss: 8.0258 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "18/18 - 10s - 559ms/step - loss: 10.8520 - val_loss: 7.3325 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "18/18 - 11s - 616ms/step - loss: 10.5376 - val_loss: 7.3374 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "18/18 - 13s - 703ms/step - loss: 10.3226 - val_loss: 7.1890 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "18/18 - 13s - 699ms/step - loss: 10.1216 - val_loss: 7.1583 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "18/18 - 12s - 684ms/step - loss: 9.7461 - val_loss: 6.3214 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "18/18 - 19s - 1s/step - loss: 9.5979 - val_loss: 6.4678 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "18/18 - 10s - 534ms/step - loss: 9.3023 - val_loss: 5.9619 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "18/18 - 10s - 544ms/step - loss: 9.2747 - val_loss: 5.9136 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "18/18 - 10s - 543ms/step - loss: 9.0014 - val_loss: 5.6353 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "18/18 - 10s - 550ms/step - loss: 8.8737 - val_loss: 5.8381 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "18/18 - 10s - 565ms/step - loss: 8.6895 - val_loss: 5.6652 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "18/18 - 12s - 641ms/step - loss: 8.5365 - val_loss: 5.4058 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "18/18 - 11s - 621ms/step - loss: 8.4561 - val_loss: 5.8142 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "18/18 - 11s - 619ms/step - loss: 8.3555 - val_loss: 5.0238 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "18/18 - 10s - 532ms/step - loss: 8.2222 - val_loss: 4.9012 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "18/18 - 10s - 543ms/step - loss: 8.0838 - val_loss: 5.5604 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "18/18 - 10s - 536ms/step - loss: 7.9482 - val_loss: 4.9197 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "18/18 - 10s - 528ms/step - loss: 7.9987 - val_loss: 4.8206 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "18/18 - 10s - 530ms/step - loss: 7.8614 - val_loss: 4.5190 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "18/18 - 10s - 531ms/step - loss: 7.7426 - val_loss: 5.0433 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "18/18 - 10s - 540ms/step - loss: 7.7942 - val_loss: 4.4320 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "18/18 - 10s - 550ms/step - loss: 7.7206 - val_loss: 4.2324 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "18/18 - 10s - 542ms/step - loss: 7.5649 - val_loss: 4.3377 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "18/18 - 10s - 530ms/step - loss: 7.3881 - val_loss: 4.1967 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "18/18 - 10s - 530ms/step - loss: 7.3710 - val_loss: 4.3508 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "18/18 - 10s - 543ms/step - loss: 7.3665 - val_loss: 4.2471 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "18/18 - 10s - 565ms/step - loss: 7.2862 - val_loss: 4.6254 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "18/18 - 10s - 549ms/step - loss: 7.4755 - val_loss: 4.0449 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "18/18 - 10s - 561ms/step - loss: 7.3245 - val_loss: 3.8321 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "18/18 - 10s - 561ms/step - loss: 7.1653 - val_loss: 3.9644 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "18/18 - 10s - 547ms/step - loss: 7.1292 - val_loss: 3.9525 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "18/18 - 10s - 554ms/step - loss: 7.1185 - val_loss: 4.1623 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "18/18 - 10s - 554ms/step - loss: 7.1088 - val_loss: 4.4718 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "18/18 - 10s - 551ms/step - loss: 7.0691 - val_loss: 4.1691 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "18/18 - 10s - 557ms/step - loss: 7.0020 - val_loss: 4.1002 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "18/18 - 10s - 558ms/step - loss: 6.9906 - val_loss: 3.6179 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "18/18 - 10s - 561ms/step - loss: 6.9585 - val_loss: 3.6660 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "18/18 - 10s - 557ms/step - loss: 6.8255 - val_loss: 3.9022 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "18/18 - 10s - 560ms/step - loss: 6.8080 - val_loss: 3.5895 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "18/18 - 10s - 565ms/step - loss: 6.8090 - val_loss: 3.9287 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "18/18 - 10s - 562ms/step - loss: 6.7570 - val_loss: 3.5662 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "18/18 - 10s - 564ms/step - loss: 6.7739 - val_loss: 3.5029 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "18/18 - 10s - 562ms/step - loss: 6.7941 - val_loss: 3.6955 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "18/18 - 12s - 650ms/step - loss: 6.7865 - val_loss: 3.5702 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "18/18 - 10s - 566ms/step - loss: 6.6819 - val_loss: 3.5595 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "18/18 - 10s - 574ms/step - loss: 6.7102 - val_loss: 3.6143 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "18/18 - 11s - 585ms/step - loss: 6.6955 - val_loss: 4.3537 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "18/18 - 11s - 590ms/step - loss: 6.6943 - val_loss: 3.3215 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "18/18 - 10s - 553ms/step - loss: 6.8611 - val_loss: 3.4200 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "18/18 - 10s - 554ms/step - loss: 6.6485 - val_loss: 3.8002 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "18/18 - 10s - 564ms/step - loss: 6.7348 - val_loss: 3.9147 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "18/18 - 10s - 562ms/step - loss: 6.6208 - val_loss: 3.4790 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "18/18 - 10s - 569ms/step - loss: 6.5753 - val_loss: 3.4634 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "18/18 - 10s - 552ms/step - loss: 6.6152 - val_loss: 3.4970 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "18/18 - 10s - 570ms/step - loss: 6.5844 - val_loss: 3.6282 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "18/18 - 10s - 564ms/step - loss: 6.6907 - val_loss: 3.2305 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "18/18 - 10s - 570ms/step - loss: 6.6908 - val_loss: 3.4048 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "18/18 - 10s - 557ms/step - loss: 6.6189 - val_loss: 3.6505 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "18/18 - 11s - 584ms/step - loss: 6.5159 - val_loss: 3.2278 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "18/18 - 10s - 578ms/step - loss: 6.5712 - val_loss: 3.6556 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "18/18 - 11s - 585ms/step - loss: 6.5317 - val_loss: 3.4910 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "18/18 - 10s - 579ms/step - loss: 6.5937 - val_loss: 3.2823 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "18/18 - 10s - 576ms/step - loss: 6.5347 - val_loss: 3.3402 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "18/18 - 10s - 565ms/step - loss: 6.4798 - val_loss: 3.5330 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "18/18 - 10s - 572ms/step - loss: 6.4783 - val_loss: 3.3221 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "18/18 - 10s - 577ms/step - loss: 6.4593 - val_loss: 3.4086 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "18/18 - 10s - 573ms/step - loss: 6.4836 - val_loss: 3.5567 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "18/18 - 10s - 556ms/step - loss: 6.4799 - val_loss: 3.4861 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "18/18 - 10s - 564ms/step - loss: 6.4765 - val_loss: 3.5295 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "18/18 - 10s - 565ms/step - loss: 6.4356 - val_loss: 3.4551 - learning_rate: 2.0000e-04\n",
      "Epoch 96/100\n",
      "18/18 - 10s - 556ms/step - loss: 6.4238 - val_loss: 3.3641 - learning_rate: 2.0000e-04\n",
      "Epoch 97/100\n",
      "18/18 - 10s - 560ms/step - loss: 6.4193 - val_loss: 3.3927 - learning_rate: 2.0000e-04\n",
      "Epoch 98/100\n",
      "18/18 - 10s - 561ms/step - loss: 6.4069 - val_loss: 3.3580 - learning_rate: 2.0000e-04\n",
      "Epoch 99/100\n",
      "18/18 - 12s - 660ms/step - loss: 6.4036 - val_loss: 3.3815 - learning_rate: 2.0000e-04\n",
      "Epoch 100/100\n",
      "18/18 - 10s - 579ms/step - loss: 6.4093 - val_loss: 3.3880 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 29'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 - 34s - 2s/step - loss: 324.6101 - val_loss: 201.0165 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "20/20 - 11s - 535ms/step - loss: 163.6490 - val_loss: 123.3405 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "20/20 - 11s - 535ms/step - loss: 109.7340 - val_loss: 89.3375 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "20/20 - 11s - 539ms/step - loss: 84.5882 - val_loss: 75.3083 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "20/20 - 11s - 537ms/step - loss: 69.7196 - val_loss: 61.7855 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "20/20 - 11s - 537ms/step - loss: 59.8793 - val_loss: 52.8044 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "20/20 - 11s - 538ms/step - loss: 53.4258 - val_loss: 46.3325 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "20/20 - 11s - 536ms/step - loss: 47.8227 - val_loss: 42.2316 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "20/20 - 11s - 538ms/step - loss: 42.7277 - val_loss: 37.2001 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "20/20 - 11s - 536ms/step - loss: 39.2115 - val_loss: 34.9547 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "20/20 - 11s - 536ms/step - loss: 36.4693 - val_loss: 31.6102 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "20/20 - 11s - 541ms/step - loss: 33.8459 - val_loss: 28.5884 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "20/20 - 11s - 536ms/step - loss: 32.0076 - val_loss: 29.1383 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "20/20 - 11s - 557ms/step - loss: 30.5713 - val_loss: 26.6456 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "20/20 - 11s - 539ms/step - loss: 29.6846 - val_loss: 25.0605 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "20/20 - 11s - 538ms/step - loss: 28.7690 - val_loss: 24.4828 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "20/20 - 11s - 535ms/step - loss: 27.0700 - val_loss: 22.6772 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "20/20 - 11s - 539ms/step - loss: 25.7785 - val_loss: 22.1884 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "20/20 - 11s - 539ms/step - loss: 25.0690 - val_loss: 20.6864 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "20/20 - 11s - 536ms/step - loss: 24.4391 - val_loss: 22.0733 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "20/20 - 11s - 540ms/step - loss: 24.1299 - val_loss: 20.9182 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "20/20 - 11s - 539ms/step - loss: 23.7780 - val_loss: 20.1352 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "20/20 - 11s - 541ms/step - loss: 22.9062 - val_loss: 21.2875 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "20/20 - 11s - 540ms/step - loss: 22.3094 - val_loss: 18.7590 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "20/20 - 13s - 626ms/step - loss: 22.1712 - val_loss: 18.7371 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "20/20 - 11s - 543ms/step - loss: 21.1975 - val_loss: 17.7693 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "20/20 - 11s - 546ms/step - loss: 20.7928 - val_loss: 17.4676 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "20/20 - 11s - 559ms/step - loss: 20.3703 - val_loss: 17.0734 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "20/20 - 11s - 566ms/step - loss: 19.8818 - val_loss: 18.4245 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "20/20 - 12s - 575ms/step - loss: 19.8608 - val_loss: 19.0510 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "20/20 - 12s - 616ms/step - loss: 20.1803 - val_loss: 16.2272 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "20/20 - 13s - 644ms/step - loss: 19.0176 - val_loss: 15.7724 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "20/20 - 14s - 724ms/step - loss: 19.1816 - val_loss: 16.0478 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "20/20 - 15s - 726ms/step - loss: 18.9784 - val_loss: 15.4490 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "20/20 - 11s - 554ms/step - loss: 18.5935 - val_loss: 15.1226 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "20/20 - 12s - 582ms/step - loss: 18.1718 - val_loss: 18.2708 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "20/20 - 11s - 550ms/step - loss: 18.2930 - val_loss: 14.9904 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "20/20 - 11s - 555ms/step - loss: 17.5755 - val_loss: 14.5852 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "20/20 - 12s - 605ms/step - loss: 17.5227 - val_loss: 14.2600 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "20/20 - 14s - 683ms/step - loss: 17.6936 - val_loss: 14.1644 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "20/20 - 14s - 721ms/step - loss: 18.0361 - val_loss: 16.0616 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "20/20 - 12s - 619ms/step - loss: 17.2965 - val_loss: 14.0216 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "20/20 - 13s - 659ms/step - loss: 16.7037 - val_loss: 16.0043 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "20/20 - 12s - 594ms/step - loss: 17.3652 - val_loss: 13.4363 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "20/20 - 11s - 573ms/step - loss: 16.6034 - val_loss: 13.4753 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "20/20 - 11s - 561ms/step - loss: 16.5818 - val_loss: 15.4638 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "20/20 - 11s - 559ms/step - loss: 17.2327 - val_loss: 13.9890 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "20/20 - 11s - 551ms/step - loss: 16.0106 - val_loss: 13.2897 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "20/20 - 11s - 536ms/step - loss: 15.9871 - val_loss: 13.2839 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "20/20 - 11s - 526ms/step - loss: 15.6948 - val_loss: 13.3363 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "20/20 - 12s - 619ms/step - loss: 15.6961 - val_loss: 12.5548 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "20/20 - 11s - 559ms/step - loss: 15.5089 - val_loss: 12.4454 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "20/20 - 11s - 544ms/step - loss: 15.4454 - val_loss: 12.3423 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "20/20 - 11s - 539ms/step - loss: 15.3106 - val_loss: 12.9687 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "20/20 - 11s - 555ms/step - loss: 15.2352 - val_loss: 12.1600 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "20/20 - 11s - 564ms/step - loss: 15.1953 - val_loss: 13.1270 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "20/20 - 11s - 539ms/step - loss: 15.3993 - val_loss: 12.2248 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "20/20 - 11s - 541ms/step - loss: 15.3879 - val_loss: 11.9785 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "20/20 - 11s - 535ms/step - loss: 15.7151 - val_loss: 12.4156 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "20/20 - 11s - 542ms/step - loss: 15.2048 - val_loss: 11.8221 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "20/20 - 11s - 546ms/step - loss: 15.0108 - val_loss: 11.8135 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "20/20 - 11s - 538ms/step - loss: 14.8028 - val_loss: 14.3466 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "20/20 - 11s - 534ms/step - loss: 14.8483 - val_loss: 11.7267 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "20/20 - 11s - 550ms/step - loss: 14.4214 - val_loss: 11.5809 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "20/20 - 11s - 556ms/step - loss: 14.3359 - val_loss: 11.9452 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "20/20 - 11s - 552ms/step - loss: 14.4032 - val_loss: 12.7201 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "20/20 - 11s - 553ms/step - loss: 14.4948 - val_loss: 11.8072 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "20/20 - 11s - 546ms/step - loss: 14.2732 - val_loss: 11.2780 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "20/20 - 11s - 567ms/step - loss: 14.2071 - val_loss: 11.1343 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "20/20 - 11s - 543ms/step - loss: 14.0219 - val_loss: 11.4497 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "20/20 - 11s - 549ms/step - loss: 13.9731 - val_loss: 11.0210 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "20/20 - 13s - 667ms/step - loss: 14.0067 - val_loss: 11.0978 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "20/20 - 13s - 640ms/step - loss: 14.0851 - val_loss: 11.0238 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "20/20 - 11s - 557ms/step - loss: 14.2988 - val_loss: 11.3518 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "20/20 - 11s - 555ms/step - loss: 14.0975 - val_loss: 11.0595 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "20/20 - 11s - 563ms/step - loss: 13.7947 - val_loss: 10.7730 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "20/20 - 11s - 554ms/step - loss: 13.9009 - val_loss: 11.3820 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "20/20 - 13s - 629ms/step - loss: 13.7242 - val_loss: 10.6830 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "20/20 - 13s - 644ms/step - loss: 13.6494 - val_loss: 10.6377 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "20/20 - 14s - 711ms/step - loss: 13.6449 - val_loss: 11.0426 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "20/20 - 12s - 586ms/step - loss: 13.9701 - val_loss: 10.6661 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "20/20 - 11s - 541ms/step - loss: 13.5177 - val_loss: 10.5392 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "20/20 - 12s - 592ms/step - loss: 13.8943 - val_loss: 10.6706 - learning_rate: 0.0010\n",
      "Epoch 84/100\n",
      "20/20 - 15s - 762ms/step - loss: 13.3807 - val_loss: 10.4493 - learning_rate: 0.0010\n",
      "Epoch 85/100\n",
      "20/20 - 13s - 664ms/step - loss: 13.3454 - val_loss: 10.3973 - learning_rate: 0.0010\n",
      "Epoch 86/100\n",
      "20/20 - 13s - 657ms/step - loss: 13.6197 - val_loss: 10.6875 - learning_rate: 0.0010\n",
      "Epoch 87/100\n",
      "20/20 - 11s - 564ms/step - loss: 13.3610 - val_loss: 11.3217 - learning_rate: 0.0010\n",
      "Epoch 88/100\n",
      "20/20 - 11s - 570ms/step - loss: 13.5424 - val_loss: 11.0159 - learning_rate: 0.0010\n",
      "Epoch 89/100\n",
      "20/20 - 11s - 574ms/step - loss: 13.3899 - val_loss: 10.3106 - learning_rate: 0.0010\n",
      "Epoch 90/100\n",
      "20/20 - 11s - 557ms/step - loss: 13.3067 - val_loss: 10.7026 - learning_rate: 0.0010\n",
      "Epoch 91/100\n",
      "20/20 - 11s - 541ms/step - loss: 13.2768 - val_loss: 10.9387 - learning_rate: 0.0010\n",
      "Epoch 92/100\n",
      "20/20 - 14s - 675ms/step - loss: 13.2168 - val_loss: 10.4229 - learning_rate: 0.0010\n",
      "Epoch 93/100\n",
      "20/20 - 11s - 558ms/step - loss: 13.2697 - val_loss: 10.6443 - learning_rate: 0.0010\n",
      "Epoch 94/100\n",
      "20/20 - 12s - 613ms/step - loss: 13.0029 - val_loss: 10.1383 - learning_rate: 0.0010\n",
      "Epoch 95/100\n",
      "20/20 - 12s - 607ms/step - loss: 13.2309 - val_loss: 10.5175 - learning_rate: 0.0010\n",
      "Epoch 96/100\n",
      "20/20 - 12s - 578ms/step - loss: 13.4430 - val_loss: 10.6722 - learning_rate: 0.0010\n",
      "Epoch 97/100\n",
      "20/20 - 12s - 597ms/step - loss: 13.3879 - val_loss: 10.0717 - learning_rate: 0.0010\n",
      "Epoch 98/100\n",
      "20/20 - 12s - 594ms/step - loss: 12.9799 - val_loss: 10.1638 - learning_rate: 0.0010\n",
      "Epoch 99/100\n",
      "20/20 - 11s - 571ms/step - loss: 12.9797 - val_loss: 10.5966 - learning_rate: 0.0010\n",
      "Epoch 100/100\n",
      "20/20 - 11s - 571ms/step - loss: 13.1662 - val_loss: 10.0472 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "models = {}\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=False, verbose=1)\n",
    "\n",
    "# Preparar los datos por cluster\n",
    "for cluster in range(n_clusters):\n",
    "    display(f'Entrenando cluster numero: {cluster}')\n",
    "    cluster_data = grouped_df[grouped_df['cluster'] == cluster].copy()\n",
    "    cluster_data.sort_values(by='periodo', inplace=True)\n",
    "    \n",
    "    X, y = [], []\n",
    "    X_weights = []\n",
    "\n",
    "    for key, data in cluster_data.groupby(['customer_id', 'product_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'quarter', 'month', 'customer_id', 'product_id', 'tn']].values\n",
    "        if len(series) > 2:  # Asegurarse de que haya suficientes datos\n",
    "            X.append(series[:-2])  # Todos los datos excepto los últimos 2\n",
    "            y.append(series[-1, -1])\n",
    "            \n",
    "            product_id = key[1]\n",
    "            total_tn = total_tn_dict.get(product_id, 0)\n",
    "            X_weights.append(total_tn)\n",
    "            \n",
    "    \n",
    "    #Padleft para que todos los registros tengan el mismo shape\n",
    "    max_len = max(len(seq) for seq in X)\n",
    "    X_padded = np.array([np.pad(seq, ((max_len - len(seq), 0), (0, 0)), mode='constant') for seq in X]).astype(np.float32)\n",
    "    y = np.array(y).astype(np.float32)\n",
    "    X_weights = np.array(X_weights).astype(np.float32)\n",
    "\n",
    "    # Debug\n",
    "    # print(len(X))\n",
    "    # print(len(X_padded))\n",
    "    # print(len(X_weights))\n",
    "    # print(len(y))\n",
    "    callbacks = [reduce_lr]\n",
    "\n",
    "    if cluster in [0, 12]:\n",
    "        callbacks.append(early_stopping)\n",
    "    \n",
    "    # Construir y entrenar el modelo\n",
    "    model = build_lstm_model((X_padded.shape[1], X_padded.shape[2]))\n",
    "    model.fit(X_padded, y, epochs=100, verbose=2, batch_size=200, validation_split=0.2, sample_weight=X_weights, callbacks=callbacks)\n",
    "    models[cluster] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 7: Sumarizar las predicciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalers = joblib.load('scalers.pkl')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    if cluster not in models:\n",
    "        continue\n",
    "\n",
    "    model = models[cluster]\n",
    "    cluster_data = grouped_df[grouped_df['cluster'] == cluster].copy()\n",
    "    \n",
    "    X_pred_data = []\n",
    "    keys = []\n",
    "    for key, data in cluster_data.groupby(['customer_id', 'product_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'quarter', 'month', 'customer_id', 'product_id', 'tn']].values\n",
    "        max_len = len(series) - 1\n",
    "        X_pred = np.pad(series[1:], ((max_len - len(series[1:]), 0), (0, 0)), mode='constant').astype(np.float32)\n",
    "        X_pred_data.append(X_pred)\n",
    "        keys.append(key)\n",
    "    \n",
    "    if len(X_pred_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    max_len_pred = max(len(seq) for seq in X_pred_data)\n",
    "    X_pred_padded = np.array([np.pad(seq, ((max_len_pred - len(seq), 0), (0, 0)), mode='constant') for seq in X_pred_data]).astype(np.float32)\n",
    "    X_pred_padded = np.reshape(X_pred_padded, (X_pred_padded.shape[0], X_pred_padded.shape[1], X_pred_padded.shape[2]))\n",
    "    \n",
    "    preds = model.predict(X_pred_padded, verbose=0)\n",
    "    inversed_preds_df = []\n",
    "    \n",
    "    for key, pred in zip(keys, preds):\n",
    "        customer_id, product_id = key\n",
    "        scaler_key = f'{product_id}_{customer_id}'\n",
    "        scaler_tn = scalers[scaler_key]\n",
    "        inverse_pred = scaler_tn.inverse_transform([pred])\n",
    "        \n",
    "        if inverse_pred[0][0] < 0:\n",
    "            inversed_preds_df.append(0)\n",
    "            predictions.append([customer_id, product_id, 0])\n",
    "        else:\n",
    "            inversed_preds_df.append(inverse_pred[0][0])\n",
    "            predictions.append([customer_id, product_id, inverse_pred[0][0]])\n",
    "    \n",
    "    pred_df_temp = pd.DataFrame(inversed_preds_df, columns=['prediccion'])\n",
    "    pred_df_temp.to_csv(f\"predicciones_temprales_cluster_pID{cluster}.csv\", index=False)\n",
    "    \n",
    "consilated_df_temp = pd.DataFrame(predictions, columns=['customer_id', 'product_id', 'prediccion'])\n",
    "consilated_df_temp.to_csv(f\"predicciones_temprales_todos_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>prediccion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>1285.889299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>1122.865283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>823.382444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>663.090058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>555.758389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>21263</td>\n",
       "      <td>0.013954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>21265</td>\n",
       "      <td>0.014951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>21266</td>\n",
       "      <td>0.011970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>21267</td>\n",
       "      <td>0.030644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>21276</td>\n",
       "      <td>0.015078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id   prediccion\n",
       "0         20001  1285.889299\n",
       "1         20002  1122.865283\n",
       "2         20003   823.382444\n",
       "3         20004   663.090058\n",
       "4         20005   555.758389\n",
       "..          ...          ...\n",
       "775       21263     0.013954\n",
       "776       21265     0.014951\n",
       "777       21266     0.011970\n",
       "778       21267     0.030644\n",
       "779       21276     0.015078\n",
       "\n",
       "[780 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarized_preds = consilated_df_temp.groupby(['product_id'])['prediccion'].sum().reset_index()\n",
    "\n",
    "final_predictions_df = pd.DataFrame(summarized_preds, columns=['product_id', 'prediccion'])\n",
    "\n",
    "final_predictions_df.to_csv('predicciones_finales.csv', index=False)\n",
    "\n",
    "display(final_predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
