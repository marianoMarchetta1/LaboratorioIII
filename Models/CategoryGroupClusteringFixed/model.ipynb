{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Datasets/final_dataset_descr.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>99.43861</td>\n",
       "      <td>99.43861</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>10002</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>38.68301</td>\n",
       "      <td>35.72806</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>10003</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>143.49426</td>\n",
       "      <td>143.49426</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>10004</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>184.72927</td>\n",
       "      <td>184.72927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>10005</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periodo  customer_id  product_id  plan_precios_cuidados  cust_request_qty  \\\n",
       "0   201701        10001       20001                      0                11   \n",
       "1   201701        10002       20001                      0                17   \n",
       "2   201701        10003       20001                      0                17   \n",
       "3   201701        10004       20001                      0                 9   \n",
       "4   201701        10005       20001                      0                23   \n",
       "\n",
       "   cust_request_tn         tn cat1         cat2     cat3  brand  sku_size  \\\n",
       "0         99.43861   99.43861   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "1         38.68301   35.72806   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "2        143.49426  143.49426   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "3        184.72927  184.72927   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "4         19.08407   19.08407   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "\n",
       "  descripcion quarter  month  close_quarter  age  \n",
       "0      genoma      Q1      1              0    0  \n",
       "1      genoma      Q1      1              0    0  \n",
       "2      genoma      Q1      1              0    0  \n",
       "3      genoma      Q1      1              0    0  \n",
       "4      genoma      Q1      1              0    0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1: Filtrar y eliminar productos con poca historia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completamos el dataset con 0 para los producto / cliente que no existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_ids = [20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df[df['product_id'].isin(product_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "\n",
    "df = df[df['periodo'] >= '2018-12-01']\n",
    "\n",
    "product_info = df[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion']].drop_duplicates()\n",
    "\n",
    "min_max_periods = df.groupby(['customer_id', 'product_id'])['periodo'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "cont = 1\n",
    "\n",
    "for _, row in min_max_periods.iterrows():\n",
    "    customer_id = row['customer_id']\n",
    "    product_id = row['product_id']\n",
    "    min_period = row['min']\n",
    "    max_period = '2019-12-01'\n",
    "    all_periods = pd.date_range(min_period, max_period, freq='MS')\n",
    "    \n",
    "    combinations = pd.DataFrame({\n",
    "        'customer_id': [customer_id] * len(all_periods),\n",
    "        'product_id': [product_id] * len(all_periods),\n",
    "        'periodo': all_periods\n",
    "    })\n",
    "    \n",
    "    merged_df = pd.merge(combinations, df, on=['customer_id', 'product_id', 'periodo'], how='left')\n",
    "    \n",
    "    merged_df['tn'] = merged_df['tn'].fillna(0)\n",
    "    \n",
    "    merged_df['tn'] = merged_df['tn'].fillna(0)\n",
    "    merged_df['cat1'] = merged_df['product_id'].map(product_info.set_index('product_id')['cat1'])\n",
    "    merged_df['cat2'] = merged_df['product_id'].map(product_info.set_index('product_id')['cat2'])\n",
    "    merged_df['cat3'] = merged_df['product_id'].map(product_info.set_index('product_id')['cat3'])\n",
    "    merged_df['brand'] = merged_df['product_id'].map(product_info.set_index('product_id')['brand'])\n",
    "    merged_df['sku_size'] = merged_df['product_id'].map(product_info.set_index('product_id')['sku_size'])\n",
    "    merged_df['descripcion'] = merged_df['product_id'].map(product_info.set_index('product_id')['descripcion'])\n",
    "    \n",
    "    merged_df['quarter'] = 'Q' + merged_df['periodo'].dt.to_period('Q').astype(str).str[-1]\n",
    "    merged_df['month'] = merged_df['periodo'].dt.month.astype(str).str.zfill(2)\n",
    "    \n",
    "    merged_df['plan_precios_cuidados'] = merged_df['plan_precios_cuidados'].fillna(0)\n",
    "    merged_df['cust_request_qty'] = merged_df['cust_request_qty'].fillna(0)\n",
    "    merged_df['cust_request_tn'] = merged_df['cust_request_tn'].fillna(0)\n",
    "    merged_df['close_quarter'] = merged_df['close_quarter'].fillna(0)\n",
    "    merged_df['age'] = merged_df['age'].fillna(0)\n",
    "    merged_df['mes_inicial'] = min_period\n",
    "    \n",
    "    all_dfs.append(merged_df)\n",
    "    \n",
    "    print(f\"procesado {cont} de {len(min_max_periods)}\")\n",
    "    cont += 1\n",
    "\n",
    "df_full = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "df_full = df_full.sort_values(by=['customer_id', 'product_id', 'periodo'])\n",
    "\n",
    "df_full['periodo'] = df_full['periodo'].dt.strftime('%Y%m')\n",
    "\n",
    "display(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('final_dataset_completo_con_ceros.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"final_dataset_completo_con_ceros.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10618</td>\n",
       "      <td>20845</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>PISOS</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Profesional menta</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>Gel</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Industrial 5L</td>\n",
       "      <td>Q4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>Gel</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Industrial 5L</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>PISOS</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Profesinal pisos plastificados</td>\n",
       "      <td>Q4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>HC</td>\n",
       "      <td>PROFESIONAL</td>\n",
       "      <td>PISOS</td>\n",
       "      <td>MUSCULO</td>\n",
       "      <td>5000</td>\n",
       "      <td>Profesinal pisos plastificados</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10618       20845   201912                    0.0   \n",
       "2040579        10618       20886   201911                    0.0   \n",
       "2040580        10618       20886   201912                    0.0   \n",
       "2040581        10618       20953   201911                    0.0   \n",
       "2040582        10618       20953   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn         tn cat1         cat2  \\\n",
       "0                    20.0        254.62373  254.62373   HC  ROPA LAVADO   \n",
       "1                    53.0        393.26092  386.60688   HC  ROPA LAVADO   \n",
       "2                    39.0        309.90610  309.90610   HC  ROPA LAVADO   \n",
       "3                    23.0        142.87158  130.54927   HC  ROPA LAVADO   \n",
       "4                    33.0        364.37071  364.37071   HC  ROPA LAVADO   \n",
       "...                   ...              ...        ...  ...          ...   \n",
       "2040578               0.0          0.00000    0.00000   HC  PROFESIONAL   \n",
       "2040579               1.0          0.01884    0.01884   HC  PROFESIONAL   \n",
       "2040580               0.0          0.00000    0.00000   HC  PROFESIONAL   \n",
       "2040581               1.0          0.01817    0.01817   HC  PROFESIONAL   \n",
       "2040582               0.0          0.00000    0.00000   HC  PROFESIONAL   \n",
       "\n",
       "            cat3    brand  sku_size                     descripcion quarter  \\\n",
       "0        Liquido    ARIEL      3000                          genoma      Q4   \n",
       "1        Liquido    ARIEL      3000                          genoma      Q1   \n",
       "2        Liquido    ARIEL      3000                          genoma      Q1   \n",
       "3        Liquido    ARIEL      3000                          genoma      Q1   \n",
       "4        Liquido    ARIEL      3000                          genoma      Q2   \n",
       "...          ...      ...       ...                             ...     ...   \n",
       "2040578    PISOS  MUSCULO      5000               Profesional menta      Q4   \n",
       "2040579      Gel  MUSCULO      5000                   Industrial 5L      Q4   \n",
       "2040580      Gel  MUSCULO      5000                   Industrial 5L      Q4   \n",
       "2040581    PISOS  MUSCULO      5000  Profesinal pisos plastificados      Q4   \n",
       "2040582    PISOS  MUSCULO      5000  Profesinal pisos plastificados      Q4   \n",
       "\n",
       "         month  close_quarter   age mes_inicial  \n",
       "0           12            1.0  23.0  2018-12-01  \n",
       "1            1            0.0  24.0  2018-12-01  \n",
       "2            2            0.0  25.0  2018-12-01  \n",
       "3            3            1.0  26.0  2018-12-01  \n",
       "4            4            0.0  27.0  2018-12-01  \n",
       "...        ...            ...   ...         ...  \n",
       "2040578     12            0.0   0.0  2019-11-01  \n",
       "2040579     11            0.0   4.0  2019-11-01  \n",
       "2040580     12            0.0   0.0  2019-11-01  \n",
       "2040581     11            0.0   4.0  2019-11-01  \n",
       "2040582     12            0.0   0.0  2019-11-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Aplicar LabelEncoder a las columnas categóricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10618</td>\n",
       "      <td>20845</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10618       20845   201912                    0.0   \n",
       "2040579        10618       20886   201911                    0.0   \n",
       "2040580        10618       20886   201912                    0.0   \n",
       "2040581        10618       20953   201911                    0.0   \n",
       "2040582        10618       20953   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn         tn  cat1  cat2  cat3  \\\n",
       "0                    20.0        254.62373  254.62373     1    10    47   \n",
       "1                    53.0        393.26092  386.60688     1    10    47   \n",
       "2                    39.0        309.90610  309.90610     1    10    47   \n",
       "3                    23.0        142.87158  130.54927     1    10    47   \n",
       "4                    33.0        364.37071  364.37071     1    10    47   \n",
       "...                   ...              ...        ...   ...   ...   ...   \n",
       "2040578               0.0          0.00000    0.00000     1     8    54   \n",
       "2040579               1.0          0.01884    0.01884     1     8    31   \n",
       "2040580               0.0          0.00000    0.00000     1     8    31   \n",
       "2040581               1.0          0.01817    0.01817     1     8    54   \n",
       "2040582               0.0          0.00000    0.00000     1     8    54   \n",
       "\n",
       "         brand  sku_size  descripcion  quarter  month  close_quarter   age  \\\n",
       "0            0      3000          384        3     12            1.0  23.0   \n",
       "1            0      3000          384        0      1            0.0  24.0   \n",
       "2            0      3000          384        0      2            0.0  25.0   \n",
       "3            0      3000          384        0      3            1.0  26.0   \n",
       "4            0      3000          384        1      4            0.0  27.0   \n",
       "...        ...       ...          ...      ...    ...            ...   ...   \n",
       "2040578     21      5000          260        3     12            0.0   0.0   \n",
       "2040579     21      5000          158        3     11            0.0   4.0   \n",
       "2040580     21      5000          158        3     12            0.0   0.0   \n",
       "2040581     21      5000          256        3     11            0.0   4.0   \n",
       "2040582     21      5000          256        3     12            0.0   0.0   \n",
       "\n",
       "        mes_inicial  \n",
       "0        2018-12-01  \n",
       "1        2018-12-01  \n",
       "2        2018-12-01  \n",
       "3        2018-12-01  \n",
       "4        2018-12-01  \n",
       "...             ...  \n",
       "2040578  2019-11-01  \n",
       "2040579  2019-11-01  \n",
       "2040580  2019-11-01  \n",
       "2040581  2019-11-01  \n",
       "2040582  2019-11-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'quarter']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_full[col] = le.fit_transform(df_full[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "display(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: Agrupar ventas por periodo, cat1, cat2, cat3, brand, customer_id y product_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "scaled_data = []\n",
    "\n",
    "for (product_id, customer_id), group in df_full.groupby(['product_id', 'customer_id']):\n",
    "    group = group.copy()\n",
    "    scaler = StandardScaler()\n",
    "    group['tn'] = scaler.fit_transform(group[['tn']])\n",
    "    \n",
    "    # Guardar el scaler en el diccionario con una clave única\n",
    "    key = f'{product_id}_{customer_id}'\n",
    "    scalers[key] = scaler\n",
    "    \n",
    "    # Añadir el grupo escalado a la lista\n",
    "    scaled_data.append(group)\n",
    "\n",
    "scaled_df = pd.concat(scaled_data, ignore_index=True)\n",
    "\n",
    "joblib.dump(scalers, 'scalers.pkl')\n",
    "\n",
    "scaled_df.to_csv('scaled_final_dataset.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10618</td>\n",
       "      <td>20845</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>0.01884</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10618</td>\n",
       "      <td>20886</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>0.01817</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10618</td>\n",
       "      <td>20953</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>5000</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10618       20845   201912                    0.0   \n",
       "2040579        10618       20886   201911                    0.0   \n",
       "2040580        10618       20886   201912                    0.0   \n",
       "2040581        10618       20953   201911                    0.0   \n",
       "2040582        10618       20953   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn         tn  cat1  cat2  cat3  \\\n",
       "0                    20.0        254.62373  254.62373     1    10    47   \n",
       "1                    53.0        393.26092  386.60688     1    10    47   \n",
       "2                    39.0        309.90610  309.90610     1    10    47   \n",
       "3                    23.0        142.87158  130.54927     1    10    47   \n",
       "4                    33.0        364.37071  364.37071     1    10    47   \n",
       "...                   ...              ...        ...   ...   ...   ...   \n",
       "2040578               0.0          0.00000    0.00000     1     8    54   \n",
       "2040579               1.0          0.01884    0.01884     1     8    31   \n",
       "2040580               0.0          0.00000    0.00000     1     8    31   \n",
       "2040581               1.0          0.01817    0.01817     1     8    54   \n",
       "2040582               0.0          0.00000    0.00000     1     8    54   \n",
       "\n",
       "         brand  sku_size  descripcion  quarter  month  close_quarter   age  \\\n",
       "0            0      3000          384        3     12            1.0  23.0   \n",
       "1            0      3000          384        0      1            0.0  24.0   \n",
       "2            0      3000          384        0      2            0.0  25.0   \n",
       "3            0      3000          384        0      3            1.0  26.0   \n",
       "4            0      3000          384        1      4            0.0  27.0   \n",
       "...        ...       ...          ...      ...    ...            ...   ...   \n",
       "2040578     21      5000          260        3     12            0.0   0.0   \n",
       "2040579     21      5000          158        3     11            0.0   4.0   \n",
       "2040580     21      5000          158        3     12            0.0   0.0   \n",
       "2040581     21      5000          256        3     11            0.0   4.0   \n",
       "2040582     21      5000          256        3     12            0.0   0.0   \n",
       "\n",
       "        mes_inicial  \n",
       "0        2018-12-01  \n",
       "1        2018-12-01  \n",
       "2        2018-12-01  \n",
       "3        2018-12-01  \n",
       "4        2018-12-01  \n",
       "...             ...  \n",
       "2040578  2019-11-01  \n",
       "2040579  2019-11-01  \n",
       "2040580  2019-11-01  \n",
       "2040581  2019-11-01  \n",
       "2040582  2019-11-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>0.300570</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>1.379834</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>0.752630</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>-0.714023</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1.198002</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>10495</td>\n",
       "      <td>21276</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>-0.238688</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.729144</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00371</td>\n",
       "      <td>1.696976</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>10550</td>\n",
       "      <td>21276</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.729144</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "      <td>412</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  product_id  periodo  plan_precios_cuidados  \\\n",
       "0              10001       20001   201812                    0.0   \n",
       "1              10001       20001   201901                    0.0   \n",
       "2              10001       20001   201902                    0.0   \n",
       "3              10001       20001   201903                    0.0   \n",
       "4              10001       20001   201904                    0.0   \n",
       "...              ...         ...      ...                    ...   \n",
       "2040578        10495       21276   201912                    0.0   \n",
       "2040579        10550       21276   201909                    0.0   \n",
       "2040580        10550       21276   201910                    0.0   \n",
       "2040581        10550       21276   201911                    0.0   \n",
       "2040582        10550       21276   201912                    0.0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn        tn  cat1  cat2  cat3  brand  \\\n",
       "0                    20.0        254.62373  0.300570     1    10    47      0   \n",
       "1                    53.0        393.26092  1.379834     1    10    47      0   \n",
       "2                    39.0        309.90610  0.752630     1    10    47      0   \n",
       "3                    23.0        142.87158 -0.714023     1    10    47      0   \n",
       "4                    33.0        364.37071  1.198002     1    10    47      0   \n",
       "...                   ...              ...       ...   ...   ...   ...    ...   \n",
       "2040578               0.0          0.00000 -0.333333     2     6    18     23   \n",
       "2040579               1.0          0.00075 -0.238688     2     6    18     23   \n",
       "2040580               0.0          0.00000 -0.729144     2     6    18     23   \n",
       "2040581               2.0          0.00371  1.696976     2     6    18     23   \n",
       "2040582               0.0          0.00000 -0.729144     2     6    18     23   \n",
       "\n",
       "         sku_size  descripcion  quarter  month  close_quarter   age  \\\n",
       "0            3000          384        3     12            1.0  23.0   \n",
       "1            3000          384        0      1            0.0  24.0   \n",
       "2            3000          384        0      2            0.0  25.0   \n",
       "3            3000          384        0      3            1.0  26.0   \n",
       "4            3000          384        1      4            0.0  27.0   \n",
       "...           ...          ...      ...    ...            ...   ...   \n",
       "2040578       140          412        3     12            0.0   0.0   \n",
       "2040579       140          412        2      9            1.0   6.0   \n",
       "2040580       140          412        3     10            0.0   0.0   \n",
       "2040581       140          412        3     11            0.0   8.0   \n",
       "2040582       140          412        3     12            0.0   0.0   \n",
       "\n",
       "        mes_inicial  \n",
       "0        2018-12-01  \n",
       "1        2018-12-01  \n",
       "2        2018-12-01  \n",
       "3        2018-12-01  \n",
       "4        2018-12-01  \n",
       "...             ...  \n",
       "2040578  2019-03-01  \n",
       "2040579  2019-09-01  \n",
       "2040580  2019-09-01  \n",
       "2040581  2019-09-01  \n",
       "2040582  2019-09-01  \n",
       "\n",
       "[2040583 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id                int64\n",
       "product_id                 int64\n",
       "periodo                    int64\n",
       "plan_precios_cuidados    float64\n",
       "cust_request_qty         float64\n",
       "cust_request_tn          float64\n",
       "tn                       float64\n",
       "cat1                       int64\n",
       "cat2                       int64\n",
       "cat3                       int64\n",
       "brand                      int64\n",
       "sku_size                   int64\n",
       "descripcion                int64\n",
       "quarter                    int64\n",
       "month                      int64\n",
       "close_quarter            float64\n",
       "age                      float64\n",
       "mes_inicial               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupo y sumarizo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = scaled_df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id', 'quarter', 'month']).agg({\n",
    "    'cust_request_qty': 'sum',\n",
    "    'cust_request_tn': 'sum',\n",
    "    'tn': 'sum'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15840</th>\n",
       "      <td>201812</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>0.300570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80495</th>\n",
       "      <td>201901</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>1.379834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175202</th>\n",
       "      <td>201902</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>0.752630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294172</th>\n",
       "      <td>201903</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>-0.714023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431929</th>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>1.198002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583070</th>\n",
       "      <td>201905</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>439.90647</td>\n",
       "      <td>1.815680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744214</th>\n",
       "      <td>201906</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.92436</td>\n",
       "      <td>-1.242479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916759</th>\n",
       "      <td>201907</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>144.78714</td>\n",
       "      <td>-0.597595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099078</th>\n",
       "      <td>201908</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.63991</td>\n",
       "      <td>-1.506478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288478</th>\n",
       "      <td>201909</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>111.51691</td>\n",
       "      <td>-0.889808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485389</th>\n",
       "      <td>201910</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>21.0</td>\n",
       "      <td>178.49426</td>\n",
       "      <td>-0.342115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687214</th>\n",
       "      <td>201911</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>240.59870</td>\n",
       "      <td>0.153639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892409</th>\n",
       "      <td>201912</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>214.72185</td>\n",
       "      <td>-0.307856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         periodo  cat1  cat2  cat3  brand  customer_id  product_id  quarter  \\\n",
       "15840     201812     1    10    47      0        10001       20001        3   \n",
       "80495     201901     1    10    47      0        10001       20001        0   \n",
       "175202    201902     1    10    47      0        10001       20001        0   \n",
       "294172    201903     1    10    47      0        10001       20001        0   \n",
       "431929    201904     1    10    47      0        10001       20001        1   \n",
       "583070    201905     1    10    47      0        10001       20001        1   \n",
       "744214    201906     1    10    47      0        10001       20001        1   \n",
       "916759    201907     1    10    47      0        10001       20001        2   \n",
       "1099078   201908     1    10    47      0        10001       20001        2   \n",
       "1288478   201909     1    10    47      0        10001       20001        2   \n",
       "1485389   201910     1    10    47      0        10001       20001        3   \n",
       "1687214   201911     1    10    47      0        10001       20001        3   \n",
       "1892409   201912     1    10    47      0        10001       20001        3   \n",
       "\n",
       "         month  cust_request_qty  cust_request_tn        tn  \n",
       "15840       12              20.0        254.62373  0.300570  \n",
       "80495        1              53.0        393.26092  1.379834  \n",
       "175202       2              39.0        309.90610  0.752630  \n",
       "294172       3              23.0        142.87158 -0.714023  \n",
       "431929       4              33.0        364.37071  1.198002  \n",
       "583070       5              31.0        439.90647  1.815680  \n",
       "744214       6               7.0         65.92436 -1.242479  \n",
       "916759       7              14.0        144.78714 -0.597595  \n",
       "1099078      8               9.0         33.63991 -1.506478  \n",
       "1288478      9              18.0        111.51691 -0.889808  \n",
       "1485389     10              21.0        178.49426 -0.342115  \n",
       "1687214     11              21.0        240.59870  0.153639  \n",
       "1892409     12              18.0        214.72185 -0.307856  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(grouped_df[(grouped_df[\"customer_id\"] == 10001) & (grouped_df[\"product_id\"] == 20001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico DTW para agrupar los registros (series de categorias/clientes similares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivoted_df = grouped_df.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'], columns='periodo', values='tn').fillna(0)\n",
    "\n",
    "inertia = []\n",
    "max_clusters = 30\n",
    "\n",
    "for k in range(4, max_clusters + 1):\n",
    "    print(f\"Running K: {k}\")\n",
    "    model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", random_state=0)\n",
    "    model.fit(pivoted_df.values)\n",
    "    inertia.append(model.inertia_)\n",
    "    display(inertia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(4, max_clusters + 1), inertia, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máximo valor de la columna 'nombre_de_la_columna': 3.4641016151377557\n",
      "Mínimo valor de la columna 'nombre_de_la_columna': -2.479356382671416\n"
     ]
    }
   ],
   "source": [
    "# Debug de los valores\n",
    "max_value = pivoted_df[201812].max()\n",
    "min_value = pivoted_df[201812].min()\n",
    "\n",
    "print(f\"Máximo valor de la columna 'nombre_de_la_columna': {max_value}\")\n",
    "print(f\"Mínimo valor de la columna 'nombre_de_la_columna': {min_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2.296914</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10002</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.27780</td>\n",
       "      <td>1.211273</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10003</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.27256</td>\n",
       "      <td>0.342311</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10004</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.13628</td>\n",
       "      <td>-1.040164</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10005</td>\n",
       "      <td>20609</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>-0.011106</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040578</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10367</td>\n",
       "      <td>21222</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040579</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10482</td>\n",
       "      <td>21192</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040580</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10482</td>\n",
       "      <td>21222</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040581</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10513</td>\n",
       "      <td>21222</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040582</th>\n",
       "      <td>201912</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>21192</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.534522</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040583 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         periodo  cat1  cat2  cat3  brand  customer_id  product_id  quarter  \\\n",
       "0         201812     0     0     4     22        10001       20609        3   \n",
       "1         201812     0     0     4     22        10002       20609        3   \n",
       "2         201812     0     0     4     22        10003       20609        3   \n",
       "3         201812     0     0     4     22        10004       20609        3   \n",
       "4         201812     0     0     4     22        10005       20609        3   \n",
       "...          ...   ...   ...   ...    ...          ...         ...      ...   \n",
       "2040578   201912     3    13    82     32        10367       21222        3   \n",
       "2040579   201912     3    13    82     32        10482       21192        3   \n",
       "2040580   201912     3    13    82     32        10482       21222        3   \n",
       "2040581   201912     3    13    82     32        10513       21222        3   \n",
       "2040582   201912     3    13    82     32        10552       21192        3   \n",
       "\n",
       "         month  cust_request_qty  cust_request_tn        tn  cluster  \n",
       "0           12               6.0          0.87535  2.296914       25  \n",
       "1           12               8.0          0.27780  1.211273       15  \n",
       "2           12               1.0          0.27256  0.342311       13  \n",
       "3           12               1.0          0.13628 -1.040164       27  \n",
       "4           12               7.0          0.06290 -0.011106       28  \n",
       "...        ...               ...              ...       ...      ...  \n",
       "2040578     12               0.0          0.00000 -0.426401        3  \n",
       "2040579     12               0.0          0.00000 -0.377964        0  \n",
       "2040580     12               0.0          0.00000 -0.377964        0  \n",
       "2040581     12               0.0          0.00000 -0.577350       24  \n",
       "2040582     12               0.0          0.00000 -0.534522        4  \n",
       "\n",
       "[2040583 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_clusters = 30\n",
    "model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=0)\n",
    "cluster_labels = model.fit_predict(pivoted_df.values)\n",
    "\n",
    "pivoted_df['cluster'] = cluster_labels\n",
    "\n",
    "grouped_df = grouped_df.merge(pivoted_df['cluster'], left_on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'], right_index=True)\n",
    "\n",
    "grouped_df.to_csv('grouped_with_30_clusters_scaled.csv', index=False)\n",
    "\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5: Armar un modelo LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "le_factor = 0.1\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(le_factor), input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(LSTM(256, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    \n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(256, activation='tanh', kernel_regularizer=l2(le_factor), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(128, activation='tanh', kernel_regularizer=l2(le_factor)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(128, activation='tanh', kernel_regularizer=l2(le_factor)))\n",
    "    model.add(Dense(64, activation='tanh', kernel_regularizer=l2(le_factor))) \n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 6: Entrenar y predecir con el modelo LSTM para cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "  Número de registros: 150078\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 21329\n",
      "\n",
      "Cluster 1:\n",
      "  Número de registros: 45339\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3798\n",
      "\n",
      "Cluster 2:\n",
      "  Número de registros: 17991\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 7463\n",
      "\n",
      "Cluster 3:\n",
      "  Número de registros: 76999\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5923\n",
      "\n",
      "Cluster 4:\n",
      "  Número de registros: 140100\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 13456\n",
      "\n",
      "Cluster 5:\n",
      "  Número de registros: 29631\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 2417\n",
      "\n",
      "Cluster 6:\n",
      "  Número de registros: 66404\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5108\n",
      "\n",
      "Cluster 7:\n",
      "  Número de registros: 19475\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3212\n",
      "\n",
      "Cluster 8:\n",
      "  Número de registros: 81501\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 7180\n",
      "\n",
      "Cluster 9:\n",
      "  Número de registros: 106399\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 9811\n",
      "\n",
      "Cluster 10:\n",
      "  Número de registros: 63869\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4913\n",
      "\n",
      "Cluster 11:\n",
      "  Número de registros: 118606\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 10816\n",
      "\n",
      "Cluster 12:\n",
      "  Número de registros: 211805\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 19800\n",
      "\n",
      "Cluster 13:\n",
      "  Número de registros: 74657\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 8945\n",
      "\n",
      "Cluster 14:\n",
      "  Número de registros: 41593\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3553\n",
      "\n",
      "Cluster 15:\n",
      "  Número de registros: 44096\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3392\n",
      "\n",
      "Cluster 16:\n",
      "  Número de registros: 67374\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5661\n",
      "\n",
      "Cluster 17:\n",
      "  Número de registros: 68278\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5720\n",
      "\n",
      "Cluster 18:\n",
      "  Número de registros: 65505\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 7851\n",
      "\n",
      "Cluster 19:\n",
      "  Número de registros: 60274\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 5573\n",
      "\n",
      "Cluster 20:\n",
      "  Número de registros: 49884\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4266\n",
      "\n",
      "Cluster 21:\n",
      "  Número de registros: 38015\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3701\n",
      "\n",
      "Cluster 22:\n",
      "  Número de registros: 37441\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 3437\n",
      "\n",
      "Cluster 23:\n",
      "  Número de registros: 70929\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 6186\n",
      "\n",
      "Cluster 24:\n",
      "  Número de registros: 66083\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 14036\n",
      "\n",
      "Cluster 25:\n",
      "  Número de registros: 32071\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 2467\n",
      "\n",
      "Cluster 26:\n",
      "  Número de registros: 18550\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 1428\n",
      "\n",
      "Cluster 27:\n",
      "  Número de registros: 73702\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 6268\n",
      "\n",
      "Cluster 28:\n",
      "  Número de registros: 50813\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4350\n",
      "\n",
      "Cluster 29:\n",
      "  Número de registros: 53121\n",
      "  Número de combinaciones únicas 'customer_id' y 'product_id': 4798\n",
      "\n",
      "Total 206858\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "total = 0\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_number = i\n",
    "\n",
    "    # Filtrar el DataFrame por el cluster deseado\n",
    "    cluster_data = grouped_df[grouped_df['cluster'] == cluster_number]\n",
    "\n",
    "    unique_combinations = cluster_data[['customer_id', 'product_id']].drop_duplicates().shape[0]\n",
    "    total += unique_combinations\n",
    "    print(f\"Cluster {cluster_number}:\")\n",
    "    print(f\"  Número de registros: {len(cluster_data)}\")\n",
    "    print(f\"  Número de combinaciones únicas 'customer_id' y 'product_id': {unique_combinations}\")\n",
    "    print()\n",
    "    \n",
    "print(f\"Total {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo los pesos y los guardo en un dic para mejorar la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20001: 1504.68856,\n",
       " 20002: 1087.30855,\n",
       " 20003: 892.50129,\n",
       " 20004: 637.90002,\n",
       " 20005: 593.24443,\n",
       " 20006: 417.23228,\n",
       " 20007: 390.43432,\n",
       " 20008: 195.36854,\n",
       " 20009: 495.03574000000003,\n",
       " 20010: 359.59998,\n",
       " 20011: 392.3829,\n",
       " 20012: 173.13004,\n",
       " 20013: 318.09141,\n",
       " 20014: 272.02812,\n",
       " 20015: 297.27663,\n",
       " 20016: 273.20202,\n",
       " 20017: 216.90773,\n",
       " 20018: 141.63569999999999,\n",
       " 20019: 351.54708,\n",
       " 20020: 266.06358,\n",
       " 20021: 203.76721,\n",
       " 20022: 210.8346,\n",
       " 20023: 181.13277,\n",
       " 20024: 270.45018,\n",
       " 20025: 241.83432000000002,\n",
       " 20026: 235.10419,\n",
       " 20027: 155.25876,\n",
       " 20028: 109.92618,\n",
       " 20029: 150.64869,\n",
       " 20030: 102.7572,\n",
       " 20031: 139.91577,\n",
       " 20032: 527.79811,\n",
       " 20033: 96.76212,\n",
       " 20035: 179.97912,\n",
       " 20037: 63.37274,\n",
       " 20038: 157.68477000000001,\n",
       " 20039: 128.40394,\n",
       " 20041: 113.11379,\n",
       " 20042: 124.20086,\n",
       " 20043: 93.77222,\n",
       " 20044: 59.61747,\n",
       " 20045: 149.89961,\n",
       " 20046: 149.9563,\n",
       " 20047: 71.49763,\n",
       " 20049: 124.84836,\n",
       " 20050: 117.02742,\n",
       " 20051: 132.46038000000001,\n",
       " 20052: 95.51068,\n",
       " 20053: 146.36584,\n",
       " 20054: 121.2091,\n",
       " 20055: 65.85747,\n",
       " 20056: 63.7182,\n",
       " 20057: 141.36774,\n",
       " 20058: 117.82985,\n",
       " 20059: 111.51138999999999,\n",
       " 20061: 106.44895,\n",
       " 20062: 90.97637,\n",
       " 20063: 103.52372,\n",
       " 20065: 77.98518,\n",
       " 20066: 119.28134,\n",
       " 20067: 88.49566,\n",
       " 20068: 92.92737,\n",
       " 20069: 136.63984,\n",
       " 20070: 68.44316,\n",
       " 20071: 104.77091,\n",
       " 20072: 32.2686,\n",
       " 20073: 122.47019,\n",
       " 20074: 98.07553,\n",
       " 20075: 105.42961,\n",
       " 20076: 68.45922,\n",
       " 20077: 34.58847,\n",
       " 20079: 27.68496,\n",
       " 20080: 8.84426,\n",
       " 20081: 94.25871,\n",
       " 20082: 78.87082,\n",
       " 20084: 94.68236,\n",
       " 20085: 221.56404,\n",
       " 20086: 42.51979,\n",
       " 20087: 89.63008,\n",
       " 20089: 102.33626,\n",
       " 20090: 35.97712,\n",
       " 20091: 92.24345,\n",
       " 20092: 47.45286,\n",
       " 20093: 73.28412,\n",
       " 20094: 59.12954,\n",
       " 20095: 28.52853,\n",
       " 20096: 44.24611,\n",
       " 20097: 55.29078,\n",
       " 20099: 57.827130000000004,\n",
       " 20100: 1.3792,\n",
       " 20101: 72.80091,\n",
       " 20102: 19.17797,\n",
       " 20103: 63.99622,\n",
       " 20106: 74.51262,\n",
       " 20107: 86.05666,\n",
       " 20108: 58.34718,\n",
       " 20109: 61.8432,\n",
       " 20111: 93.88818,\n",
       " 20112: 107.06263,\n",
       " 20114: 44.16048,\n",
       " 20116: 118.60205,\n",
       " 20117: 36.0325,\n",
       " 20118: 61.15123,\n",
       " 20119: 36.26867,\n",
       " 20120: 72.21068,\n",
       " 20121: 78.96685000000001,\n",
       " 20122: 64.62255,\n",
       " 20123: 59.148179999999996,\n",
       " 20124: 38.10841,\n",
       " 20125: 47.559329999999996,\n",
       " 20126: 94.95288,\n",
       " 20127: 170.32792,\n",
       " 20129: 52.92339,\n",
       " 20130: 83.3077,\n",
       " 20132: 40.09161,\n",
       " 20133: 30.835350000000002,\n",
       " 20134: 49.98357,\n",
       " 20135: 101.03175,\n",
       " 20137: 37.55127,\n",
       " 20138: 28.33495,\n",
       " 20139: 53.48889,\n",
       " 20140: 58.149,\n",
       " 20142: 35.89411,\n",
       " 20143: 29.92779,\n",
       " 20144: 41.30604,\n",
       " 20145: 65.41274,\n",
       " 20146: 30.472260000000002,\n",
       " 20148: 57.163380000000004,\n",
       " 20150: 51.35196,\n",
       " 20151: 44.81184,\n",
       " 20152: 23.926669999999998,\n",
       " 20153: 48.78881,\n",
       " 20155: 61.25175,\n",
       " 20157: 55.77624,\n",
       " 20158: 55.02588,\n",
       " 20159: 61.78339,\n",
       " 20160: 43.18815,\n",
       " 20161: 34.9225,\n",
       " 20162: 51.43634,\n",
       " 20164: 71.3349,\n",
       " 20166: 33.52579,\n",
       " 20167: 43.851729999999996,\n",
       " 20168: 15.155560000000001,\n",
       " 20170: 64.4108,\n",
       " 20174: 91.1178,\n",
       " 20175: 19.70895,\n",
       " 20176: 23.87612,\n",
       " 20177: 24.51696,\n",
       " 20179: 39.70194,\n",
       " 20180: 36.95611,\n",
       " 20181: 33.36284,\n",
       " 20182: 20.87961,\n",
       " 20183: 20.47573,\n",
       " 20184: 26.13296,\n",
       " 20187: 19.99026,\n",
       " 20188: 22.88825,\n",
       " 20189: 32.59489,\n",
       " 20192: 21.45354,\n",
       " 20193: 24.92724,\n",
       " 20196: 35.180600000000005,\n",
       " 20197: 27.90918,\n",
       " 20198: 22.40804,\n",
       " 20200: 20.87857,\n",
       " 20201: 26.166990000000002,\n",
       " 20202: 24.48113,\n",
       " 20203: 55.2097,\n",
       " 20205: 29.13191,\n",
       " 20206: 23.43432,\n",
       " 20207: 18.51692,\n",
       " 20208: 20.14227,\n",
       " 20209: 16.08116,\n",
       " 20210: 58.78261,\n",
       " 20211: 16.10922,\n",
       " 20212: 22.494790000000002,\n",
       " 20213: 58.84043,\n",
       " 20215: 23.14009,\n",
       " 20216: 9.20284,\n",
       " 20218: 31.369320000000002,\n",
       " 20219: 23.10671,\n",
       " 20220: 19.46485,\n",
       " 20222: 21.01334,\n",
       " 20224: 21.26943,\n",
       " 20225: 16.66252,\n",
       " 20226: 11.32801,\n",
       " 20227: 38.712650000000004,\n",
       " 20228: 5.0385100000000005,\n",
       " 20230: 18.17921,\n",
       " 20231: 21.31542,\n",
       " 20232: 28.2828,\n",
       " 20233: 35.59532,\n",
       " 20234: 4.84796,\n",
       " 20235: 32.13552,\n",
       " 20236: 34.70341,\n",
       " 20237: 29.51673,\n",
       " 20238: 16.61631,\n",
       " 20239: 20.10279,\n",
       " 20240: 19.51977,\n",
       " 20241: 12.8158,\n",
       " 20242: 4.03604,\n",
       " 20244: 26.46154,\n",
       " 20246: 24.60939,\n",
       " 20249: 8.00053,\n",
       " 20251: 14.79641,\n",
       " 20252: 12.20597,\n",
       " 20253: 15.26885,\n",
       " 20254: 17.46847,\n",
       " 20255: 12.15816,\n",
       " 20256: 17.63436,\n",
       " 20257: 51.77148,\n",
       " 20259: 12.43223,\n",
       " 20261: 43.74576,\n",
       " 20262: 25.6396,\n",
       " 20263: 20.855059999999998,\n",
       " 20264: 16.582530000000002,\n",
       " 20265: 4.57848,\n",
       " 20266: 49.55769,\n",
       " 20267: 18.01748,\n",
       " 20268: 11.99271,\n",
       " 20269: 19.13008,\n",
       " 20270: 6.778370000000001,\n",
       " 20271: 8.34905,\n",
       " 20272: 12.66437,\n",
       " 20273: 12.4634,\n",
       " 20275: 14.93964,\n",
       " 20276: 18.62694,\n",
       " 20277: 9.61208,\n",
       " 20278: 6.27424,\n",
       " 20280: 17.02489,\n",
       " 20281: 9.81001,\n",
       " 20282: 12.8247,\n",
       " 20283: 12.82352,\n",
       " 20284: 11.79748,\n",
       " 20285: 4.3322,\n",
       " 20286: 50.55931,\n",
       " 20288: 21.12175,\n",
       " 20289: 10.01,\n",
       " 20290: 21.56585,\n",
       " 20291: 9.75289,\n",
       " 20292: 9.80518,\n",
       " 20295: 6.04878,\n",
       " 20296: 18.64666,\n",
       " 20297: 35.089,\n",
       " 20298: 11.28594,\n",
       " 20299: 10.61151,\n",
       " 20300: 17.14787,\n",
       " 20301: 8.31315,\n",
       " 20302: 2.35687,\n",
       " 20303: 18.3878,\n",
       " 20304: 5.88588,\n",
       " 20305: 2.56676,\n",
       " 20306: 32.270920000000004,\n",
       " 20307: 13.525310000000001,\n",
       " 20309: 8.112119999999999,\n",
       " 20310: 9.749849999999999,\n",
       " 20311: 18.16724,\n",
       " 20313: 27.15804,\n",
       " 20314: 18.852050000000002,\n",
       " 20315: 16.64451,\n",
       " 20316: 27.23436,\n",
       " 20317: 14.823310000000001,\n",
       " 20319: 11.81172,\n",
       " 20320: 13.48012,\n",
       " 20321: 27.62797,\n",
       " 20322: 5.57864,\n",
       " 20323: 24.40797,\n",
       " 20324: 10.957410000000001,\n",
       " 20325: 5.26778,\n",
       " 20326: 15.6484,\n",
       " 20327: 11.90046,\n",
       " 20328: 9.333870000000001,\n",
       " 20329: 11.05355,\n",
       " 20330: 13.79536,\n",
       " 20332: 12.96845,\n",
       " 20334: 0.18631999999999999,\n",
       " 20335: 11.7811,\n",
       " 20336: 8.61019,\n",
       " 20337: 13.82469,\n",
       " 20338: 8.13739,\n",
       " 20340: 8.06882,\n",
       " 20341: 8.46183,\n",
       " 20342: 11.97748,\n",
       " 20343: 11.96007,\n",
       " 20344: 15.9908,\n",
       " 20346: 14.96339,\n",
       " 20348: 11.72618,\n",
       " 20349: 17.46373,\n",
       " 20350: 1.23931,\n",
       " 20351: 14.08152,\n",
       " 20352: 17.35029,\n",
       " 20353: 6.63564,\n",
       " 20354: 12.29584,\n",
       " 20355: 25.633470000000003,\n",
       " 20356: 13.82404,\n",
       " 20357: 18.325,\n",
       " 20358: 10.92,\n",
       " 20359: 14.16642,\n",
       " 20361: 13.57393,\n",
       " 20362: 6.61718,\n",
       " 20364: 30.711399999999998,\n",
       " 20365: 10.90371,\n",
       " 20366: 10.34722,\n",
       " 20367: 9.55006,\n",
       " 20368: 10.4105,\n",
       " 20372: 4.66326,\n",
       " 20375: 4.38548,\n",
       " 20377: 7.24521,\n",
       " 20378: 7.68167,\n",
       " 20379: 6.4533,\n",
       " 20380: 14.64208,\n",
       " 20381: 18.21145,\n",
       " 20382: 9.3097,\n",
       " 20383: 7.43444,\n",
       " 20384: 7.21169,\n",
       " 20385: 7.4722,\n",
       " 20386: 11.22573,\n",
       " 20387: 8.58751,\n",
       " 20388: 6.6502799999999995,\n",
       " 20389: 34.54417,\n",
       " 20390: 9.2709,\n",
       " 20395: 26.65657,\n",
       " 20396: 5.93867,\n",
       " 20398: 1.0473,\n",
       " 20400: 9.74484,\n",
       " 20401: 9.16454,\n",
       " 20402: 15.10782,\n",
       " 20403: 10.9533,\n",
       " 20404: 9.25704,\n",
       " 20406: 10.276150000000001,\n",
       " 20407: 6.63483,\n",
       " 20408: 11.77663,\n",
       " 20409: 2.40926,\n",
       " 20410: 10.14597,\n",
       " 20411: 9.12526,\n",
       " 20414: 28.50819,\n",
       " 20415: 4.85906,\n",
       " 20416: 0.8585,\n",
       " 20417: 7.38835,\n",
       " 20418: 0.5267000000000001,\n",
       " 20419: 13.59539,\n",
       " 20421: 11.95817,\n",
       " 20422: 0.92601,\n",
       " 20424: 5.42003,\n",
       " 20426: 8.05316,\n",
       " 20428: 1.65654,\n",
       " 20429: 0.9946900000000001,\n",
       " 20432: 5.4624,\n",
       " 20433: 15.75243,\n",
       " 20434: 8.25951,\n",
       " 20438: 16.27824,\n",
       " 20440: 34.28062,\n",
       " 20442: 23.79055,\n",
       " 20443: 8.00212,\n",
       " 20447: 6.65567,\n",
       " 20449: 1.08866,\n",
       " 20450: 6.24297,\n",
       " 20453: 16.32757,\n",
       " 20456: 6.10371,\n",
       " 20458: 22.48458,\n",
       " 20459: 15.42616,\n",
       " 20460: 8.94893,\n",
       " 20463: 10.21467,\n",
       " 20464: 3.55347,\n",
       " 20465: 3.54101,\n",
       " 20466: 7.27034,\n",
       " 20469: 7.43516,\n",
       " 20470: 8.27397,\n",
       " 20473: 6.6120600000000005,\n",
       " 20474: 9.65317,\n",
       " 20476: 19.73377,\n",
       " 20477: 6.01202,\n",
       " 20478: 5.35491,\n",
       " 20479: 7.92463,\n",
       " 20480: 5.21702,\n",
       " 20481: 8.74287,\n",
       " 20482: 5.68619,\n",
       " 20483: 8.52225,\n",
       " 20484: 1.56548,\n",
       " 20488: 8.62499,\n",
       " 20490: 0.48618999999999996,\n",
       " 20491: 11.94289,\n",
       " 20495: 12.65039,\n",
       " 20496: 6.49094,\n",
       " 20497: 5.5455,\n",
       " 20500: 8.61567,\n",
       " 20502: 5.83704,\n",
       " 20503: 0.83948,\n",
       " 20505: 1.41934,\n",
       " 20508: 4.02445,\n",
       " 20509: 1.10399,\n",
       " 20510: 9.0775,\n",
       " 20513: 12.77476,\n",
       " 20514: 5.77288,\n",
       " 20517: 4.0131,\n",
       " 20520: 6.9888,\n",
       " 20521: 9.54402,\n",
       " 20522: 12.64676,\n",
       " 20523: 7.63168,\n",
       " 20524: 3.37671,\n",
       " 20525: 15.9413,\n",
       " 20526: 13.65706,\n",
       " 20527: 7.74072,\n",
       " 20530: 2.70755,\n",
       " 20531: 12.40991,\n",
       " 20532: 6.38886,\n",
       " 20536: 5.33565,\n",
       " 20537: 12.0668,\n",
       " 20538: 6.02223,\n",
       " 20539: 5.81923,\n",
       " 20540: 6.4430000000000005,\n",
       " 20541: 8.18859,\n",
       " 20542: 8.25353,\n",
       " 20544: 6.43633,\n",
       " 20547: 12.23047,\n",
       " 20548: 12.24028,\n",
       " 20549: 4.18343,\n",
       " 20551: 5.13514,\n",
       " 20552: 2.39711,\n",
       " 20553: 7.04809,\n",
       " 20555: 1.79267,\n",
       " 20558: 3.4579999999999997,\n",
       " 20559: 7.1281,\n",
       " 20560: 5.1622,\n",
       " 20561: 0.37611,\n",
       " 20563: 7.4654,\n",
       " 20565: 3.83425,\n",
       " 20567: 7.02946,\n",
       " 20568: 2.85831,\n",
       " 20569: 12.84918,\n",
       " 20570: 6.88673,\n",
       " 20571: 7.32115,\n",
       " 20572: 3.90293,\n",
       " 20574: 2.60938,\n",
       " 20575: 9.11891,\n",
       " 20576: 12.3194,\n",
       " 20577: 8.44628,\n",
       " 20578: 3.4644,\n",
       " 20579: 3.101,\n",
       " 20580: 15.91044,\n",
       " 20583: 3.65662,\n",
       " 20585: 7.09842,\n",
       " 20586: 0.33821,\n",
       " 20588: 0.6005999999999999,\n",
       " 20589: 6.74483,\n",
       " 20592: 12.04725,\n",
       " 20593: 8.18131,\n",
       " 20596: 4.49828,\n",
       " 20597: 0.23346,\n",
       " 20599: 4.55627,\n",
       " 20600: 1.55091,\n",
       " 20601: 0.57274,\n",
       " 20602: 3.26558,\n",
       " 20603: 7.70408,\n",
       " 20604: 13.179879999999999,\n",
       " 20605: 6.43393,\n",
       " 20606: 2.34434,\n",
       " 20609: 0.47173,\n",
       " 20611: 9.19572,\n",
       " 20612: 4.18386,\n",
       " 20614: 2.23479,\n",
       " 20615: 14.06805,\n",
       " 20617: 1.36963,\n",
       " 20620: 6.91217,\n",
       " 20621: 7.23179,\n",
       " 20622: 4.72215,\n",
       " 20623: 10.20951,\n",
       " 20624: 5.34485,\n",
       " 20627: 9.34447,\n",
       " 20628: 3.28467,\n",
       " 20629: 2.32608,\n",
       " 20632: 2.05983,\n",
       " 20633: 6.92866,\n",
       " 20636: 0.37304,\n",
       " 20637: 4.74964,\n",
       " 20638: 8.33849,\n",
       " 20639: 4.9791,\n",
       " 20640: 2.72739,\n",
       " 20641: 2.19101,\n",
       " 20642: 3.02384,\n",
       " 20644: 2.6208,\n",
       " 20646: 4.05327,\n",
       " 20647: 1.92544,\n",
       " 20649: 14.79183,\n",
       " 20651: 2.37627,\n",
       " 20652: 3.69362,\n",
       " 20653: 3.43338,\n",
       " 20654: 5.23921,\n",
       " 20655: 2.58001,\n",
       " 20657: 3.71165,\n",
       " 20658: 3.40902,\n",
       " 20659: 7.73487,\n",
       " 20660: 0.22825,\n",
       " 20661: 3.46726,\n",
       " 20662: 11.50205,\n",
       " 20663: 0.17495,\n",
       " 20664: 2.96002,\n",
       " 20666: 2.1839999999999997,\n",
       " 20667: 2.54736,\n",
       " 20670: 0.8472799999999999,\n",
       " 20672: 3.4144799999999997,\n",
       " 20673: 9.15194,\n",
       " 20674: 8.38539,\n",
       " 20676: 2.1457800000000002,\n",
       " 20677: 2.29468,\n",
       " 20678: 0.391,\n",
       " 20679: 10.29418,\n",
       " 20680: 5.18177,\n",
       " 20681: 8.83008,\n",
       " 20682: 5.3508000000000004,\n",
       " 20684: 0.76875,\n",
       " 20685: 0.9511499999999999,\n",
       " 20686: 9.60049,\n",
       " 20689: 2.2932,\n",
       " 20691: 7.99868,\n",
       " 20693: 3.71123,\n",
       " 20694: 8.90416,\n",
       " 20696: 4.999,\n",
       " 20697: 2.59869,\n",
       " 20699: 3.03395,\n",
       " 20700: 3.11353,\n",
       " 20701: 3.76215,\n",
       " 20702: 2.03063,\n",
       " 20703: 9.4657,\n",
       " 20705: 1.0319399999999999,\n",
       " 20706: 4.85225,\n",
       " 20708: 4.31759,\n",
       " 20709: 8.64319,\n",
       " 20711: 7.00535,\n",
       " 20713: 1.468,\n",
       " 20714: 4.22997,\n",
       " 20715: 1.62241,\n",
       " 20719: 7.74604,\n",
       " 20720: 8.95012,\n",
       " 20721: 0.10375,\n",
       " 20724: 1.47147,\n",
       " 20729: 0.3598,\n",
       " 20730: 1.58932,\n",
       " 20732: 5.96173,\n",
       " 20733: 1.11124,\n",
       " 20735: 1.37555,\n",
       " 20737: 2.07177,\n",
       " 20739: 2.92295,\n",
       " 20741: 1.35364,\n",
       " 20742: 0.91993,\n",
       " 20743: 1.11232,\n",
       " 20744: 3.90804,\n",
       " 20745: 1.30308,\n",
       " 20746: 4.26365,\n",
       " 20749: 2.73283,\n",
       " 20750: 1.37322,\n",
       " 20751: 3.04284,\n",
       " 20754: 8.55531,\n",
       " 20756: 4.40403,\n",
       " 20757: 2.37376,\n",
       " 20758: 4.3724300000000005,\n",
       " 20759: 1.49063,\n",
       " 20761: 2.47668,\n",
       " 20762: 6.8284899999999995,\n",
       " 20765: 0.25988,\n",
       " 20768: 1.51197,\n",
       " 20771: 0.95168,\n",
       " 20772: 2.53867,\n",
       " 20773: 0.18296,\n",
       " 20774: 5.44871,\n",
       " 20775: 2.19749,\n",
       " 20777: 2.07783,\n",
       " 20781: 0.70834,\n",
       " 20783: 3.65745,\n",
       " 20785: 2.93047,\n",
       " 20786: 0.60138,\n",
       " 20788: 1.47053,\n",
       " 20789: 1.4464299999999999,\n",
       " 20793: 2.64061,\n",
       " 20795: 3.56276,\n",
       " 20800: 0.66127,\n",
       " 20801: 1.66194,\n",
       " 20802: 1.04494,\n",
       " 20803: 1.01973,\n",
       " 20807: 2.14955,\n",
       " 20809: 3.19917,\n",
       " 20810: 1.60242,\n",
       " 20811: 2.65751,\n",
       " 20812: 0.76986,\n",
       " 20815: 6.33522,\n",
       " 20817: 1.35408,\n",
       " 20818: 1.56334,\n",
       " 20820: 1.8666099999999999,\n",
       " 20822: 5.92346,\n",
       " 20823: 1.7907,\n",
       " 20824: 0.28688,\n",
       " 20826: 1.78647,\n",
       " 20827: 2.60942,\n",
       " 20828: 0.47617,\n",
       " 20830: 1.11598,\n",
       " 20831: 2.65266,\n",
       " 20832: 0.23123,\n",
       " 20835: 0.9755199999999999,\n",
       " 20838: 1.96905,\n",
       " 20840: 1.73797,\n",
       " 20843: 0.77417,\n",
       " 20845: 2.22201,\n",
       " 20846: 0.55875,\n",
       " 20847: 1.02539,\n",
       " 20849: 2.0799600000000003,\n",
       " 20852: 0.29701,\n",
       " 20853: 2.8984199999999998,\n",
       " 20855: 1.27043,\n",
       " 20859: 3.06268,\n",
       " 20862: 1.30819,\n",
       " 20863: 1.09065,\n",
       " 20864: 1.22199,\n",
       " 20865: 1.69039,\n",
       " 20870: 1.32583,\n",
       " 20877: 0.8133,\n",
       " 20878: 1.26697,\n",
       " 20879: 1.46764,\n",
       " 20882: 1.48505,\n",
       " 20883: 0.86097,\n",
       " 20885: 1.4784899999999999,\n",
       " 20886: 1.09253,\n",
       " 20892: 1.0854000000000001,\n",
       " 20894: 1.89225,\n",
       " 20899: 2.32761,\n",
       " 20901: 0.73891,\n",
       " 20902: 1.56067,\n",
       " 20904: 4.55923,\n",
       " 20906: 0.72535,\n",
       " 20907: 3.47802,\n",
       " 20908: 2.29554,\n",
       " 20910: 5.20738,\n",
       " 20912: 3.03888,\n",
       " 20913: 0.78256,\n",
       " 20914: 1.06175,\n",
       " 20917: 2.29184,\n",
       " 20919: 0.43074,\n",
       " 20920: 2.4836,\n",
       " 20922: 1.04686,\n",
       " 20924: 3.70952,\n",
       " 20925: 0.53211,\n",
       " 20927: 3.05867,\n",
       " 20928: 2.92793,\n",
       " 20931: 1.43487,\n",
       " 20932: 1.8699999999999999,\n",
       " 20933: 3.68027,\n",
       " 20936: 0.16453,\n",
       " 20937: 1.03081,\n",
       " 20941: 0.82051,\n",
       " 20942: 1.72156,\n",
       " 20945: 0.86828,\n",
       " 20946: 3.14228,\n",
       " 20947: 0.45535000000000003,\n",
       " 20948: 0.41822,\n",
       " 20949: 0.88794,\n",
       " 20951: 0.3287,\n",
       " 20953: 2.3988,\n",
       " 20956: 0.59267,\n",
       " 20957: 1.10726,\n",
       " 20961: 0.83519,\n",
       " 20962: 1.99182,\n",
       " 20965: 0.55034,\n",
       " 20966: 1.73472,\n",
       " 20967: 1.55408,\n",
       " 20968: 1.91479,\n",
       " 20970: 0.43677,\n",
       " 20975: 1.69045,\n",
       " 20976: 0.61586,\n",
       " 20982: 0.13177999999999998,\n",
       " 20985: 0.7459899999999999,\n",
       " 20986: 0.57885,\n",
       " 20987: 0.16926,\n",
       " 20990: 0.4538,\n",
       " 20991: 0.14446,\n",
       " 20994: 0.18545,\n",
       " 20995: 1.55285,\n",
       " 20996: 0.22495,\n",
       " 20997: 0.88972,\n",
       " 21001: 0.52382,\n",
       " 21003: 0.44116,\n",
       " 21006: 0.96358,\n",
       " 21007: 1.8007300000000002,\n",
       " 21008: 0.99321,\n",
       " 21014: 1.09473,\n",
       " 21016: 0.45426,\n",
       " 21022: 0.76216,\n",
       " 21024: 0.39419,\n",
       " 21027: 1.73954,\n",
       " 21028: 0.54486,\n",
       " 21032: 0.17637,\n",
       " 21033: 1.2670000000000001,\n",
       " 21034: 0.55135,\n",
       " 21035: 1.80884,\n",
       " 21037: 0.4412,\n",
       " 21038: 0.4357,\n",
       " 21039: 1.9437399999999998,\n",
       " 21040: 0.6441600000000001,\n",
       " 21042: 1.61798,\n",
       " 21044: 1.76904,\n",
       " 21048: 0.45052000000000003,\n",
       " 21049: 0.30829999999999996,\n",
       " 21055: 0.41164,\n",
       " 21056: 0.64179,\n",
       " 21057: 0.67593,\n",
       " 21058: 1.84115,\n",
       " 21064: 0.32759,\n",
       " 21065: 0.59662,\n",
       " 21073: 0.75292,\n",
       " 21074: 0.00311,\n",
       " 21077: 0.21584,\n",
       " 21079: 0.68459,\n",
       " 21080: 0.37668999999999997,\n",
       " 21084: 0.47287999999999997,\n",
       " 21086: 0.63302,\n",
       " 21087: 1.02205,\n",
       " 21088: 0.08061,\n",
       " 21092: 0.99937,\n",
       " 21093: 0.2881,\n",
       " 21097: 1.34469,\n",
       " 21099: 0.17034,\n",
       " 21105: 0.43487,\n",
       " 21109: 0.9539,\n",
       " 21110: 1.52502,\n",
       " 21111: 1.46188,\n",
       " 21112: 0.55859,\n",
       " 21114: 1.49352,\n",
       " 21118: 0.30167,\n",
       " 21119: 1.14641,\n",
       " 21126: 0.47548,\n",
       " 21129: 0.7841,\n",
       " 21131: 0.08076,\n",
       " 21135: 0.29346,\n",
       " 21140: 0.55694,\n",
       " 21142: 0.10701999999999999,\n",
       " 21144: 0.36366,\n",
       " 21146: 0.39513,\n",
       " 21153: 0.25441,\n",
       " 21154: 0.12383000000000001,\n",
       " 21155: 0.23501,\n",
       " 21157: 0.16893,\n",
       " 21159: 0.14194,\n",
       " 21163: 0.339,\n",
       " 21164: 0.15114,\n",
       " 21167: 0.10550999999999999,\n",
       " 21168: 0.14085,\n",
       " 21170: 0.04078,\n",
       " 21171: 0.30362,\n",
       " 21176: 0.44148000000000004,\n",
       " 21179: 0.29605000000000004,\n",
       " 21180: 0.02,\n",
       " 21182: 0.06688,\n",
       " 21184: 0.15858,\n",
       " 21190: 0.13297,\n",
       " 21191: 0.09023,\n",
       " 21192: 0.012379999999999999,\n",
       " 21194: 0.08985,\n",
       " 21196: 0.19440000000000002,\n",
       " 21200: 0.12786,\n",
       " 21201: 0.18025,\n",
       " 21202: 0.04587,\n",
       " 21207: 0.1208,\n",
       " 21209: 0.07853,\n",
       " 21212: 0.10563,\n",
       " 21214: 0.24428,\n",
       " 21218: 0.03348,\n",
       " 21222: 0.02184,\n",
       " 21224: 0.07537,\n",
       " 21226: 0.04866,\n",
       " 21227: 0.42182000000000003,\n",
       " 21233: 0.03557,\n",
       " 21244: 0.01552,\n",
       " 21245: 0.02403,\n",
       " 21246: 0.02117,\n",
       " 21248: 0.01129,\n",
       " 21252: 0.08560000000000001,\n",
       " 21256: 0.012709999999999999,\n",
       " 21259: 0.01412,\n",
       " 21262: 0.01834,\n",
       " 21263: 0.0127,\n",
       " 21265: 0.050069999999999996,\n",
       " 21266: 0.05121,\n",
       " 21267: 0.015690000000000003,\n",
       " 21276: 0.008919999999999999}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_tn_dict = df[df['periodo'] == 201912].groupby('product_id')['tn'].sum().to_dict()\n",
    "display(total_tn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 32s - 2s/step - loss: 303.1837 - val_loss: 216.4988 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "16/16 - 10s - 656ms/step - loss: 171.1437 - val_loss: 128.2903 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "16/16 - 12s - 721ms/step - loss: 109.6694 - val_loss: 90.3568 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "16/16 - 10s - 638ms/step - loss: 80.6424 - val_loss: 70.2535 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "16/16 - 9s - 580ms/step - loss: 66.2425 - val_loss: 57.6019 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "16/16 - 9s - 563ms/step - loss: 56.0405 - val_loss: 50.1689 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "16/16 - 9s - 561ms/step - loss: 50.5731 - val_loss: 45.0119 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "16/16 - 9s - 555ms/step - loss: 45.2202 - val_loss: 40.3840 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "16/16 - 9s - 571ms/step - loss: 42.1187 - val_loss: 37.1901 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "16/16 - 9s - 555ms/step - loss: 39.0244 - val_loss: 34.8589 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "16/16 - 9s - 576ms/step - loss: 35.9318 - val_loss: 33.5580 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "16/16 - 9s - 571ms/step - loss: 33.5138 - val_loss: 32.6151 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "16/16 - 9s - 567ms/step - loss: 31.6581 - val_loss: 29.8951 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "16/16 - 9s - 573ms/step - loss: 30.2272 - val_loss: 29.0336 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "16/16 - 10s - 638ms/step - loss: 28.8729 - val_loss: 28.9333 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "16/16 - 10s - 655ms/step - loss: 27.9020 - val_loss: 27.8587 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "16/16 - 10s - 609ms/step - loss: 26.9790 - val_loss: 27.4225 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "16/16 - 9s - 571ms/step - loss: 26.3919 - val_loss: 23.6409 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "16/16 - 9s - 563ms/step - loss: 25.3260 - val_loss: 26.6617 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "16/16 - 9s - 572ms/step - loss: 24.3845 - val_loss: 23.7190 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "16/16 - 10s - 622ms/step - loss: 23.7895 - val_loss: 22.8616 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "16/16 - 11s - 685ms/step - loss: 23.2105 - val_loss: 22.4844 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "16/16 - 12s - 731ms/step - loss: 22.6389 - val_loss: 22.9169 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "16/16 - 14s - 850ms/step - loss: 22.2777 - val_loss: 21.1138 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "16/16 - 10s - 631ms/step - loss: 21.8598 - val_loss: 20.4843 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "16/16 - 11s - 661ms/step - loss: 21.4200 - val_loss: 22.2013 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "16/16 - 10s - 626ms/step - loss: 21.1521 - val_loss: 23.2866 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "16/16 - 11s - 708ms/step - loss: 20.9011 - val_loss: 19.2066 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "16/16 - 11s - 685ms/step - loss: 20.5095 - val_loss: 20.1213 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "16/16 - 10s - 594ms/step - loss: 20.2553 - val_loss: 20.4375 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "16/16 - 11s - 693ms/step - loss: 19.8707 - val_loss: 18.2307 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "16/16 - 9s - 570ms/step - loss: 20.0126 - val_loss: 18.7808 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "16/16 - 9s - 576ms/step - loss: 19.5902 - val_loss: 19.4647 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "16/16 - 9s - 575ms/step - loss: 19.1266 - val_loss: 17.9212 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "16/16 - 9s - 583ms/step - loss: 19.5513 - val_loss: 22.7315 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "16/16 - 10s - 596ms/step - loss: 19.8784 - val_loss: 21.9692 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "16/16 - 10s - 609ms/step - loss: 18.8246 - val_loss: 17.3230 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "16/16 - 9s - 576ms/step - loss: 18.7115 - val_loss: 18.1872 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "16/16 - 9s - 562ms/step - loss: 18.5811 - val_loss: 20.6809 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "16/16 - 10s - 604ms/step - loss: 18.5280 - val_loss: 18.6185 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "16/16 - 9s - 570ms/step - loss: 18.2771 - val_loss: 17.0653 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "16/16 - 9s - 584ms/step - loss: 18.0525 - val_loss: 17.5021 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "16/16 - 9s - 579ms/step - loss: 18.1014 - val_loss: 20.0035 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "16/16 - 9s - 583ms/step - loss: 17.9746 - val_loss: 16.7390 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "16/16 - 9s - 578ms/step - loss: 17.8148 - val_loss: 16.7373 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "16/16 - 9s - 579ms/step - loss: 17.7045 - val_loss: 19.6955 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "16/16 - 10s - 614ms/step - loss: 17.7977 - val_loss: 16.4404 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "16/16 - 10s - 639ms/step - loss: 17.3055 - val_loss: 17.4377 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "16/16 - 10s - 651ms/step - loss: 17.2452 - val_loss: 15.8404 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "16/16 - 10s - 595ms/step - loss: 17.3680 - val_loss: 17.9047 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "16/16 - 9s - 562ms/step - loss: 17.2135 - val_loss: 17.7980 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "16/16 - 9s - 567ms/step - loss: 17.0865 - val_loss: 16.1003 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "16/16 - 9s - 572ms/step - loss: 16.9455 - val_loss: 16.9212 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "16/16 - 9s - 594ms/step - loss: 16.9370 - val_loss: 17.3418 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "16/16 - 9s - 587ms/step - loss: 16.9260 - val_loss: 16.3340 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "16/16 - 9s - 592ms/step - loss: 16.8729 - val_loss: 15.9860 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "16/16 - 9s - 575ms/step - loss: 16.6446 - val_loss: 16.9552 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "16/16 - 9s - 583ms/step - loss: 16.6747 - val_loss: 15.7605 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "16/16 - 9s - 582ms/step - loss: 16.6311 - val_loss: 16.2643 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "16/16 - 10s - 602ms/step - loss: 16.5418 - val_loss: 16.7884 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "16/16 - 9s - 577ms/step - loss: 16.6146 - val_loss: 15.1406 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "16/16 - 9s - 562ms/step - loss: 16.5675 - val_loss: 16.2246 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "16/16 - 11s - 679ms/step - loss: 16.3033 - val_loss: 16.3844 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "16/16 - 9s - 570ms/step - loss: 16.4469 - val_loss: 15.3046 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "16/16 - 10s - 621ms/step - loss: 16.5125 - val_loss: 17.5054 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "16/16 - 9s - 577ms/step - loss: 16.1922 - val_loss: 15.6938 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "16/16 - 10s - 624ms/step - loss: 16.6373 - val_loss: 15.2428 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "16/16 - 10s - 604ms/step - loss: 16.4074 - val_loss: 16.6672 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "16/16 - 10s - 631ms/step - loss: 16.0954 - val_loss: 15.6289 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "16/16 - 9s - 583ms/step - loss: 16.0904 - val_loss: 16.6550 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "16/16 - 9s - 592ms/step - loss: 16.1372 - val_loss: 15.7161 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "16/16 - 10s - 631ms/step - loss: 15.9887 - val_loss: 15.5627 - learning_rate: 2.0000e-04\n",
      "Epoch 73/100\n",
      "16/16 - 11s - 666ms/step - loss: 16.0739 - val_loss: 15.5315 - learning_rate: 2.0000e-04\n",
      "Epoch 74/100\n",
      "16/16 - 10s - 626ms/step - loss: 16.0016 - val_loss: 16.0147 - learning_rate: 2.0000e-04\n",
      "Epoch 75/100\n",
      "16/16 - 10s - 630ms/step - loss: 16.0018 - val_loss: 15.8883 - learning_rate: 2.0000e-04\n",
      "Epoch 76/100\n",
      "16/16 - 11s - 667ms/step - loss: 16.0222 - val_loss: 16.5320 - learning_rate: 2.0000e-04\n",
      "Epoch 77/100\n",
      "16/16 - 11s - 671ms/step - loss: 15.9680 - val_loss: 15.9586 - learning_rate: 2.0000e-04\n",
      "Epoch 78/100\n",
      "16/16 - 10s - 612ms/step - loss: 15.9319 - val_loss: 15.6924 - learning_rate: 2.0000e-04\n",
      "Epoch 79/100\n",
      "16/16 - 10s - 631ms/step - loss: 15.9491 - val_loss: 15.9119 - learning_rate: 2.0000e-04\n",
      "Epoch 80/100\n",
      "16/16 - 10s - 610ms/step - loss: 16.0143 - val_loss: 15.3687 - learning_rate: 2.0000e-04\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "16/16 - 10s - 627ms/step - loss: 16.0468 - val_loss: 16.0465 - learning_rate: 2.0000e-04\n",
      "Epoch 82/100\n",
      "16/16 - 12s - 765ms/step - loss: 15.9102 - val_loss: 15.9625 - learning_rate: 4.0000e-05\n",
      "Epoch 83/100\n",
      "16/16 - 11s - 716ms/step - loss: 15.9234 - val_loss: 16.0045 - learning_rate: 4.0000e-05\n",
      "Epoch 84/100\n",
      "16/16 - 11s - 685ms/step - loss: 15.9058 - val_loss: 15.8377 - learning_rate: 4.0000e-05\n",
      "Epoch 85/100\n",
      "16/16 - 10s - 595ms/step - loss: 15.9130 - val_loss: 15.8629 - learning_rate: 4.0000e-05\n",
      "Epoch 86/100\n",
      "16/16 - 9s - 593ms/step - loss: 15.8982 - val_loss: 15.8693 - learning_rate: 4.0000e-05\n",
      "Epoch 86: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 - 35s - 4s/step - loss: 335.6215 - val_loss: 257.9214 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "8/8 - 5s - 600ms/step - loss: 224.4283 - val_loss: 170.2218 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "8/8 - 5s - 578ms/step - loss: 149.7619 - val_loss: 113.7130 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "8/8 - 5s - 640ms/step - loss: 102.6602 - val_loss: 79.2933 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "8/8 - 5s - 600ms/step - loss: 74.4425 - val_loss: 59.2045 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "8/8 - 5s - 633ms/step - loss: 58.1359 - val_loss: 47.7388 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "8/8 - 7s - 821ms/step - loss: 48.8320 - val_loss: 41.1530 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "8/8 - 4s - 537ms/step - loss: 43.4277 - val_loss: 37.2227 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "8/8 - 4s - 535ms/step - loss: 40.1355 - val_loss: 34.7313 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "8/8 - 4s - 542ms/step - loss: 37.9992 - val_loss: 33.0496 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "8/8 - 4s - 550ms/step - loss: 36.5286 - val_loss: 31.8564 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "8/8 - 4s - 548ms/step - loss: 35.4706 - val_loss: 30.9802 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "8/8 - 4s - 533ms/step - loss: 34.6863 - val_loss: 30.3212 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "8/8 - 4s - 559ms/step - loss: 34.0919 - val_loss: 29.8158 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "8/8 - 4s - 547ms/step - loss: 33.6332 - val_loss: 29.4219 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "8/8 - 4s - 535ms/step - loss: 33.2738 - val_loss: 29.1108 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "8/8 - 4s - 542ms/step - loss: 32.9888 - val_loss: 28.8627 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "8/8 - 4s - 534ms/step - loss: 32.7609 - val_loss: 28.6635 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "8/8 - 4s - 554ms/step - loss: 32.5777 - val_loss: 28.5030 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "8/8 - 4s - 543ms/step - loss: 32.4299 - val_loss: 28.3733 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "8/8 - 4s - 560ms/step - loss: 32.3105 - val_loss: 28.2686 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "8/8 - 5s - 607ms/step - loss: 32.2140 - val_loss: 28.1840 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "8/8 - 4s - 527ms/step - loss: 32.1361 - val_loss: 28.1158 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "8/8 - 4s - 544ms/step - loss: 32.0733 - val_loss: 28.0609 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "8/8 - 4s - 522ms/step - loss: 32.0229 - val_loss: 28.0168 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 31s - 1s/step - loss: 221.1239 - val_loss: 100.9498 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "24/24 - 15s - 607ms/step - loss: 62.6396 - val_loss: 31.2514 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "24/24 - 14s - 579ms/step - loss: 26.3013 - val_loss: 16.8511 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "24/24 - 15s - 630ms/step - loss: 18.1067 - val_loss: 12.7040 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "24/24 - 14s - 581ms/step - loss: 15.3522 - val_loss: 11.0076 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "24/24 - 15s - 610ms/step - loss: 14.1355 - val_loss: 10.1885 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "24/24 - 15s - 609ms/step - loss: 13.5259 - val_loss: 9.7629 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "24/24 - 14s - 599ms/step - loss: 13.2060 - val_loss: 9.5384 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "24/24 - 14s - 580ms/step - loss: 13.0378 - val_loss: 9.4212 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "24/24 - 14s - 577ms/step - loss: 12.9508 - val_loss: 9.3614 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "24/24 - 14s - 589ms/step - loss: 12.9070 - val_loss: 9.3318 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "24/24 - 15s - 614ms/step - loss: 12.8855 - val_loss: 9.3175 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "24/24 - 15s - 632ms/step - loss: 12.8752 - val_loss: 9.3107 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "24/24 - 16s - 646ms/step - loss: 12.8704 - val_loss: 9.3077 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "24/24 - 17s - 697ms/step - loss: 12.8683 - val_loss: 9.3063 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "24/24 - 15s - 621ms/step - loss: 12.8673 - val_loss: 9.3057 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "24/24 - 14s - 579ms/step - loss: 12.8669 - val_loss: 9.3054 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "24/24 - 14s - 602ms/step - loss: 12.8667 - val_loss: 9.3053 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "24/24 - 14s - 575ms/step - loss: 12.8666 - val_loss: 9.3053 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "24/24 - 14s - 586ms/step - loss: 12.8666 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "24/24 - 15s - 622ms/step - loss: 12.8666 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "24/24 - 15s - 620ms/step - loss: 12.8666 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "24/24 - 15s - 611ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "24/24 - 16s - 684ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "24/24 - 14s - 577ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "24/24 - 13s - 559ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "24/24 - 14s - 600ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "24/24 - 14s - 589ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "24/24 - 14s - 577ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "24/24 - 14s - 588ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "24/24 - 15s - 609ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "24/24 - 14s - 588ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "24/24 - 14s - 599ms/step - loss: 12.8665 - val_loss: 9.3052 - learning_rate: 2.0000e-04\n",
      "Epoch 34/100\n",
      "24/24 - 14s - 578ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 35/100\n",
      "24/24 - 14s - 591ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 36/100\n",
      "24/24 - 16s - 673ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 37/100\n",
      "24/24 - 14s - 594ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "24/24 - 14s - 572ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "24/24 - 14s - 563ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 40/100\n",
      "24/24 - 14s - 587ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 41/100\n",
      "24/24 - 14s - 585ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "24/24 - 14s - 569ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 2.0000e-04\n",
      "Epoch 43/100\n",
      "24/24 - 14s - 593ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 44/100\n",
      "24/24 - 14s - 594ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 45/100\n",
      "24/24 - 15s - 623ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 46/100\n",
      "24/24 - 14s - 569ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 47/100\n",
      "24/24 - 13s - 558ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 48/100\n",
      "24/24 - 14s - 567ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 49/100\n",
      "24/24 - 14s - 570ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 50/100\n",
      "24/24 - 16s - 652ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 51/100\n",
      "24/24 - 15s - 633ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "24/24 - 20s - 843ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 4.0000e-05\n",
      "Epoch 53/100\n",
      "24/24 - 21s - 857ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 54/100\n",
      "24/24 - 17s - 727ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 55/100\n",
      "24/24 - 18s - 762ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 56/100\n",
      "24/24 - 23s - 949ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 57/100\n",
      "24/24 - 22s - 902ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 58/100\n",
      "24/24 - 22s - 908ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 59/100\n",
      "24/24 - 23s - 942ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 60/100\n",
      "24/24 - 21s - 876ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 61/100\n",
      "24/24 - 21s - 865ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 62/100\n",
      "24/24 - 21s - 884ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 63/100\n",
      "24/24 - 21s - 863ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "24/24 - 21s - 860ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "24/24 - 21s - 876ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "24/24 - 21s - 872ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "24/24 - 21s - 884ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "24/24 - 21s - 888ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "24/24 - 18s - 753ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "24/24 - 17s - 697ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "24/24 - 20s - 820ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "24/24 - 18s - 747ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "24/24 - 18s - 752ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "24/24 - 21s - 881ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "24/24 - 17s - 723ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "24/24 - 19s - 793ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "24/24 - 20s - 825ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "24/24 - 18s - 763ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "24/24 - 18s - 765ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "24/24 - 18s - 747ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "24/24 - 17s - 724ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "24/24 - 16s - 669ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "24/24 - 17s - 696ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "24/24 - 17s - 715ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "24/24 - 17s - 701ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "24/24 - 18s - 752ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "24/24 - 16s - 680ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "24/24 - 19s - 792ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "24/24 - 17s - 700ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "24/24 - 17s - 693ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "24/24 - 16s - 686ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "24/24 - 16s - 679ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "24/24 - 16s - 678ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "24/24 - 16s - 656ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "24/24 - 17s - 709ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "24/24 - 16s - 674ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "24/24 - 18s - 732ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "24/24 - 19s - 797ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "24/24 - 21s - 881ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "24/24 - 18s - 769ms/step - loss: 12.8665 - val_loss: 9.3051 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 4'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 - 61s - 1s/step - loss: 128.5162 - val_loss: 27.2612 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "54/54 - 48s - 894ms/step - loss: 19.0148 - val_loss: 13.5387 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "54/54 - 37s - 690ms/step - loss: 13.5387 - val_loss: 11.7117 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "54/54 - 34s - 624ms/step - loss: 12.5786 - val_loss: 11.2955 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "54/54 - 34s - 632ms/step - loss: 12.3589 - val_loss: 11.2041 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "54/54 - 34s - 625ms/step - loss: 12.3129 - val_loss: 11.1866 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "54/54 - 41s - 751ms/step - loss: 12.3045 - val_loss: 11.1837 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "54/54 - 32s - 587ms/step - loss: 12.3032 - val_loss: 11.1833 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "54/54 - 31s - 576ms/step - loss: 12.3030 - val_loss: 11.1833 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "54/54 - 32s - 585ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "54/54 - 32s - 598ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "54/54 - 31s - 568ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "54/54 - 30s - 559ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "54/54 - 30s - 564ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "54/54 - 30s - 554ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "54/54 - 30s - 556ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "54/54 - 31s - 579ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "54/54 - 32s - 589ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "54/54 - 30s - 559ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 2.0000e-04\n",
      "Epoch 20/100\n",
      "54/54 - 33s - 602ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 2.0000e-04\n",
      "Epoch 21/100\n",
      "54/54 - 31s - 571ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 2.0000e-04\n",
      "Epoch 22/100\n",
      "54/54 - 31s - 575ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 2.0000e-04\n",
      "Epoch 23/100\n",
      "54/54 - 31s - 569ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 2.0000e-04\n",
      "Epoch 24/100\n",
      "54/54 - 31s - 579ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 2.0000e-04\n",
      "Epoch 25/100\n",
      "54/54 - 31s - 566ms/step - loss: 12.3030 - val_loss: 11.1832 - learning_rate: 2.0000e-04\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 5'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 - 26s - 3s/step - loss: 325.3663 - val_loss: 246.9347 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "10/10 - 6s - 553ms/step - loss: 199.5052 - val_loss: 153.0265 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "10/10 - 6s - 596ms/step - loss: 125.1330 - val_loss: 100.6482 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "10/10 - 6s - 560ms/step - loss: 84.9311 - val_loss: 73.5562 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "10/10 - 5s - 542ms/step - loss: 64.5230 - val_loss: 60.0870 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "10/10 - 5s - 539ms/step - loss: 54.3690 - val_loss: 53.2893 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "10/10 - 5s - 544ms/step - loss: 49.1191 - val_loss: 49.5983 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "10/10 - 6s - 552ms/step - loss: 46.1569 - val_loss: 47.3860 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "10/10 - 5s - 544ms/step - loss: 44.3184 - val_loss: 45.9473 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "10/10 - 5s - 545ms/step - loss: 43.0948 - val_loss: 44.9612 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "10/10 - 5s - 541ms/step - loss: 42.2432 - val_loss: 44.2611 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "10/10 - 5s - 543ms/step - loss: 41.6316 - val_loss: 43.7503 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "10/10 - 6s - 555ms/step - loss: 41.1812 - val_loss: 43.3695 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "10/10 - 6s - 560ms/step - loss: 40.8431 - val_loss: 43.0814 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "10/10 - 5s - 549ms/step - loss: 40.5864 - val_loss: 42.8616 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "10/10 - 6s - 554ms/step - loss: 40.3901 - val_loss: 42.6932 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "10/10 - 6s - 563ms/step - loss: 40.2395 - val_loss: 42.5639 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "10/10 - 6s - 619ms/step - loss: 40.1241 - val_loss: 42.4649 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "10/10 - 6s - 573ms/step - loss: 40.0357 - val_loss: 42.3891 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "10/10 - 6s - 552ms/step - loss: 39.9681 - val_loss: 42.3314 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "10/10 - 6s - 560ms/step - loss: 39.9168 - val_loss: 42.2876 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "10/10 - 6s - 560ms/step - loss: 39.8779 - val_loss: 42.2546 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "10/10 - 7s - 703ms/step - loss: 39.8486 - val_loss: 42.2298 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "10/10 - 5s - 535ms/step - loss: 39.8266 - val_loss: 42.2112 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "10/10 - 5s - 549ms/step - loss: 39.8102 - val_loss: 42.1974 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 6'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/21 - 27s - 1s/step - loss: 227.9498 - val_loss: 112.1627 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "21/21 - 11s - 539ms/step - loss: 68.3558 - val_loss: 34.5672 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "21/21 - 11s - 540ms/step - loss: 24.7824 - val_loss: 15.9950 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "21/21 - 11s - 539ms/step - loss: 14.0365 - val_loss: 10.6672 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "21/21 - 12s - 561ms/step - loss: 10.4796 - val_loss: 8.4838 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "21/21 - 12s - 583ms/step - loss: 8.8900 - val_loss: 7.4050 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "21/21 - 12s - 591ms/step - loss: 8.0684 - val_loss: 6.8176 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "21/21 - 13s - 610ms/step - loss: 7.6117 - val_loss: 6.4845 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "21/21 - 12s - 568ms/step - loss: 7.3515 - val_loss: 6.2944 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "21/21 - 13s - 604ms/step - loss: 7.2035 - val_loss: 6.1871 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "21/21 - 13s - 604ms/step - loss: 7.1206 - val_loss: 6.1277 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "21/21 - 12s - 569ms/step - loss: 7.0751 - val_loss: 6.0955 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "21/21 - 12s - 587ms/step - loss: 7.0506 - val_loss: 6.0784 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "21/21 - 14s - 650ms/step - loss: 7.0378 - val_loss: 6.0696 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "21/21 - 12s - 583ms/step - loss: 7.0312 - val_loss: 6.0651 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "21/21 - 12s - 586ms/step - loss: 7.0279 - val_loss: 6.0629 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "21/21 - 12s - 550ms/step - loss: 7.0263 - val_loss: 6.0618 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "21/21 - 11s - 532ms/step - loss: 7.0255 - val_loss: 6.0613 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "21/21 - 11s - 534ms/step - loss: 7.0252 - val_loss: 6.0611 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "21/21 - 11s - 531ms/step - loss: 7.0250 - val_loss: 6.0610 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "21/21 - 11s - 524ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "21/21 - 11s - 531ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "21/21 - 13s - 614ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "21/21 - 11s - 538ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "21/21 - 12s - 550ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "21/21 - 12s - 560ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "21/21 - 11s - 543ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "21/21 - 12s - 561ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "21/21 - 12s - 567ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "21/21 - 12s - 561ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "21/21 - 12s - 554ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 32/100\n",
      "21/21 - 12s - 575ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 33/100\n",
      "21/21 - 12s - 552ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 34/100\n",
      "21/21 - 11s - 546ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 35/100\n",
      "21/21 - 12s - 555ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 36/100\n",
      "21/21 - 12s - 552ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 37/100\n",
      "21/21 - 12s - 580ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "21/21 - 11s - 548ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "21/21 - 12s - 583ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "21/21 - 12s - 554ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 2.0000e-04\n",
      "Epoch 41/100\n",
      "21/21 - 12s - 558ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 42/100\n",
      "21/21 - 11s - 533ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 43/100\n",
      "21/21 - 11s - 541ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 44/100\n",
      "21/21 - 12s - 565ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 45/100\n",
      "21/21 - 12s - 583ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 46/100\n",
      "21/21 - 12s - 556ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 47/100\n",
      "21/21 - 12s - 551ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 48/100\n",
      "21/21 - 11s - 542ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 49/100\n",
      "21/21 - 13s - 608ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "21/21 - 11s - 543ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 4.0000e-05\n",
      "Epoch 51/100\n",
      "21/21 - 11s - 541ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 1.0000e-05\n",
      "Epoch 52/100\n",
      "21/21 - 11s - 545ms/step - loss: 7.0249 - val_loss: 6.0609 - learning_rate: 1.0000e-05\n",
      "Epoch 52: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 7'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 - 23s - 2s/step - loss: 313.1774 - val_loss: 214.3237 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "13/13 - 7s - 546ms/step - loss: 176.1359 - val_loss: 128.3549 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "13/13 - 7s - 544ms/step - loss: 109.6551 - val_loss: 83.0678 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "13/13 - 7s - 572ms/step - loss: 75.9457 - val_loss: 60.4623 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "13/13 - 7s - 576ms/step - loss: 58.5208 - val_loss: 48.9186 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "13/13 - 7s - 535ms/step - loss: 48.7095 - val_loss: 40.9272 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "13/13 - 7s - 546ms/step - loss: 42.1967 - val_loss: 35.8240 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "13/13 - 7s - 542ms/step - loss: 38.0122 - val_loss: 33.2604 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "13/13 - 7s - 549ms/step - loss: 35.3885 - val_loss: 29.9704 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "13/13 - 7s - 558ms/step - loss: 32.3400 - val_loss: 27.1843 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "13/13 - 7s - 544ms/step - loss: 30.4094 - val_loss: 25.3626 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "13/13 - 7s - 545ms/step - loss: 28.5165 - val_loss: 23.8192 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "13/13 - 8s - 625ms/step - loss: 27.6341 - val_loss: 22.9391 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "13/13 - 8s - 602ms/step - loss: 26.6675 - val_loss: 21.3942 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "13/13 - 7s - 542ms/step - loss: 24.7955 - val_loss: 20.3725 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "13/13 - 7s - 537ms/step - loss: 24.2143 - val_loss: 19.4415 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "13/13 - 7s - 535ms/step - loss: 22.8886 - val_loss: 18.6943 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "13/13 - 7s - 562ms/step - loss: 22.0971 - val_loss: 17.8495 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "13/13 - 7s - 567ms/step - loss: 21.4762 - val_loss: 17.4991 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "13/13 - 7s - 539ms/step - loss: 21.1547 - val_loss: 17.3152 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "13/13 - 7s - 534ms/step - loss: 20.4352 - val_loss: 15.9924 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "13/13 - 7s - 535ms/step - loss: 19.7011 - val_loss: 15.6439 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "13/13 - 7s - 520ms/step - loss: 18.9703 - val_loss: 14.9527 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "13/13 - 7s - 523ms/step - loss: 18.6426 - val_loss: 14.5153 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "13/13 - 7s - 541ms/step - loss: 18.1421 - val_loss: 14.1588 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 8'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 - 39s - 1s/step - loss: 210.4629 - val_loss: 90.0400 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "29/29 - 17s - 576ms/step - loss: 57.1287 - val_loss: 35.7802 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "29/29 - 19s - 668ms/step - loss: 31.6834 - val_loss: 26.6908 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "29/29 - 18s - 605ms/step - loss: 26.5259 - val_loss: 24.0400 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "29/29 - 16s - 560ms/step - loss: 24.7846 - val_loss: 22.9801 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "29/29 - 16s - 538ms/step - loss: 24.0444 - val_loss: 22.5014 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "29/29 - 16s - 544ms/step - loss: 23.7056 - val_loss: 22.2810 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "29/29 - 16s - 538ms/step - loss: 23.5507 - val_loss: 22.1819 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "29/29 - 16s - 546ms/step - loss: 23.4821 - val_loss: 22.1389 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "29/29 - 17s - 587ms/step - loss: 23.4529 - val_loss: 22.1211 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "29/29 - 17s - 602ms/step - loss: 23.4410 - val_loss: 22.1140 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "29/29 - 17s - 587ms/step - loss: 23.4363 - val_loss: 22.1113 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "29/29 - 17s - 592ms/step - loss: 23.4345 - val_loss: 22.1103 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "29/29 - 17s - 576ms/step - loss: 23.4339 - val_loss: 22.1099 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "29/29 - 17s - 574ms/step - loss: 23.4337 - val_loss: 22.1098 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "29/29 - 19s - 644ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "29/29 - 27s - 921ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "29/29 - 30s - 1s/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "29/29 - 32s - 1s/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "29/29 - 23s - 792ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "29/29 - 22s - 763ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "29/29 - 26s - 882ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "29/29 - 24s - 822ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "29/29 - 25s - 875ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "29/29 - 25s - 872ms/step - loss: 23.4336 - val_loss: 22.1097 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 - 70s - 2s/step - loss: 155.9207 - val_loss: 43.5152 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "40/40 - 34s - 854ms/step - loss: 24.0574 - val_loss: 17.0939 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "40/40 - 39s - 967ms/step - loss: 13.2279 - val_loss: 13.4792 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "40/40 - 36s - 907ms/step - loss: 11.1684 - val_loss: 12.4389 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "40/40 - 39s - 970ms/step - loss: 10.5189 - val_loss: 12.0842 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "40/40 - 41s - 1s/step - loss: 10.2980 - val_loss: 11.9664 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "40/40 - 33s - 835ms/step - loss: 10.2266 - val_loss: 11.9300 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "40/40 - 39s - 976ms/step - loss: 10.2052 - val_loss: 11.9197 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "40/40 - 39s - 966ms/step - loss: 10.1993 - val_loss: 11.9170 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "40/40 - 40s - 1s/step - loss: 10.1978 - val_loss: 11.9163 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "40/40 - 38s - 945ms/step - loss: 10.1975 - val_loss: 11.9162 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "40/40 - 38s - 955ms/step - loss: 10.1974 - val_loss: 11.9162 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "40/40 - 34s - 841ms/step - loss: 10.1974 - val_loss: 11.9162 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "40/40 - 37s - 922ms/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "40/40 - 40s - 1s/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "40/40 - 37s - 919ms/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "40/40 - 38s - 961ms/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "40/40 - 41s - 1s/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "40/40 - 45s - 1s/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "40/40 - 46s - 1s/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "40/40 - 40s - 994ms/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "40/40 - 41s - 1s/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 2.0000e-04\n",
      "Epoch 23/100\n",
      "40/40 - 41s - 1s/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 2.0000e-04\n",
      "Epoch 24/100\n",
      "40/40 - 43s - 1s/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 2.0000e-04\n",
      "Epoch 25/100\n",
      "40/40 - 37s - 925ms/step - loss: 10.1974 - val_loss: 11.9161 - learning_rate: 2.0000e-04\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 - 25s - 1s/step - loss: 244.0152 - val_loss: 127.8784 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "20/20 - 12s - 600ms/step - loss: 86.1864 - val_loss: 48.0506 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "20/20 - 12s - 608ms/step - loss: 40.3832 - val_loss: 27.6542 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "20/20 - 12s - 576ms/step - loss: 28.5017 - val_loss: 21.7121 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "20/20 - 15s - 742ms/step - loss: 24.5506 - val_loss: 19.2807 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "20/20 - 13s - 641ms/step - loss: 22.7801 - val_loss: 18.0722 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "20/20 - 13s - 662ms/step - loss: 21.8581 - val_loss: 17.4072 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "20/20 - 14s - 678ms/step - loss: 21.3377 - val_loss: 17.0220 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "20/20 - 13s - 651ms/step - loss: 21.0338 - val_loss: 16.7957 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "20/20 - 13s - 639ms/step - loss: 20.8553 - val_loss: 16.6633 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "20/20 - 13s - 651ms/step - loss: 20.7516 - val_loss: 16.5870 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "20/20 - 13s - 658ms/step - loss: 20.6922 - val_loss: 16.5439 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "20/20 - 13s - 645ms/step - loss: 20.6590 - val_loss: 16.5199 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "20/20 - 12s - 622ms/step - loss: 20.6407 - val_loss: 16.5070 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "20/20 - 13s - 629ms/step - loss: 20.6308 - val_loss: 16.5001 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "20/20 - 13s - 647ms/step - loss: 20.6257 - val_loss: 16.4965 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "20/20 - 13s - 645ms/step - loss: 20.6230 - val_loss: 16.4947 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "20/20 - 13s - 653ms/step - loss: 20.6216 - val_loss: 16.4937 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "20/20 - 13s - 646ms/step - loss: 20.6210 - val_loss: 16.4933 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "20/20 - 13s - 637ms/step - loss: 20.6206 - val_loss: 16.4931 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "20/20 - 14s - 700ms/step - loss: 20.6205 - val_loss: 16.4930 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "20/20 - 14s - 683ms/step - loss: 20.6204 - val_loss: 16.4929 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "20/20 - 13s - 662ms/step - loss: 20.6204 - val_loss: 16.4929 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "20/20 - 13s - 674ms/step - loss: 20.6204 - val_loss: 16.4929 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "20/20 - 13s - 626ms/step - loss: 20.6204 - val_loss: 16.4929 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 11'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44/44 - 42s - 952ms/step - loss: 152.1197 - val_loss: 38.9539 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "44/44 - 28s - 645ms/step - loss: 25.0078 - val_loss: 16.3099 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "44/44 - 26s - 594ms/step - loss: 15.8118 - val_loss: 13.2402 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "44/44 - 28s - 639ms/step - loss: 14.0955 - val_loss: 12.4021 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "44/44 - 28s - 637ms/step - loss: 13.5927 - val_loss: 12.1450 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "44/44 - 30s - 674ms/step - loss: 13.4410 - val_loss: 12.0707 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "44/44 - 29s - 649ms/step - loss: 13.3986 - val_loss: 12.0511 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "44/44 - 26s - 601ms/step - loss: 13.3879 - val_loss: 12.0464 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "44/44 - 28s - 629ms/step - loss: 13.3854 - val_loss: 12.0454 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "44/44 - 28s - 644ms/step - loss: 13.3849 - val_loss: 12.0452 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "44/44 - 28s - 632ms/step - loss: 13.3847 - val_loss: 12.0451 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "44/44 - 29s - 654ms/step - loss: 13.3847 - val_loss: 12.0450 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "44/44 - 27s - 620ms/step - loss: 13.3846 - val_loss: 12.0450 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "44/44 - 27s - 609ms/step - loss: 13.3846 - val_loss: 12.0449 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "44/44 - 27s - 605ms/step - loss: 13.3845 - val_loss: 12.0449 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "44/44 - 28s - 638ms/step - loss: 13.3845 - val_loss: 12.0448 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "44/44 - 28s - 633ms/step - loss: 13.3844 - val_loss: 12.0448 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "44/44 - 27s - 617ms/step - loss: 13.3844 - val_loss: 12.0447 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "44/44 - 26s - 600ms/step - loss: 13.3843 - val_loss: 12.0447 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "44/44 - 28s - 630ms/step - loss: 13.3843 - val_loss: 12.0447 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "44/44 - 27s - 620ms/step - loss: 13.3842 - val_loss: 12.0446 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "44/44 - 27s - 604ms/step - loss: 13.3842 - val_loss: 12.0446 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "44/44 - 27s - 606ms/step - loss: 13.3842 - val_loss: 12.0445 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "44/44 - 29s - 658ms/step - loss: 13.3841 - val_loss: 12.0445 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "44/44 - 27s - 623ms/step - loss: 13.3841 - val_loss: 12.0445 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 12'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 - 60s - 751ms/step - loss: 83.5236 - val_loss: 9.7765 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "80/80 - 48s - 602ms/step - loss: 6.0202 - val_loss: 5.1027 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "80/80 - 48s - 598ms/step - loss: 4.2289 - val_loss: 4.6310 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "80/80 - 52s - 648ms/step - loss: 4.0374 - val_loss: 4.5847 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "80/80 - 53s - 664ms/step - loss: 4.0203 - val_loss: 4.5814 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "80/80 - 50s - 630ms/step - loss: 4.0191 - val_loss: 4.5812 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "80/80 - 50s - 631ms/step - loss: 4.0191 - val_loss: 4.5812 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "80/80 - 50s - 627ms/step - loss: 4.0190 - val_loss: 4.5812 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "80/80 - 50s - 629ms/step - loss: 4.0190 - val_loss: 4.5812 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "80/80 - 51s - 643ms/step - loss: 4.0190 - val_loss: 4.5811 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "80/80 - 52s - 653ms/step - loss: 4.0190 - val_loss: 4.5811 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "80/80 - 50s - 622ms/step - loss: 4.0190 - val_loss: 4.5811 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "80/80 - 49s - 618ms/step - loss: 4.0189 - val_loss: 4.5811 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "80/80 - 49s - 619ms/step - loss: 4.0189 - val_loss: 4.5811 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "80/80 - 51s - 633ms/step - loss: 4.0189 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "80/80 - 48s - 597ms/step - loss: 4.0189 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "80/80 - 52s - 649ms/step - loss: 4.0189 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "80/80 - 49s - 611ms/step - loss: 4.0189 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "80/80 - 49s - 611ms/step - loss: 4.0189 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "80/80 - 48s - 598ms/step - loss: 4.0189 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "80/80 - 48s - 596ms/step - loss: 4.0188 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "80/80 - 49s - 615ms/step - loss: 4.0188 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "80/80 - 48s - 596ms/step - loss: 4.0188 - val_loss: 4.5810 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "80/80 - 50s - 627ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "80/80 - 48s - 596ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "80/80 - 52s - 649ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "80/80 - 49s - 607ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "80/80 - 49s - 610ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "80/80 - 48s - 606ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 30/100\n",
      "80/80 - 49s - 614ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 31/100\n",
      "80/80 - 47s - 587ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 32/100\n",
      "80/80 - 48s - 603ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 33/100\n",
      "80/80 - 50s - 620ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 34/100\n",
      "80/80 - 49s - 612ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 35/100\n",
      "80/80 - 48s - 605ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 36/100\n",
      "80/80 - 52s - 648ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 37/100\n",
      "80/80 - 49s - 614ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "80/80 - 48s - 603ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "80/80 - 50s - 629ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 40/100\n",
      "80/80 - 49s - 610ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 41/100\n",
      "80/80 - 49s - 607ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 42/100\n",
      "80/80 - 48s - 604ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 43/100\n",
      "80/80 - 51s - 642ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 44/100\n",
      "80/80 - 47s - 593ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 45/100\n",
      "80/80 - 49s - 617ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 46/100\n",
      "80/80 - 48s - 601ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 47/100\n",
      "80/80 - 48s - 596ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "80/80 - 48s - 600ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 4.0000e-05\n",
      "Epoch 49/100\n",
      "80/80 - 49s - 611ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 50/100\n",
      "80/80 - 48s - 596ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 51/100\n",
      "80/80 - 48s - 605ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 52/100\n",
      "80/80 - 47s - 588ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 53/100\n",
      "80/80 - 47s - 590ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 54/100\n",
      "80/80 - 47s - 587ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 55/100\n",
      "80/80 - 49s - 617ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 56/100\n",
      "80/80 - 50s - 626ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 57/100\n",
      "80/80 - 48s - 600ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 58/100\n",
      "80/80 - 47s - 594ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 59/100\n",
      "80/80 - 48s - 594ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 60/100\n",
      "80/80 - 49s - 614ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 61/100\n",
      "80/80 - 48s - 594ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 62/100\n",
      "80/80 - 50s - 631ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 63/100\n",
      "80/80 - 49s - 610ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "80/80 - 50s - 621ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "80/80 - 50s - 628ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "80/80 - 48s - 599ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "80/80 - 48s - 603ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "80/80 - 49s - 616ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "80/80 - 49s - 619ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "80/80 - 48s - 606ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "80/80 - 48s - 604ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "80/80 - 48s - 602ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "80/80 - 48s - 604ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "80/80 - 50s - 629ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "80/80 - 49s - 612ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "80/80 - 48s - 604ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "80/80 - 48s - 597ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "80/80 - 48s - 595ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "80/80 - 47s - 590ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "80/80 - 48s - 603ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "80/80 - 49s - 618ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "80/80 - 49s - 608ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "80/80 - 47s - 583ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "80/80 - 49s - 616ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "80/80 - 48s - 602ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "80/80 - 49s - 607ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "80/80 - 48s - 602ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "80/80 - 49s - 612ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "80/80 - 48s - 606ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "80/80 - 47s - 589ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "80/80 - 47s - 591ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "80/80 - 47s - 593ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "80/80 - 48s - 603ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "80/80 - 49s - 612ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "80/80 - 48s - 601ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "80/80 - 48s - 595ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "80/80 - 47s - 587ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "80/80 - 48s - 606ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "80/80 - 49s - 610ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "80/80 - 51s - 634ms/step - loss: 4.0188 - val_loss: 4.5809 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 13'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 88s - 2s/step - loss: 183.0320 - val_loss: 65.4367 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "36/36 - 21s - 587ms/step - loss: 43.2503 - val_loss: 30.6796 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "36/36 - 20s - 545ms/step - loss: 28.7070 - val_loss: 25.8307 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "36/36 - 33s - 909ms/step - loss: 25.9577 - val_loss: 24.4189 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "36/36 - 22s - 614ms/step - loss: 25.0564 - val_loss: 23.8973 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "36/36 - 20s - 542ms/step - loss: 24.7165 - val_loss: 23.6996 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "36/36 - 20s - 543ms/step - loss: 24.5898 - val_loss: 23.6283 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "36/36 - 19s - 523ms/step - loss: 24.5452 - val_loss: 23.6041 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "36/36 - 19s - 524ms/step - loss: 24.5305 - val_loss: 23.5965 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "36/36 - 21s - 586ms/step - loss: 24.5260 - val_loss: 23.5943 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "36/36 - 19s - 525ms/step - loss: 24.5247 - val_loss: 23.5936 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "36/36 - 19s - 539ms/step - loss: 24.5244 - val_loss: 23.5935 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "36/36 - 20s - 558ms/step - loss: 24.5243 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "36/36 - 21s - 592ms/step - loss: 24.5243 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "36/36 - 19s - 537ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "36/36 - 19s - 526ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "36/36 - 19s - 531ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "36/36 - 19s - 531ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "36/36 - 19s - 537ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "36/36 - 19s - 526ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "36/36 - 20s - 547ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "36/36 - 20s - 564ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "36/36 - 20s - 547ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 2.0000e-04\n",
      "Epoch 24/100\n",
      "36/36 - 20s - 558ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 2.0000e-04\n",
      "Epoch 25/100\n",
      "36/36 - 21s - 570ms/step - loss: 24.5242 - val_loss: 23.5934 - learning_rate: 2.0000e-04\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 14'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 - 26s - 2s/step - loss: 307.2566 - val_loss: 215.8842 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "15/15 - 8s - 552ms/step - loss: 179.9417 - val_loss: 136.7137 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "15/15 - 8s - 550ms/step - loss: 119.2010 - val_loss: 96.3588 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "15/15 - 8s - 559ms/step - loss: 88.7521 - val_loss: 75.1885 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "15/15 - 8s - 532ms/step - loss: 71.5511 - val_loss: 62.4762 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "15/15 - 8s - 546ms/step - loss: 60.7753 - val_loss: 53.9139 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "15/15 - 8s - 555ms/step - loss: 52.9963 - val_loss: 47.2578 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "15/15 - 8s - 534ms/step - loss: 47.2319 - val_loss: 42.1431 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "15/15 - 8s - 529ms/step - loss: 42.6937 - val_loss: 38.0900 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "15/15 - 8s - 519ms/step - loss: 38.8098 - val_loss: 34.5707 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "15/15 - 8s - 525ms/step - loss: 35.9230 - val_loss: 31.8203 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "15/15 - 8s - 529ms/step - loss: 32.7465 - val_loss: 29.2381 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "15/15 - 8s - 518ms/step - loss: 30.4743 - val_loss: 27.3873 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "15/15 - 8s - 524ms/step - loss: 28.6441 - val_loss: 25.4373 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "15/15 - 8s - 512ms/step - loss: 26.8388 - val_loss: 23.6703 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "15/15 - 8s - 532ms/step - loss: 25.3079 - val_loss: 22.2906 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "15/15 - 8s - 521ms/step - loss: 24.1584 - val_loss: 21.6470 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "15/15 - 8s - 525ms/step - loss: 22.9863 - val_loss: 19.9873 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "15/15 - 8s - 524ms/step - loss: 21.7892 - val_loss: 19.0233 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "15/15 - 8s - 509ms/step - loss: 21.1844 - val_loss: 18.7587 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "15/15 - 8s - 511ms/step - loss: 20.2527 - val_loss: 17.5427 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "15/15 - 8s - 523ms/step - loss: 20.0117 - val_loss: 17.1035 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "15/15 - 8s - 535ms/step - loss: 18.7229 - val_loss: 16.0553 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "15/15 - 8s - 540ms/step - loss: 18.1163 - val_loss: 15.5210 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "15/15 - 8s - 531ms/step - loss: 17.5615 - val_loss: 14.9641 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 15'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 - 22s - 2s/step - loss: 301.6559 - val_loss: 205.8013 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "14/14 - 7s - 525ms/step - loss: 158.2759 - val_loss: 108.8925 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "14/14 - 7s - 518ms/step - loss: 88.8903 - val_loss: 66.4273 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "14/14 - 7s - 513ms/step - loss: 59.5482 - val_loss: 49.0449 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "14/14 - 7s - 521ms/step - loss: 47.3880 - val_loss: 41.5011 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "14/14 - 7s - 527ms/step - loss: 41.8313 - val_loss: 37.7425 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "14/14 - 7s - 528ms/step - loss: 38.9171 - val_loss: 35.6347 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "14/14 - 7s - 529ms/step - loss: 37.2284 - val_loss: 34.3621 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "14/14 - 9s - 650ms/step - loss: 36.1858 - val_loss: 33.5532 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "14/14 - 7s - 533ms/step - loss: 35.5119 - val_loss: 33.0196 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "14/14 - 7s - 530ms/step - loss: 35.0628 - val_loss: 32.6598 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "14/14 - 8s - 537ms/step - loss: 34.7583 - val_loss: 32.4145 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "14/14 - 7s - 530ms/step - loss: 34.5503 - val_loss: 32.2467 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "14/14 - 8s - 541ms/step - loss: 34.4080 - val_loss: 32.1320 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "14/14 - 7s - 524ms/step - loss: 34.3110 - val_loss: 32.0540 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "14/14 - 7s - 524ms/step - loss: 34.2452 - val_loss: 32.0013 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "14/14 - 7s - 531ms/step - loss: 34.2009 - val_loss: 31.9660 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "14/14 - 8s - 540ms/step - loss: 34.1712 - val_loss: 31.9426 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "14/14 - 8s - 545ms/step - loss: 34.1516 - val_loss: 31.9271 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "14/14 - 8s - 546ms/step - loss: 34.1387 - val_loss: 31.9170 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "14/14 - 8s - 550ms/step - loss: 34.1304 - val_loss: 31.9104 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "14/14 - 8s - 556ms/step - loss: 34.1249 - val_loss: 31.9062 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "14/14 - 8s - 558ms/step - loss: 34.1215 - val_loss: 31.9035 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "14/14 - 8s - 572ms/step - loss: 34.1193 - val_loss: 31.9018 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "14/14 - 8s - 541ms/step - loss: 34.1179 - val_loss: 31.9007 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 16'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 - 26s - 1s/step - loss: 261.4652 - val_loss: 142.5040 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 - 13s - 544ms/step - loss: 104.7237 - val_loss: 65.2382 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 - 12s - 540ms/step - loss: 60.7270 - val_loss: 44.8397 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 - 12s - 529ms/step - loss: 48.1596 - val_loss: 37.8309 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 - 12s - 538ms/step - loss: 43.3234 - val_loss: 34.7333 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "23/23 - 19s - 839ms/step - loss: 41.0626 - val_loss: 33.1867 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "23/23 - 20s - 865ms/step - loss: 39.8981 - val_loss: 32.3623 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "23/23 - 18s - 779ms/step - loss: 39.2685 - val_loss: 31.9104 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "23/23 - 18s - 781ms/step - loss: 38.9221 - val_loss: 31.6611 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "23/23 - 20s - 879ms/step - loss: 38.7311 - val_loss: 31.5240 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "23/23 - 23s - 1s/step - loss: 38.6263 - val_loss: 31.4491 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "23/23 - 22s - 941ms/step - loss: 38.5693 - val_loss: 31.4085 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "23/23 - 22s - 952ms/step - loss: 38.5385 - val_loss: 31.3866 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "23/23 - 21s - 930ms/step - loss: 38.5219 - val_loss: 31.3748 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "23/23 - 22s - 959ms/step - loss: 38.5129 - val_loss: 31.3685 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "23/23 - 24s - 1s/step - loss: 38.5081 - val_loss: 31.3651 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "23/23 - 21s - 898ms/step - loss: 38.5055 - val_loss: 31.3632 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "23/23 - 17s - 758ms/step - loss: 38.5041 - val_loss: 31.3622 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "23/23 - 17s - 727ms/step - loss: 38.5033 - val_loss: 31.3616 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "23/23 - 16s - 700ms/step - loss: 38.5028 - val_loss: 31.3613 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "23/23 - 16s - 706ms/step - loss: 38.5026 - val_loss: 31.3610 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "23/23 - 16s - 699ms/step - loss: 38.5024 - val_loss: 31.3609 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "23/23 - 16s - 713ms/step - loss: 38.5023 - val_loss: 31.3608 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "23/23 - 17s - 730ms/step - loss: 38.5022 - val_loss: 31.3608 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "23/23 - 17s - 732ms/step - loss: 38.5021 - val_loss: 31.3607 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 17'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 - 39s - 2s/step - loss: 255.4469 - val_loss: 128.4581 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 - 17s - 751ms/step - loss: 97.8918 - val_loss: 57.7012 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 - 17s - 750ms/step - loss: 60.7575 - val_loss: 42.8075 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 - 17s - 737ms/step - loss: 52.3313 - val_loss: 38.5489 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 - 17s - 746ms/step - loss: 49.4974 - val_loss: 36.7863 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "23/23 - 17s - 735ms/step - loss: 48.2279 - val_loss: 35.9227 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "23/23 - 17s - 755ms/step - loss: 47.5808 - val_loss: 35.4643 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "23/23 - 17s - 747ms/step - loss: 47.2331 - val_loss: 35.2159 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "23/23 - 17s - 720ms/step - loss: 47.0449 - val_loss: 35.0823 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "23/23 - 17s - 750ms/step - loss: 46.9446 - val_loss: 35.0120 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "23/23 - 17s - 731ms/step - loss: 46.8924 - val_loss: 34.9759 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "23/23 - 17s - 748ms/step - loss: 46.8659 - val_loss: 34.9579 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "23/23 - 16s - 714ms/step - loss: 46.8528 - val_loss: 34.9492 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "23/23 - 17s - 743ms/step - loss: 46.8466 - val_loss: 34.9451 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "23/23 - 18s - 762ms/step - loss: 46.8436 - val_loss: 34.9432 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "23/23 - 14s - 624ms/step - loss: 46.8423 - val_loss: 34.9423 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "23/23 - 15s - 646ms/step - loss: 46.8417 - val_loss: 34.9420 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "23/23 - 17s - 741ms/step - loss: 46.8415 - val_loss: 34.9418 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "23/23 - 21s - 916ms/step - loss: 46.8413 - val_loss: 34.9417 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "23/23 - 19s - 807ms/step - loss: 46.8413 - val_loss: 34.9417 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "23/23 - 18s - 794ms/step - loss: 46.8413 - val_loss: 34.9417 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "23/23 - 18s - 796ms/step - loss: 46.8413 - val_loss: 34.9417 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "23/23 - 18s - 778ms/step - loss: 46.8413 - val_loss: 34.9417 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "23/23 - 18s - 770ms/step - loss: 46.8413 - val_loss: 34.9417 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "23/23 - 19s - 805ms/step - loss: 46.8413 - val_loss: 34.9417 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 18'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 - 43s - 1s/step - loss: 194.3372 - val_loss: 72.8073 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "32/32 - 27s - 844ms/step - loss: 48.4160 - val_loss: 30.7448 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "32/32 - 25s - 796ms/step - loss: 29.9200 - val_loss: 24.5208 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "32/32 - 28s - 889ms/step - loss: 26.3357 - val_loss: 22.6562 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "32/32 - 31s - 960ms/step - loss: 25.1095 - val_loss: 21.9191 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "32/32 - 38s - 1s/step - loss: 24.6046 - val_loss: 21.6060 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "32/32 - 40s - 1s/step - loss: 24.3909 - val_loss: 21.4754 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "32/32 - 28s - 860ms/step - loss: 24.3033 - val_loss: 21.4234 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "32/32 - 26s - 819ms/step - loss: 24.2692 - val_loss: 21.4038 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "32/32 - 27s - 838ms/step - loss: 24.2565 - val_loss: 21.3967 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "32/32 - 25s - 796ms/step - loss: 24.2521 - val_loss: 21.3943 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "32/32 - 26s - 818ms/step - loss: 24.2506 - val_loss: 21.3935 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "32/32 - 27s - 829ms/step - loss: 24.2501 - val_loss: 21.3933 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "32/32 - 24s - 763ms/step - loss: 24.2500 - val_loss: 21.3932 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "32/32 - 27s - 852ms/step - loss: 24.2499 - val_loss: 21.3932 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "32/32 - 25s - 776ms/step - loss: 24.2499 - val_loss: 21.3932 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "32/32 - 28s - 884ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "32/32 - 32s - 1s/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "32/32 - 25s - 777ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "32/32 - 24s - 763ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "32/32 - 23s - 727ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "32/32 - 24s - 759ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "32/32 - 23s - 728ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "32/32 - 24s - 742ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "32/32 - 24s - 736ms/step - loss: 24.2499 - val_loss: 21.3931 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 19'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 - 35s - 2s/step - loss: 285.3475 - val_loss: 167.2430 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 - 16s - 711ms/step - loss: 126.9975 - val_loss: 90.4086 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 - 16s - 714ms/step - loss: 83.2739 - val_loss: 70.6738 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 - 18s - 764ms/step - loss: 71.1817 - val_loss: 64.0877 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 - 18s - 802ms/step - loss: 66.6429 - val_loss: 61.2159 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "23/23 - 21s - 921ms/step - loss: 64.5407 - val_loss: 59.7858 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "23/23 - 16s - 701ms/step - loss: 63.4583 - val_loss: 59.0214 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "23/23 - 19s - 811ms/step - loss: 62.8713 - val_loss: 58.6009 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "23/23 - 18s - 799ms/step - loss: 62.5471 - val_loss: 58.3681 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "23/23 - 19s - 839ms/step - loss: 62.3679 - val_loss: 58.2398 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "23/23 - 19s - 835ms/step - loss: 62.2694 - val_loss: 58.1698 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "23/23 - 20s - 853ms/step - loss: 62.2159 - val_loss: 58.1318 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "23/23 - 21s - 897ms/step - loss: 62.1870 - val_loss: 58.1114 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "23/23 - 20s - 859ms/step - loss: 62.1714 - val_loss: 58.1005 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "23/23 - 19s - 834ms/step - loss: 62.1632 - val_loss: 58.0947 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "23/23 - 19s - 828ms/step - loss: 62.1588 - val_loss: 58.0916 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "23/23 - 19s - 844ms/step - loss: 62.1564 - val_loss: 58.0899 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "23/23 - 18s - 777ms/step - loss: 62.1551 - val_loss: 58.0890 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "23/23 - 17s - 747ms/step - loss: 62.1544 - val_loss: 58.0885 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "23/23 - 18s - 766ms/step - loss: 62.1540 - val_loss: 58.0882 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "23/23 - 20s - 850ms/step - loss: 62.1538 - val_loss: 58.0880 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "23/23 - 22s - 962ms/step - loss: 62.1536 - val_loss: 58.0879 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "23/23 - 17s - 742ms/step - loss: 62.1536 - val_loss: 58.0878 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "23/23 - 17s - 750ms/step - loss: 62.1535 - val_loss: 58.0878 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "23/23 - 16s - 685ms/step - loss: 62.1535 - val_loss: 58.0878 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 - 28s - 2s/step - loss: 295.4574 - val_loss: 189.2362 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "18/18 - 12s - 642ms/step - loss: 150.3723 - val_loss: 109.0368 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "18/18 - 11s - 637ms/step - loss: 93.8621 - val_loss: 81.2848 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "18/18 - 12s - 645ms/step - loss: 74.7032 - val_loss: 62.9995 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "18/18 - 12s - 655ms/step - loss: 60.5280 - val_loss: 53.0405 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "18/18 - 11s - 619ms/step - loss: 60.2333 - val_loss: 50.3627 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "18/18 - 12s - 657ms/step - loss: 49.8173 - val_loss: 45.4349 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "18/18 - 12s - 676ms/step - loss: 46.4894 - val_loss: 44.1083 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "18/18 - 11s - 628ms/step - loss: 44.1023 - val_loss: 45.1257 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "18/18 - 11s - 627ms/step - loss: 42.4965 - val_loss: 38.1222 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "18/18 - 12s - 640ms/step - loss: 40.2637 - val_loss: 36.4658 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "18/18 - 11s - 619ms/step - loss: 39.1561 - val_loss: 39.2677 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "18/18 - 11s - 627ms/step - loss: 38.8567 - val_loss: 39.6921 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "18/18 - 11s - 637ms/step - loss: 38.1403 - val_loss: 32.5077 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "18/18 - 12s - 645ms/step - loss: 35.8646 - val_loss: 32.9462 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "18/18 - 12s - 642ms/step - loss: 35.1830 - val_loss: 32.8083 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "18/18 - 12s - 671ms/step - loss: 34.2878 - val_loss: 34.0278 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "18/18 - 12s - 675ms/step - loss: 33.7620 - val_loss: 31.3927 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "18/18 - 12s - 655ms/step - loss: 34.2610 - val_loss: 29.3155 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "18/18 - 13s - 737ms/step - loss: 32.4302 - val_loss: 28.5178 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "18/18 - 12s - 674ms/step - loss: 32.0747 - val_loss: 29.0250 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "18/18 - 12s - 655ms/step - loss: 31.3765 - val_loss: 34.8115 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "18/18 - 11s - 638ms/step - loss: 38.9537 - val_loss: 26.8841 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "18/18 - 13s - 713ms/step - loss: 32.4191 - val_loss: 31.0865 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "18/18 - 12s - 684ms/step - loss: 30.1104 - val_loss: 26.8041 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 21'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 - 26s - 2s/step - loss: 304.5737 - val_loss: 207.6629 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "15/15 - 10s - 693ms/step - loss: 169.4916 - val_loss: 123.1993 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "15/15 - 12s - 768ms/step - loss: 107.9486 - val_loss: 82.5272 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "15/15 - 11s - 710ms/step - loss: 78.5869 - val_loss: 63.3324 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "15/15 - 11s - 701ms/step - loss: 62.9027 - val_loss: 52.4538 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "15/15 - 10s - 686ms/step - loss: 53.7609 - val_loss: 45.6871 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "15/15 - 12s - 803ms/step - loss: 47.5732 - val_loss: 39.4274 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "15/15 - 11s - 744ms/step - loss: 42.6769 - val_loss: 34.9685 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "15/15 - 11s - 757ms/step - loss: 39.6801 - val_loss: 32.1864 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "15/15 - 11s - 762ms/step - loss: 36.8229 - val_loss: 31.4239 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "15/15 - 12s - 774ms/step - loss: 34.7646 - val_loss: 29.4730 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "15/15 - 12s - 769ms/step - loss: 33.1883 - val_loss: 26.9059 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "15/15 - 11s - 739ms/step - loss: 31.6418 - val_loss: 24.9978 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "15/15 - 12s - 794ms/step - loss: 30.2282 - val_loss: 24.4787 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "15/15 - 11s - 740ms/step - loss: 30.2690 - val_loss: 23.5950 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "15/15 - 11s - 756ms/step - loss: 28.4193 - val_loss: 21.9922 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "15/15 - 13s - 859ms/step - loss: 27.2131 - val_loss: 20.8841 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "15/15 - 13s - 837ms/step - loss: 26.4088 - val_loss: 20.5034 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "15/15 - 12s - 823ms/step - loss: 25.8550 - val_loss: 19.8095 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "15/15 - 14s - 959ms/step - loss: 25.0210 - val_loss: 18.8978 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "15/15 - 11s - 757ms/step - loss: 23.9525 - val_loss: 18.0305 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "15/15 - 11s - 744ms/step - loss: 23.6313 - val_loss: 17.9317 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "15/15 - 12s - 767ms/step - loss: 23.4854 - val_loss: 17.6824 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "15/15 - 11s - 729ms/step - loss: 22.5592 - val_loss: 17.0000 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "15/15 - 12s - 794ms/step - loss: 22.2086 - val_loss: 16.1198 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 22'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 - 25s - 2s/step - loss: 311.2208 - val_loss: 216.8768 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "14/14 - 10s - 716ms/step - loss: 177.1512 - val_loss: 132.1025 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "14/14 - 10s - 707ms/step - loss: 113.4016 - val_loss: 88.9922 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "14/14 - 10s - 739ms/step - loss: 80.7689 - val_loss: 68.1948 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "14/14 - 10s - 735ms/step - loss: 63.7569 - val_loss: 54.2476 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "14/14 - 10s - 747ms/step - loss: 53.3285 - val_loss: 46.4201 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "14/14 - 10s - 721ms/step - loss: 46.7723 - val_loss: 40.6188 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "14/14 - 10s - 729ms/step - loss: 41.1931 - val_loss: 38.0453 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "14/14 - 10s - 715ms/step - loss: 38.3403 - val_loss: 34.2207 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "14/14 - 10s - 706ms/step - loss: 35.0336 - val_loss: 30.8395 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "14/14 - 11s - 756ms/step - loss: 32.5309 - val_loss: 28.5557 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "14/14 - 10s - 701ms/step - loss: 30.7490 - val_loss: 26.6410 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "14/14 - 11s - 776ms/step - loss: 29.0279 - val_loss: 25.0595 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "14/14 - 10s - 680ms/step - loss: 27.6230 - val_loss: 23.6757 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "14/14 - 11s - 776ms/step - loss: 26.1385 - val_loss: 23.2371 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "14/14 - 10s - 740ms/step - loss: 25.4737 - val_loss: 22.0530 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "14/14 - 11s - 758ms/step - loss: 24.0371 - val_loss: 20.6241 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "14/14 - 10s - 722ms/step - loss: 23.1628 - val_loss: 19.8340 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "14/14 - 11s - 782ms/step - loss: 22.2433 - val_loss: 19.3134 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "14/14 - 11s - 812ms/step - loss: 21.7669 - val_loss: 18.6105 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "14/14 - 10s - 717ms/step - loss: 21.0530 - val_loss: 18.3967 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "14/14 - 13s - 923ms/step - loss: 20.5351 - val_loss: 18.1580 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "14/14 - 10s - 733ms/step - loss: 20.3123 - val_loss: 17.0418 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "14/14 - 10s - 713ms/step - loss: 19.5447 - val_loss: 16.3792 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "14/14 - 10s - 710ms/step - loss: 19.0074 - val_loss: 16.0076 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 23'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 - 33s - 1s/step - loss: 221.5501 - val_loss: 99.4611 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "25/25 - 15s - 581ms/step - loss: 66.1013 - val_loss: 36.6845 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "25/25 - 14s - 574ms/step - loss: 34.7818 - val_loss: 24.9375 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "25/25 - 15s - 584ms/step - loss: 28.1300 - val_loss: 21.5310 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "25/25 - 16s - 624ms/step - loss: 25.8548 - val_loss: 20.1167 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "25/25 - 15s - 613ms/step - loss: 24.8400 - val_loss: 19.4328 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "25/25 - 14s - 570ms/step - loss: 24.3342 - val_loss: 19.0830 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "25/25 - 15s - 580ms/step - loss: 24.0746 - val_loss: 18.9036 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "25/25 - 14s - 578ms/step - loss: 23.9425 - val_loss: 18.8136 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "25/25 - 15s - 585ms/step - loss: 23.8769 - val_loss: 18.7697 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "25/25 - 15s - 584ms/step - loss: 23.8454 - val_loss: 18.7490 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "25/25 - 14s - 577ms/step - loss: 23.8308 - val_loss: 18.7395 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "25/25 - 14s - 575ms/step - loss: 23.8242 - val_loss: 18.7354 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "25/25 - 15s - 598ms/step - loss: 23.8213 - val_loss: 18.7336 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "25/25 - 15s - 600ms/step - loss: 23.8201 - val_loss: 18.7328 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "25/25 - 15s - 588ms/step - loss: 23.8196 - val_loss: 18.7325 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "25/25 - 15s - 595ms/step - loss: 23.8194 - val_loss: 18.7324 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "25/25 - 17s - 684ms/step - loss: 23.8193 - val_loss: 18.7324 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "25/25 - 15s - 613ms/step - loss: 23.8193 - val_loss: 18.7324 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "25/25 - 15s - 604ms/step - loss: 23.8193 - val_loss: 18.7323 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "25/25 - 15s - 598ms/step - loss: 23.8192 - val_loss: 18.7323 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "25/25 - 14s - 565ms/step - loss: 23.8192 - val_loss: 18.7323 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "25/25 - 14s - 574ms/step - loss: 23.8192 - val_loss: 18.7323 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "25/25 - 14s - 572ms/step - loss: 23.8192 - val_loss: 18.7323 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "25/25 - 15s - 584ms/step - loss: 23.8192 - val_loss: 18.7323 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 24'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57/57 - 45s - 794ms/step - loss: 118.0552 - val_loss: 24.8684 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 - 31s - 547ms/step - loss: 15.6388 - val_loss: 14.0123 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 - 32s - 557ms/step - loss: 11.2095 - val_loss: 12.5194 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 - 33s - 583ms/step - loss: 10.4325 - val_loss: 12.1999 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 - 33s - 573ms/step - loss: 10.2698 - val_loss: 12.1380 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 - 41s - 716ms/step - loss: 10.2400 - val_loss: 12.1279 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 - 34s - 595ms/step - loss: 10.2354 - val_loss: 12.1264 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 - 33s - 581ms/step - loss: 10.2348 - val_loss: 12.1263 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 - 35s - 610ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 - 33s - 578ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 - 33s - 586ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 - 33s - 583ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 - 33s - 583ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 - 33s - 578ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 - 35s - 613ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 - 31s - 552ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 - 33s - 573ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "57/57 - 32s - 568ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 - 32s - 557ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 2.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 - 32s - 558ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 2.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 - 32s - 559ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 2.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 - 33s - 576ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 2.0000e-04\n",
      "Epoch 23/100\n",
      "57/57 - 32s - 563ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 2.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 - 35s - 611ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 2.0000e-04\n",
      "Epoch 25/100\n",
      "57/57 - 34s - 589ms/step - loss: 10.2347 - val_loss: 12.1262 - learning_rate: 2.0000e-04\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 25'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 - 26s - 3s/step - loss: 323.6674 - val_loss: 240.2557 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "10/10 - 6s - 600ms/step - loss: 201.1366 - val_loss: 145.6626 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "10/10 - 6s - 594ms/step - loss: 125.5631 - val_loss: 91.1120 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "10/10 - 6s - 565ms/step - loss: 83.4266 - val_loss: 62.0195 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "10/10 - 6s - 580ms/step - loss: 61.3786 - val_loss: 47.1053 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "10/10 - 6s - 604ms/step - loss: 50.0766 - val_loss: 39.3756 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "10/10 - 6s - 613ms/step - loss: 44.0966 - val_loss: 35.1171 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "10/10 - 6s - 582ms/step - loss: 40.6902 - val_loss: 32.5640 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "10/10 - 6s - 570ms/step - loss: 38.5825 - val_loss: 30.9164 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "10/10 - 6s - 564ms/step - loss: 37.1912 - val_loss: 29.7975 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "10/10 - 6s - 568ms/step - loss: 36.2314 - val_loss: 29.0098 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "10/10 - 6s - 558ms/step - loss: 35.5474 - val_loss: 28.4394 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "10/10 - 6s - 565ms/step - loss: 35.0472 - val_loss: 28.0172 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "10/10 - 6s - 560ms/step - loss: 34.6743 - val_loss: 27.6997 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "10/10 - 6s - 554ms/step - loss: 34.3927 - val_loss: 27.4586 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "10/10 - 6s - 561ms/step - loss: 34.1782 - val_loss: 27.2745 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "10/10 - 6s - 596ms/step - loss: 34.0143 - val_loss: 27.1337 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "10/10 - 6s - 583ms/step - loss: 33.8889 - val_loss: 27.0259 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "10/10 - 6s - 569ms/step - loss: 33.7929 - val_loss: 26.9435 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "10/10 - 6s - 557ms/step - loss: 33.7196 - val_loss: 26.8807 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "10/10 - 6s - 600ms/step - loss: 33.6639 - val_loss: 26.8330 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "10/10 - 6s - 560ms/step - loss: 33.6216 - val_loss: 26.7970 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "10/10 - 6s - 564ms/step - loss: 33.5897 - val_loss: 26.7698 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "10/10 - 6s - 568ms/step - loss: 33.5657 - val_loss: 26.7495 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "10/10 - 6s - 587ms/step - loss: 33.5478 - val_loss: 26.7343 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 26'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 - 20s - 3s/step - loss: 358.5937 - val_loss: 310.3441 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "6/6 - 3s - 573ms/step - loss: 270.6674 - val_loss: 234.5799 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "6/6 - 4s - 590ms/step - loss: 202.7038 - val_loss: 177.9910 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "6/6 - 3s - 576ms/step - loss: 152.7236 - val_loss: 137.3790 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "6/6 - 3s - 560ms/step - loss: 117.3053 - val_loss: 109.1751 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "6/6 - 3s - 557ms/step - loss: 92.9567 - val_loss: 90.0928 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "6/6 - 3s - 579ms/step - loss: 76.6023 - val_loss: 77.4118 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "6/6 - 3s - 569ms/step - loss: 65.7739 - val_loss: 69.0484 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "6/6 - 3s - 580ms/step - loss: 58.6272 - val_loss: 63.5062 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "6/6 - 3s - 556ms/step - loss: 53.8652 - val_loss: 59.7678 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "6/6 - 3s - 548ms/step - loss: 50.6217 - val_loss: 57.1734 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "6/6 - 3s - 578ms/step - loss: 48.3425 - val_loss: 55.3101 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "6/6 - 3s - 580ms/step - loss: 46.6846 - val_loss: 53.9261 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "6/6 - 3s - 577ms/step - loss: 45.4391 - val_loss: 52.8679 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "6/6 - 3s - 574ms/step - loss: 44.4784 - val_loss: 52.0406 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "6/6 - 4s - 728ms/step - loss: 43.7220 - val_loss: 51.3824 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "6/6 - 7s - 1s/step - loss: 43.1171 - val_loss: 50.8517 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "6/6 - 4s - 687ms/step - loss: 42.6271 - val_loss: 50.4189 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "6/6 - 4s - 620ms/step - loss: 42.2259 - val_loss: 50.0623 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "6/6 - 3s - 534ms/step - loss: 41.8943 - val_loss: 49.7660 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "6/6 - 3s - 547ms/step - loss: 41.6179 - val_loss: 49.5179 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "6/6 - 5s - 882ms/step - loss: 41.3860 - val_loss: 49.3090 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "6/6 - 4s - 660ms/step - loss: 41.1903 - val_loss: 49.1322 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "6/6 - 3s - 553ms/step - loss: 41.0246 - val_loss: 48.9823 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "6/6 - 3s - 579ms/step - loss: 40.8838 - val_loss: 48.8547 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 27'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 - 34s - 1s/step - loss: 237.0170 - val_loss: 121.9627 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "26/26 - 20s - 788ms/step - loss: 78.2466 - val_loss: 54.6779 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "26/26 - 18s - 679ms/step - loss: 42.5220 - val_loss: 39.9653 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "26/26 - 17s - 656ms/step - loss: 33.6744 - val_loss: 35.2417 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "26/26 - 16s - 617ms/step - loss: 30.4660 - val_loss: 33.2560 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "26/26 - 16s - 614ms/step - loss: 29.0354 - val_loss: 32.3069 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "26/26 - 14s - 538ms/step - loss: 28.3332 - val_loss: 31.8285 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "26/26 - 15s - 589ms/step - loss: 27.9767 - val_loss: 31.5846 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "26/26 - 14s - 553ms/step - loss: 27.7955 - val_loss: 31.4613 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "26/26 - 14s - 545ms/step - loss: 27.7043 - val_loss: 31.3998 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "26/26 - 14s - 540ms/step - loss: 27.6592 - val_loss: 31.3696 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "26/26 - 14s - 549ms/step - loss: 27.6371 - val_loss: 31.3550 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "26/26 - 14s - 555ms/step - loss: 27.6265 - val_loss: 31.3479 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "26/26 - 15s - 575ms/step - loss: 27.6213 - val_loss: 31.3445 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "26/26 - 16s - 604ms/step - loss: 27.6188 - val_loss: 31.3428 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "26/26 - 15s - 591ms/step - loss: 27.6176 - val_loss: 31.3420 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "26/26 - 15s - 572ms/step - loss: 27.6170 - val_loss: 31.3416 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "26/26 - 17s - 669ms/step - loss: 27.6167 - val_loss: 31.3414 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "26/26 - 16s - 605ms/step - loss: 27.6165 - val_loss: 31.3412 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "26/26 - 15s - 585ms/step - loss: 27.6164 - val_loss: 31.3412 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "26/26 - 15s - 570ms/step - loss: 27.6163 - val_loss: 31.3411 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "26/26 - 15s - 570ms/step - loss: 27.6163 - val_loss: 31.3411 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "26/26 - 15s - 573ms/step - loss: 27.6162 - val_loss: 31.3411 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "26/26 - 15s - 578ms/step - loss: 27.6162 - val_loss: 31.3410 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "26/26 - 15s - 588ms/step - loss: 27.6162 - val_loss: 31.3410 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 28'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 - 26s - 1s/step - loss: 277.7418 - val_loss: 157.7038 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "18/18 - 10s - 550ms/step - loss: 122.6257 - val_loss: 67.3876 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "18/18 - 10s - 550ms/step - loss: 64.9517 - val_loss: 37.4937 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "18/18 - 10s - 552ms/step - loss: 46.0217 - val_loss: 27.2522 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "18/18 - 10s - 551ms/step - loss: 39.0214 - val_loss: 22.9058 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "18/18 - 10s - 547ms/step - loss: 35.8155 - val_loss: 20.7153 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "18/18 - 10s - 561ms/step - loss: 34.1287 - val_loss: 19.4995 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "18/18 - 10s - 572ms/step - loss: 33.1661 - val_loss: 18.7819 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "18/18 - 10s - 557ms/step - loss: 32.5889 - val_loss: 18.3444 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "18/18 - 10s - 561ms/step - loss: 32.2346 - val_loss: 18.0741 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "18/18 - 10s - 562ms/step - loss: 32.0156 - val_loss: 17.9071 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "18/18 - 10s - 573ms/step - loss: 31.8805 - val_loss: 17.8044 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "18/18 - 10s - 575ms/step - loss: 31.7978 - val_loss: 17.7420 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "18/18 - 10s - 560ms/step - loss: 31.7478 - val_loss: 17.7045 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "18/18 - 10s - 554ms/step - loss: 31.7178 - val_loss: 17.6822 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "18/18 - 10s - 583ms/step - loss: 31.7002 - val_loss: 17.6692 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "18/18 - 10s - 581ms/step - loss: 31.6899 - val_loss: 17.6616 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "18/18 - 13s - 703ms/step - loss: 31.6839 - val_loss: 17.6573 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "18/18 - 11s - 596ms/step - loss: 31.6806 - val_loss: 17.6549 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "18/18 - 11s - 592ms/step - loss: 31.6786 - val_loss: 17.6535 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "18/18 - 11s - 584ms/step - loss: 31.6776 - val_loss: 17.6527 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "18/18 - 10s - 580ms/step - loss: 31.6770 - val_loss: 17.6523 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "18/18 - 10s - 562ms/step - loss: 31.6766 - val_loss: 17.6520 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "18/18 - 10s - 565ms/step - loss: 31.6764 - val_loss: 17.6519 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "18/18 - 10s - 579ms/step - loss: 31.6763 - val_loss: 17.6518 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entrenando cluster numero: 29'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 - 25s - 1s/step - loss: 322.1709 - val_loss: 193.9314 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "20/20 - 11s - 541ms/step - loss: 152.2137 - val_loss: 112.2022 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "20/20 - 11s - 548ms/step - loss: 98.4854 - val_loss: 80.7170 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "20/20 - 11s - 543ms/step - loss: 74.1472 - val_loss: 62.5771 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "20/20 - 11s - 551ms/step - loss: 60.8116 - val_loss: 52.8904 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "20/20 - 11s - 560ms/step - loss: 51.6047 - val_loss: 44.9961 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "20/20 - 11s - 538ms/step - loss: 46.3467 - val_loss: 39.3439 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "20/20 - 11s - 561ms/step - loss: 41.0408 - val_loss: 37.6280 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "20/20 - 11s - 549ms/step - loss: 38.0087 - val_loss: 32.0769 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "20/20 - 11s - 547ms/step - loss: 33.6540 - val_loss: 30.5104 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "20/20 - 11s - 552ms/step - loss: 31.3771 - val_loss: 27.2143 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "20/20 - 11s - 560ms/step - loss: 29.2149 - val_loss: 26.4898 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "20/20 - 11s - 574ms/step - loss: 27.7309 - val_loss: 24.6340 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "20/20 - 12s - 597ms/step - loss: 26.2284 - val_loss: 24.1346 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "20/20 - 13s - 630ms/step - loss: 25.2104 - val_loss: 21.8015 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "20/20 - 12s - 592ms/step - loss: 23.7679 - val_loss: 21.2119 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "20/20 - 12s - 579ms/step - loss: 23.0524 - val_loss: 20.5375 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "20/20 - 11s - 574ms/step - loss: 22.1091 - val_loss: 19.6610 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "20/20 - 15s - 728ms/step - loss: 21.4245 - val_loss: 18.0143 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "20/20 - 12s - 578ms/step - loss: 20.4442 - val_loss: 17.6931 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "20/20 - 12s - 579ms/step - loss: 20.3369 - val_loss: 17.2707 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "20/20 - 11s - 565ms/step - loss: 19.3090 - val_loss: 16.5258 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "20/20 - 12s - 605ms/step - loss: 18.8604 - val_loss: 15.8989 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "20/20 - 12s - 590ms/step - loss: 18.1949 - val_loss: 15.1603 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "20/20 - 12s - 610ms/step - loss: 17.8665 - val_loss: 14.7538 - learning_rate: 0.0010\n",
      "Epoch 25: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "models = {}\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=False, verbose=1)\n",
    "\n",
    "# Preparar los datos por cluster\n",
    "for cluster in range(n_clusters):\n",
    "    display(f'Entrenando cluster numero: {cluster}')\n",
    "    cluster_data = grouped_df[grouped_df['cluster'] == cluster].copy()\n",
    "    cluster_data.sort_values(by='periodo', inplace=True)\n",
    "    \n",
    "    X, y = [], []\n",
    "    X_weights = []\n",
    "\n",
    "    for key, data in cluster_data.groupby(['customer_id', 'product_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'quarter', 'month', 'customer_id', 'product_id', 'tn']].values\n",
    "        if len(series) > 2:  # Asegurarse de que haya suficientes datos\n",
    "            X.append(series[:-2])  # Todos los datos excepto los últimos 2\n",
    "            y.append(series[-1, -1])\n",
    "            \n",
    "            product_id = key[1]\n",
    "            total_tn = total_tn_dict.get(product_id, 0)\n",
    "            X_weights.append(total_tn)\n",
    "            \n",
    "    \n",
    "    #Padleft para que todos los registros tengan el mismo shape\n",
    "    max_len = max(len(seq) for seq in X)\n",
    "    X_padded = np.array([np.pad(seq, ((max_len - len(seq), 0), (0, 0)), mode='constant') for seq in X]).astype(np.float32)\n",
    "    y = np.array(y).astype(np.float32)\n",
    "    X_weights = np.array(X_weights).astype(np.float32)\n",
    "\n",
    "    # Debug\n",
    "    # print(len(X))\n",
    "    # print(len(X_padded))\n",
    "    # print(len(X_weights))\n",
    "    # print(len(y))\n",
    "    callbacks = [reduce_lr]\n",
    "\n",
    "    # if cluster in [0, 12]:\n",
    "    callbacks.append(early_stopping)\n",
    "    \n",
    "    # Construir y entrenar el modelo\n",
    "    model = build_lstm_model((X_padded.shape[1], X_padded.shape[2]))\n",
    "    model.fit(X_padded, y, epochs=100, verbose=2, batch_size=200, validation_split=0.2, sample_weight=X_weights, callbacks=callbacks)\n",
    "    models[cluster] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 7: Sumarizar las predicciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalers = joblib.load('scalers.pkl')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    if cluster not in models:\n",
    "        continue\n",
    "\n",
    "    model = models[cluster]\n",
    "    cluster_data = grouped_df[grouped_df['cluster'] == cluster].copy()\n",
    "    \n",
    "    X_pred_data = []\n",
    "    keys = []\n",
    "    for key, data in cluster_data.groupby(['customer_id', 'product_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'quarter', 'month', 'customer_id', 'product_id', 'tn']].values\n",
    "        max_len = len(series) - 1\n",
    "        X_pred = np.pad(series[1:], ((max_len - len(series[1:]), 0), (0, 0)), mode='constant').astype(np.float32)\n",
    "        X_pred_data.append(X_pred)\n",
    "        keys.append(key)\n",
    "    \n",
    "    if len(X_pred_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    max_len_pred = max(len(seq) for seq in X_pred_data)\n",
    "    X_pred_padded = np.array([np.pad(seq, ((max_len_pred - len(seq), 0), (0, 0)), mode='constant') for seq in X_pred_data]).astype(np.float32)\n",
    "    X_pred_padded = np.reshape(X_pred_padded, (X_pred_padded.shape[0], X_pred_padded.shape[1], X_pred_padded.shape[2]))\n",
    "    \n",
    "    preds = model.predict(X_pred_padded, verbose=0)\n",
    "    inversed_preds_df = []\n",
    "    \n",
    "    for key, pred in zip(keys, preds):\n",
    "        customer_id, product_id = key\n",
    "        scaler_key = f'{product_id}_{customer_id}'\n",
    "        scaler_tn = scalers[scaler_key]\n",
    "        inverse_pred = scaler_tn.inverse_transform([pred])\n",
    "        inversed_preds_df.append(inverse_pred[0][0])\n",
    "        predictions.append([customer_id, product_id, inverse_pred[0][0]])\n",
    "    \n",
    "    pred_df_temp = pd.DataFrame(inversed_preds_df, columns=['prediccion'])\n",
    "    pred_df_temp.to_csv(f\"predicciones_temprales_cluster_pID{cluster}.csv\", index=False)\n",
    "    \n",
    "consilated_df_temp = pd.DataFrame(predictions, columns=['customer_id', 'product_id', 'prediccion'])\n",
    "consilated_df_temp.to_csv(f\"predicciones_temprales_todos_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>prediccion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>1826.508892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>1508.269984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>1060.675599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>896.409915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>879.485729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>21263</td>\n",
       "      <td>0.049886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>21265</td>\n",
       "      <td>0.166793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>21266</td>\n",
       "      <td>0.172017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>21267</td>\n",
       "      <td>0.105599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>21276</td>\n",
       "      <td>0.052597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id   prediccion\n",
       "0         20001  1826.508892\n",
       "1         20002  1508.269984\n",
       "2         20003  1060.675599\n",
       "3         20004   896.409915\n",
       "4         20005   879.485729\n",
       "..          ...          ...\n",
       "775       21263     0.049886\n",
       "776       21265     0.166793\n",
       "777       21266     0.172017\n",
       "778       21267     0.105599\n",
       "779       21276     0.052597\n",
       "\n",
       "[780 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarized_preds = consilated_df_temp.groupby(['product_id'])['prediccion'].sum().reset_index()\n",
    "\n",
    "final_predictions_df = pd.DataFrame(summarized_preds, columns=['product_id', 'prediccion'])\n",
    "\n",
    "final_predictions_df.to_csv('predicciones_finales.csv', index=False)\n",
    "\n",
    "display(final_predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
