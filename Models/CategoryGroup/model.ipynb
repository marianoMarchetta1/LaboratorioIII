{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Promediar las vtas de agosto 2019 (201908) como las de julio (201907) y septiembre (201909) para todas las observaciones\n",
    "- Buscar los 'product_id' que tengan poca hitoria (agrupandolos por product_id y periodo y validar que tengan menos registros que training_trashold), eliminarlos del conjunto, y agregarlos el un dataframe \"Predicciones\", poniendo product_id junto con una columna \"prediccion\", que sea la media de las ventas de los periodos\n",
    "- Aplicar LabelEncoder a las columnas categoricas\n",
    "- Agrupar los restantes las ventas por periodo, cat1, cat2, cat3, marca y descripcion\n",
    "- Calcular para estos el ratio de ventas por product_id (para cada grupo de cat1, cat2, cat3, marca y descripcion), guardando esto en un diccionario: cat1, cat2, cat3, marca, descripcion, product_id y ratio\n",
    "\n",
    "----\n",
    "- Agrupar las ventas por periodo, cat1, cat2, cat3, marca, descripcion y customer_id. Sumarizando los valores de las columnas cust_request_qty, cust_request_tn y tn.\n",
    "- Aplicar escalers por columna a cada grupo (guardando estos scalers en un diccionario)\n",
    "- Armar un modelo LSTM para predecir las ventas de cada uno de estos grupos (usando todas las observaciones menos las ultimas 2 para predecir la ultima )\n",
    "----\n",
    "\n",
    "- Luego, para cada grupo, hacer las predicciones con su modelo correspondiente (usando todas las observaciones menos las primeras 2). Guardando estas predicciones en un dataframe con la estructura cat1, cat2, cat3, marca, descripcion\n",
    "- sumarizar las predicciones por cat1, cat2, cat3, marca, descripcion\n",
    "- para cada cat1, cat2, cat3, marca, descripcion, buscar los product_id en el diccionario de ratios, aplicarlo sobre las predicciones sumarizadas, y armar un dataframe product_id y prediccion\n",
    "- unificar este dataframe con el \"Predicciones\"\n",
    "- guardar este df en un csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Datasets/final_dataset_descr.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Promediar las ventas de agosto 2019 (201908) con julio (201907) y septiembre (201909)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['periodo'] = df['periodo'].astype(str).str.strip()\n",
    "\n",
    "# Filtrar los datos por los periodos 201907, 201908 y 201909\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "\n",
    "# # Pivotear los datos para tener columnas separadas para cada periodo\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "\n",
    "# # Asegurar que las columnas 201907 y 201909 existen en el DataFrame\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "\n",
    "# # Calcular el promedio de julio y septiembre\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "\n",
    "# # Convertir de nuevo al formato largo\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], \n",
    "                                   var_name='periodo', value_name='tn')\n",
    "\n",
    "# # Unir con el dataframe original\n",
    "df.set_index(['product_id', 'customer_id', 'periodo'], inplace=True)\n",
    "df.update(updated_sales.set_index(['product_id', 'customer_id', 'periodo']))\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3: Filtrar y eliminar productos con poca historia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_threshold = 12\n",
    "\n",
    "# Contar el número de registros por product_id, customer_id y periodo\n",
    "product_history = df.groupby(['product_id', 'customer_id']).size().reset_index(name='counts')\n",
    "\n",
    "# Filtrar productos por customer_id con menos registros que el threshold\n",
    "products_to_keep = product_history[product_history['counts'] >= training_threshold][['product_id', 'customer_id']].drop_duplicates()\n",
    "df_filtered = df.merge(products_to_keep, on=['product_id', 'customer_id'], how='inner')\n",
    "\n",
    "# Crear el DataFrame \"Predicciones\" para productos con poca historia por customer_id\n",
    "products_to_predict = product_history[product_history['counts'] < training_threshold][['product_id', 'customer_id']].drop_duplicates()\n",
    "predicciones = df.merge(products_to_predict, on=['product_id', 'customer_id'], how='inner').groupby(['product_id', 'customer_id'])['tn'].mean().reset_index()\n",
    "predicciones.rename(columns={'tn': 'prediccion'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: Aplicar LabelEncoder a las columnas categóricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'quarter']\n",
    "\n",
    "# Aplicar LabelEncoder\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_filtered[col] = le.fit_transform(df_filtered[col])\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5: Agrupar y calcular el ratio de ventas por product_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por las columnas relevantes\n",
    "grouped_sales = df_filtered.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['tn'].sum().reset_index()\n",
    "\n",
    "# Calcular el total de ventas por grupo\n",
    "group_totals = grouped_sales.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion'])['tn'].sum().reset_index()\n",
    "\n",
    "# Unir para calcular el ratio\n",
    "ratios = pd.merge(grouped_sales, group_totals, on=['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion'], suffixes=('', '_total'))\n",
    "\n",
    "# Calcular el ratio\n",
    "ratios['ratio'] = ratios['tn'] / ratios['tn_total']\n",
    "\n",
    "# Crear un diccionario de ratios\n",
    "ratio_dict = ratios.set_index(['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['ratio'].to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 6: Agrupar ventas por periodo, cat1, cat2, cat3, brand, descripcion y customer_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar y sumarizar\n",
    "grouped_df = df_filtered.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'customer_id', 'quarter', 'month']).agg({\n",
    "    'cust_request_qty': 'sum',\n",
    "    'cust_request_tn': 'sum',\n",
    "    'tn': 'sum'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 7: Aplicar escalers por columna a cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar los scalers\n",
    "scalers = {}\n",
    "scaled_df = grouped_df.copy()\n",
    "\n",
    "# Aplicar StandardScaler a cada columna de interés\n",
    "for col in ['cust_request_qty', 'cust_request_tn', 'tn']:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df[col] = scaler.fit_transform(scaled_df[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 8: Armar un modelo LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, GRU\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(256, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    # model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True))\n",
    "    # model.add(Dropout(0.1))\n",
    "    # model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True))\n",
    "    # model.add(Dropout(0.1))\n",
    "    # model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True))\n",
    "    # model.add(Dropout(0.1))\n",
    "    # model.add(LSTM(256, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True))\n",
    "    # model.add(Dropout(0.1))\n",
    "    model.add(LSTM(128, activation='relu', kernel_regularizer=l2(0.7)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 9: Entrenar y predecir con el modelo LSTM para cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = grouped_df.groupby(['cat1', 'cat2', 'cat3', 'customer_id', 'brand', 'descripcion'])\n",
    "\n",
    "# Obtener la lista de grupos\n",
    "groups_list = list(grouped.groups.keys())\n",
    "\n",
    "group_key = groups_list[29]\n",
    "\n",
    "# Obtener el contenido del grupo específico\n",
    "group_content = grouped.get_group(group_key)\n",
    "\n",
    "group_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(grouped_df.groupby(['cat1', 'cat2', 'cat3', 'customer_id', 'brand', 'descripcion'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por las columnas relevantes\n",
    "grouped_df['periodo'] = pd.to_datetime(grouped_df['periodo'], format='%Y%m')\n",
    "\n",
    "# Crear un diccionario para almacenar los modelos por grupo\n",
    "models = {}\n",
    "predictions = []\n",
    "cont = 0\n",
    "\n",
    "for (cat1, cat2, cat3, brand, descripcion, customer_id), group_data in grouped_df.groupby(['cat1', 'cat2', 'cat3', 'customer_id', 'brand', 'descripcion']):\n",
    "    # Ordenar por periodo\n",
    "    group_data = group_data.sort_values(by='periodo')\n",
    "    \n",
    "    # Transformar los datos para LSTM\n",
    "    n_steps = 2  # Por ejemplo\n",
    "    X, y = [], []\n",
    "    \n",
    "    X = group_data[['cust_request_qty', 'cust_request_tn', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'customer_id', 'quarter', 'month', 'tn']]\n",
    "    y = group_data['tn']\n",
    "    \n",
    "    X = np.reshape(X, (X.shape[0], 1, X.shape[1]))  # (número de muestras, timesteps, características)\n",
    "    \n",
    "    # Construir y entrenar el modelo\n",
    "    model = build_lstm_model((X.shape[1], X.shape[2]))\n",
    "    model.fit(X[:-n_steps], y[n_steps:], epochs=140, verbose=0)\n",
    "    models[(cat1, cat2, cat3, brand, descripcion, customer_id)] = model\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    X_pred = group_data[['cust_request_qty', 'cust_request_tn', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'customer_id', 'quarter', 'month', 'tn']].values[-n_steps:]  # Tomar los últimos n_steps\n",
    "    X_pred = np.reshape(X_pred, (1, X_pred.shape[0], X_pred.shape[1]))  # Asegurar la forma correcta para la predicción\n",
    "    pred = model.predict(X_pred, verbose=0)\n",
    "    predictions.append([cat1, cat2, cat3, brand, descripcion, customer_id, pred[0][0]])\n",
    "    display(f\"Prediccion {pred} - cont: {cont}\")\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 10: Sumarizar las predicciones y aplicar ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las predicciones a un DataFrame\n",
    "pred_df = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'prediccion', 'customer_id'])\n",
    "\n",
    "# Sumarizar las predicciones por grupo\n",
    "summarized_preds = pred_df.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion'])['prediccion'].sum().reset_index()\n",
    "\n",
    "# Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in summarized_preds.iterrows():\n",
    "    key = (row['cat1'], row['cat2'], row['cat3'], row['brand'], row['descripcion'])\n",
    "    for (cat1, cat2, cat3, brand, descripcion, product_id), ratio in ratio_dict.items():\n",
    "        if (cat1, cat2, cat3, brand, descripcion) == key:\n",
    "            final_predictions.append([product_id, row['prediccion'] * ratio])\n",
    "\n",
    "# Convertir las predicciones finales a un DataFrame\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediccion'])\n",
    "\n",
    "# Paso 11: Desescalar las predicciones finales\n",
    "# Cargar los scalers guardados\n",
    "scalers = joblib.load('scalers.pkl')\n",
    "\n",
    "# Desescalar las predicciones finales\n",
    "final_predictions_df['prediccion'] = scalers['tn'].inverse_transform(final_predictions_df[['prediccion']])\n",
    "\n",
    "# Unificar con el DataFrame \"Predicciones\"\n",
    "final_df = pd.concat([final_predictions_df, predicciones])\n",
    "\n",
    "# Guardar el resultado en un archivo CSV\n",
    "final_df.to_csv('predicciones_finales.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
