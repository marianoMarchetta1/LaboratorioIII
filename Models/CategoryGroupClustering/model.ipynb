{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Promediar las vtas de agosto 2019 (201908) como las de julio (201907) y septiembre (201909) para todas las observaciones\n",
    "- Buscar los 'product_id' que tengan poca hitoria (agrupandolos por product_id y periodo y validar que tengan menos registros que training_trashold), eliminarlos del conjunto, y agregarlos el un dataframe \"Predicciones\", poniendo product_id junto con una columna \"prediccion\", que sea la media de las ventas de los periodos\n",
    "- Aplicar LabelEncoder a las columnas categoricas\n",
    "- Agrupar los restantes las ventas por periodo, cat1, cat2, cat3, marca y descripcion\n",
    "- Calcular para estos el ratio de ventas por product_id (para cada grupo de cat1, cat2, cat3, marca y descripcion), guardando esto en un diccionario: cat1, cat2, cat3, marca, descripcion, product_id y ratio\n",
    "\n",
    "----\n",
    "- Agrupar las ventas por periodo, cat1, cat2, cat3, marca, descripcion y customer_id. Sumarizando los valores de las columnas cust_request_qty, cust_request_tn y tn.\n",
    "- Aplicar escalers por columna a cada grupo (guardando estos scalers en un diccionario)\n",
    "- Armar un modelo LSTM para predecir las ventas de cada uno de estos grupos (usando todas las observaciones menos las ultimas 2 para predecir la ultima )\n",
    "----\n",
    "\n",
    "- Luego, para cada grupo, hacer las predicciones con su modelo correspondiente (usando todas las observaciones menos las primeras 2). Guardando estas predicciones en un dataframe con la estructura cat1, cat2, cat3, marca, descripcion\n",
    "- sumarizar las predicciones por cat1, cat2, cat3, marca, descripcion\n",
    "- para cada cat1, cat2, cat3, marca, descripcion, buscar los product_id en el diccionario de ratios, aplicarlo sobre las predicciones sumarizadas, y armar un dataframe product_id y prediccion\n",
    "- unificar este dataframe con el \"Predicciones\"\n",
    "- guardar este df en un csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Datasets/final_dataset_descr.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3: Filtrar y eliminar productos con poca historia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completamos el dataset con 0 para los producto / cliente que no existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ids = [200001, 200002, 200003, 200004, 200005, 200006, 200007, 200008, 200009, 200010, 200011, 200012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['product_id'].isin(product_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "\n",
    "# Crear un DataFrame con información de productos (esto puede ser parte del dataset original)\n",
    "product_info = df[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion']].drop_duplicates()\n",
    "\n",
    "# Paso 2: Identificar el primer y último mes en que cada cliente compró cada producto\n",
    "min_max_periods = df.groupby(['customer_id', 'product_id'])['periodo'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Crear una lista para almacenar los DataFrames generados\n",
    "all_dfs = []\n",
    "\n",
    "# Generar registros para cada combinación de customer_id y product_id\n",
    "for _, row in min_max_periods.iterrows():\n",
    "    customer_id = row['customer_id']\n",
    "    product_id = row['product_id']\n",
    "    min_period = row['min']\n",
    "    max_period = row['max']\n",
    "    all_periods = pd.date_range(min_period, max_period, freq='MS')\n",
    "    \n",
    "    combinations = pd.DataFrame({\n",
    "        'customer_id': [customer_id] * len(all_periods),\n",
    "        'product_id': [product_id] * len(all_periods),\n",
    "        'periodo': all_periods\n",
    "    })\n",
    "    \n",
    "    # Paso 3: Merge con el DataFrame original para identificar las combinaciones faltantes\n",
    "    merged_df = pd.merge(combinations, df, on=['customer_id', 'product_id', 'periodo'], how='left')\n",
    "    \n",
    "    # Completar las combinaciones faltantes con tn = 0\n",
    "    merged_df['tn'] = merged_df['tn'].fillna(0)\n",
    "    \n",
    "    # Paso 4: Rellenar las columnas con valores correspondientes de product_info\n",
    "    merged_df = pd.merge(merged_df, product_info, on='product_id', how='left', suffixes=('', '_y'))\n",
    "    \n",
    "    # Calcular las columnas 'quarter' y 'month' a partir del 'periodo'\n",
    "    merged_df['quarter'] = merged_df['periodo'].dt.to_period('Q').astype(str).str[-1]\n",
    "    merged_df['month'] = merged_df['periodo'].dt.month.astype(str).str.zfill(2)\n",
    "    \n",
    "    # Rellenar las demás columnas con valores predeterminados si es necesario\n",
    "    merged_df['plan_precios_cuidados'] = merged_df['plan_precios_cuidados'].fillna(0)\n",
    "    merged_df['cust_request_qty'] = merged_df['cust_request_qty'].fillna(0)\n",
    "    merged_df['cust_request_tn'] = merged_df['cust_request_tn'].fillna(0)\n",
    "    merged_df['close_quarter'] = merged_df['close_quarter'].fillna(0)\n",
    "    merged_df['age'] = merged_df['age'].fillna(0)\n",
    "    \n",
    "    # Agregar el DataFrame resultante a la lista\n",
    "    all_dfs.append(merged_df)\n",
    "\n",
    "# Concatenar todos los DataFrames generados\n",
    "df_full = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Ordenar por customer_id, product_id y periodo\n",
    "df_full = df_full.sort_values(by=['customer_id', 'product_id', 'periodo'])\n",
    "\n",
    "# Convertir la columna 'periodo' de vuelta al formato original (YYYYMM)\n",
    "df_full['periodo'] = df_full['periodo'].dt.strftime('%Y%m')\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_threshold = 12\n",
    "\n",
    "# Contar el número de registros por product_id, customer_id y periodo\n",
    "product_history = df.groupby(['product_id', 'customer_id']).size().reset_index(name='counts')\n",
    "\n",
    "# Filtrar productos por customer_id con menos registros que el threshold\n",
    "products_to_keep = product_history[product_history['counts'] >= training_threshold][['product_id', 'customer_id']].drop_duplicates()\n",
    "df_filtered = df.merge(products_to_keep, on=['product_id', 'customer_id'], how='inner')\n",
    "\n",
    "# Crear el DataFrame \"Predicciones\" para productos con poca historia por customer_id\n",
    "products_to_predict = product_history[product_history['counts'] < training_threshold][['product_id', 'customer_id']].drop_duplicates()\n",
    "predicciones = df.merge(products_to_predict, on=['product_id', 'customer_id'], how='inner').groupby(['product_id', 'customer_id'])['tn'].mean().reset_index()\n",
    "predicciones.rename(columns={'tn': 'prediccion'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: Aplicar LabelEncoder a las columnas categóricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'quarter']\n",
    "\n",
    "# Aplicar LabelEncoder\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_filtered[col] = le.fit_transform(df_filtered[col])\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5: Agrupar y calcular el ratio de ventas por product_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Probar con market share de otros meses\n",
    "\n",
    "# Agrupar por las columnas relevantes\n",
    "grouped_sales = df_filtered.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'product_id'])['tn'].sum().reset_index()\n",
    "\n",
    "# Calcular el total de ventas por grupo\n",
    "group_totals = grouped_sales.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand'])['tn'].sum().reset_index()\n",
    "\n",
    "# Unir para calcular el ratio\n",
    "ratios = pd.merge(grouped_sales, group_totals, on=['periodo', 'cat1', 'cat2', 'cat3', 'brand'], suffixes=('', '_total'))\n",
    "\n",
    "# Calcular el ratio\n",
    "ratios['ratio'] = ratios['tn'] / ratios['tn_total']\n",
    "\n",
    "# Crear un diccionario de ratios\n",
    "ratio_dict = ratios.set_index(['cat1', 'cat2', 'cat3', 'brand', 'product_id'])['ratio'].to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 6: Agrupar ventas por periodo, cat1, cat2, cat3, brand, descripcion y customer_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar y sumarizar\n",
    "grouped_df = df_filtered.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'quarter', 'month']).agg({\n",
    "    'cust_request_qty': 'sum',\n",
    "    'cust_request_tn': 'sum',\n",
    "    'tn': 'sum'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico DTW para agrupar los registros (series de categorias/clientes similares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "pivoted_df = grouped_df.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], columns='periodo', values='tn').fillna(0)\n",
    "\n",
    "# Escalar las series temporales\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)  # normalizar las series temporales\n",
    "scaled_series = scaler.fit_transform(pivoted_df.values)\n",
    "\n",
    "# Paso 3: Aplicar clustering con DTW\n",
    "n_clusters = 10  # Número máximo de grupos\n",
    "model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=10, random_state=0)\n",
    "cluster_labels = model.fit_predict(scaled_series)\n",
    "\n",
    "# Paso 4: Asignar el número de grupo a cada elemento de grouped_df\n",
    "pivoted_df['cluster'] = cluster_labels\n",
    "\n",
    "# Unir el número de grupo con grouped_df\n",
    "grouped_df = grouped_df.merge(pivoted_df['cluster'], left_on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], right_index=True)\n",
    "\n",
    "# Guardar el resultado en un archivo CSV (opcional)\n",
    "grouped_df.to_csv('grouped_with_clusters.csv', index=False)\n",
    "\n",
    "# Mostrar el DataFrame final con los números de grupo\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 7: Aplicar escalers por columna a cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar los scalers\n",
    "scalers = {}\n",
    "scaled_df = grouped_df.copy()\n",
    "\n",
    "# Aplicar StandardScaler a cada columna de interés\n",
    "for col in ['cust_request_qty', 'cust_request_tn', 'tn']:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df[col] = scaler.fit_transform(scaled_df[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 8: Armar un modelo LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, GRU\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='tanh', kernel_regularizer=l2(0.1), return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(100, activation='tanh', kernel_regularizer=l2(0.1), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(0.1), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(0.1), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(512, activation='tanh', kernel_regularizer=l2(0.1), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(256, activation='tanh', kernel_regularizer=l2(0.1), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(128, activation='relu', kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 9: Entrenar y predecir con el modelo LSTM para cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(len(grouped_df.groupby(['cat1', 'cat2', 'cat3', 'brand','customer_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "predictions = []\n",
    "\n",
    "# Preparar los datos por cluster\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_data = grouped_df[grouped_df['cluster'] == cluster] #TODO; Reemplazar por scale_df\n",
    "    if cluster_data.empty:\n",
    "        continue\n",
    "\n",
    "    cluster_data.sort_values(by='periodo', inplace=True)\n",
    "    \n",
    "    # Preparar los datos para LSTM\n",
    "    X, y = [], []\n",
    "    for key, data in cluster_data.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id']):\n",
    "        series = data[['cust_request_qty', 'cust_request_tn', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'quarter', 'month', 'tn']].values\n",
    "        n_steps = 2  # Número de pasos para LSTM\n",
    "        for i in range(n_steps, len(series)):\n",
    "            X.append(series[i-n_steps:i])\n",
    "            y.append(series[i, -1])  # 'tn' es la última columna\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if len(X) == 0 or len(y) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Construir y entrenar el modelo\n",
    "    model = build_lstm_model((X.shape[1], X.shape[2]))\n",
    "    model.fit(X, y, epochs=20, verbose=2, batch_size=32, validation_split=0.1)\n",
    "    models[cluster] = model\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    for key, data in cluster_data.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id']):\n",
    "        series = data[['cust_request_qty', 'cust_request_tn', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'quarter', 'month', 'tn']].values\n",
    "        X_pred = series[-n_steps:]  # Tomar los últimos n_steps\n",
    "        X_pred = np.reshape(X_pred, (1, X_pred.shape[0], X_pred.shape[1]))  # Asegurar la forma correcta para la predicción\n",
    "        pred = model.predict(X_pred, verbose=2)\n",
    "        predictions.append([key[0], key[1], key[2], key[3], pred[0][0], key[4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 10: Sumarizar las predicciones y aplicar ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>prediccion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>211.116870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>239.073733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>221.142498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>367.793635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>693.782610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>293.281043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>43.879916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.346868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>18.662593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.972551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cat1  cat2  cat3  brand  prediccion\n",
       "0     0.0   0.0   4.0   22.0  211.116870\n",
       "1     0.0   0.0  11.0   22.0  239.073733\n",
       "2     0.0   0.0  37.0   18.0  221.142498\n",
       "3     0.0   0.0  37.0   19.0  367.793635\n",
       "4     0.0   0.0  37.0   22.0  693.782610\n",
       "..    ...   ...   ...    ...         ...\n",
       "121   2.0   7.0  74.0   23.0  293.281043\n",
       "122   3.0  13.0   7.0   32.0   43.879916\n",
       "123   3.0  13.0  28.0   32.0   16.346868\n",
       "124   3.0  13.0  31.0   32.0   18.662593\n",
       "125   3.0  13.0  76.0   32.0   13.972551\n",
       "\n",
       "[126 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(summarized_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>prediccion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20609</td>\n",
       "      <td>211.116870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20325</td>\n",
       "      <td>33.006901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20266</td>\n",
       "      <td>203.177308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20503</td>\n",
       "      <td>2.889524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20299</td>\n",
       "      <td>221.142498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>21170</td>\n",
       "      <td>16.346868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>21202</td>\n",
       "      <td>11.675472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>21218</td>\n",
       "      <td>6.987121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>21192</td>\n",
       "      <td>5.082118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>21222</td>\n",
       "      <td>8.890434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>655 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id  prediccion\n",
       "0         20609  211.116870\n",
       "1         20325   33.006901\n",
       "2         20266  203.177308\n",
       "3         20503    2.889524\n",
       "4         20299  221.142498\n",
       "..          ...         ...\n",
       "650       21170   16.346868\n",
       "651       21202   11.675472\n",
       "652       21218    6.987121\n",
       "653       21192    5.082118\n",
       "654       21222    8.890434\n",
       "\n",
       "[655 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'prediccion', 'customer_id'])\n",
    "pred_df = pd.concat([pred_df, predicciones])\n",
    "\n",
    "#TODO; Descomentar cuando modifique lo de arriba para usar scaled_df\n",
    "# scaler_tn = scalers['tn']\n",
    "# pred_df['prediccion'] = scaler_tn.inverse_transform(pred_df[['prediccion']])\n",
    "\n",
    "# Sumarizar las predicciones por grupo\n",
    "summarized_preds = pred_df.groupby(['cat1', 'cat2', 'cat3', 'brand'])['prediccion'].sum().reset_index()\n",
    "\n",
    "# Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "# Crear una lista para almacenar las predicciones finales\n",
    "final_predictions = []\n",
    "\n",
    "# Iterar sobre cada fila en summarized_preds\n",
    "for index, row in summarized_preds.iterrows():\n",
    "    cat1 = row['cat1']\n",
    "    cat2 = row['cat2']\n",
    "    cat3 = row['cat3']\n",
    "    brand = row['brand']\n",
    "    prediccion = row['prediccion']\n",
    "    \n",
    "    # Buscar los ratios correspondientes en ratio_dict\n",
    "    for (cat1_dict, cat2_dict, cat3_dict, brand_dict, product_id), ratio in ratio_dict.items():\n",
    "        if cat1 == cat1_dict and cat2 == cat2_dict and cat3 == cat3_dict and brand == brand_dict:\n",
    "            # Aplicar el ratio al valor de prediccion\n",
    "            prediction_adjusted = prediccion * ratio\n",
    "            # Agregar a la lista de predicciones finales\n",
    "            final_predictions.append([product_id, prediction_adjusted])\n",
    "\n",
    "# Crear DataFrame con las predicciones finales\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediccion'])\n",
    "\n",
    "# Mostrar el DataFrame final con las predicciones ajustadas por ratios\n",
    "display(final_predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df[['product_id', 'prediccion']].to_csv('predicciones_finales.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
