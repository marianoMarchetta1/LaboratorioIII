{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Promediar las vtas de agosto 2019 (201908) como las de julio (201907) y septiembre (201909) para todas las observaciones\n",
    "- Buscar los 'product_id' que tengan poca hitoria (agrupandolos por product_id y periodo y validar que tengan menos registros que training_trashold), eliminarlos del conjunto, y agregarlos el un dataframe \"Predicciones\", poniendo product_id junto con una columna \"prediccion\", que sea la media de las ventas de los periodos\n",
    "- Aplicar LabelEncoder a las columnas categoricas\n",
    "- Agrupar los restantes las ventas por periodo, cat1, cat2, cat3, marca y descripcion\n",
    "- Calcular para estos el ratio de ventas por product_id (para cada grupo de cat1, cat2, cat3, marca y descripcion), guardando esto en un diccionario: cat1, cat2, cat3, marca, descripcion, product_id y ratio\n",
    "\n",
    "----\n",
    "- Agrupar las ventas por periodo, cat1, cat2, cat3, marca, descripcion y customer_id. Sumarizando los valores de las columnas cust_request_qty, cust_request_tn y tn.\n",
    "- Aplicar escalers por columna a cada grupo (guardando estos scalers en un diccionario)\n",
    "- Armar un modelo con AUTOML para predecir las ventas de cada uno de estos grupos (usando todas las observaciones menos las ultimas 2 para predecir la ultima )\n",
    "----\n",
    "\n",
    "- Luego, para cada grupo, hacer las predicciones con su modelo correspondiente (usando todas las observaciones menos las primeras 2). Guardando estas predicciones en un dataframe con la estructura cat1, cat2, cat3, marca, descripcion\n",
    "- sumarizar las predicciones por cat1, cat2, cat3, marca, descripcion\n",
    "- para cada cat1, cat2, cat3, marca, descripcion, buscar los product_id en el diccionario de ratios, aplicarlo sobre las predicciones sumarizadas, y armar un dataframe product_id y prediccion\n",
    "- unificar este dataframe con el \"Predicciones\"\n",
    "- guardar este df en un csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 14:27:30.825740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Datasets/final_dataset_descr.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>99.43861</td>\n",
       "      <td>99.43861</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>10002</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>38.68301</td>\n",
       "      <td>35.72806</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>10003</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>143.49426</td>\n",
       "      <td>143.49426</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>10004</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>184.72927</td>\n",
       "      <td>184.72927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>10005</td>\n",
       "      <td>20001</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>19.08407</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periodo  customer_id  product_id  plan_precios_cuidados  cust_request_qty  \\\n",
       "0   201701        10001       20001                      0                11   \n",
       "1   201701        10002       20001                      0                17   \n",
       "2   201701        10003       20001                      0                17   \n",
       "3   201701        10004       20001                      0                 9   \n",
       "4   201701        10005       20001                      0                23   \n",
       "\n",
       "   cust_request_tn         tn cat1         cat2     cat3  brand  sku_size  \\\n",
       "0         99.43861   99.43861   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "1         38.68301   35.72806   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "2        143.49426  143.49426   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "3        184.72927  184.72927   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "4         19.08407   19.08407   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "\n",
       "  descripcion quarter  month  close_quarter  age  \n",
       "0      genoma      Q1      1              0    0  \n",
       "1      genoma      Q1      1              0    0  \n",
       "2      genoma      Q1      1              0    0  \n",
       "3      genoma      Q1      1              0    0  \n",
       "4      genoma      Q1      1              0    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Promediar las ventas de agosto 2019 (201908) con julio (201907) y septiembre (201909)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los datos por los periodos 201907, 201908 y 201909\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "\n",
    "# Pivotear los datos para tener columnas separadas para cada periodo\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "\n",
    "# Asegurar que las columnas 201907 y 201909 existen en el DataFrame\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "\n",
    "# Calcular el promedio de julio y septiembre\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "\n",
    "# Convertir de nuevo al formato largo\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], \n",
    "                                   var_name='periodo', value_name='tn')\n",
    "\n",
    "# Unir con el dataframe original\n",
    "df = df.drop(columns=['tn'])\n",
    "df = pd.merge(df, updated_sales, on=['product_id', 'customer_id', 'periodo'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3: Filtrar y eliminar productos con poca historia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_threshold = 3\n",
    "\n",
    "# Contar el número de registros por product_id y periodo\n",
    "product_history = df.groupby(['product_id', 'periodo']).size().reset_index(name='counts')\n",
    "\n",
    "# Filtrar productos con menos registros que el threshold\n",
    "products_to_keep = product_history[product_history['counts'] >= training_threshold]['product_id'].unique()\n",
    "df_filtered = df[df['product_id'].isin(products_to_keep)]\n",
    "\n",
    "# Crear el DataFrame \"Predicciones\" para productos con poca historia\n",
    "products_to_predict = product_history[product_history['counts'] < training_threshold]['product_id'].unique()\n",
    "predicciones = df[df['product_id'].isin(products_to_predict)].groupby('product_id')['tn'].mean().reset_index()\n",
    "predicciones.rename(columns={'tn': 'prediccion'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: Aplicar LabelEncoder a las columnas categóricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "\n",
    "# Aplicar LabelEncoder\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_filtered[col] = le.fit_transform(df_filtered[col])\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5: Agrupar y calcular el ratio de ventas por product_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por las columnas relevantes\n",
    "grouped_sales = df_filtered.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['tn'].sum().reset_index()\n",
    "\n",
    "# Calcular el total de ventas por grupo\n",
    "group_totals = grouped_sales.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion'])['tn'].sum().reset_index()\n",
    "\n",
    "# Unir para calcular el ratio\n",
    "ratios = pd.merge(grouped_sales, group_totals, on=['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion'], suffixes=('', '_total'))\n",
    "\n",
    "# Calcular el ratio\n",
    "ratios['ratio'] = ratios['tn'] / ratios['tn_total']\n",
    "\n",
    "# Crear un diccionario de ratios\n",
    "ratio_dict = ratios.set_index(['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['ratio'].to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 6: Agrupar ventas por periodo, cat1, cat2, cat3, brand, descripcion y customer_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar y sumarizar\n",
    "grouped_df = df_filtered.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'customer_id']).agg({\n",
    "    'cust_request_qty': 'sum',\n",
    "    'cust_request_tn': 'sum',\n",
    "    'tn': 'sum'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 7: Aplicar escalers por columna a cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalers.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un diccionario para almacenar los scalers\n",
    "scalers = {}\n",
    "scaled_df = grouped_df.copy()\n",
    "\n",
    "# Aplicar StandardScaler a cada columna de interés\n",
    "for col in ['cust_request_qty', 'cust_request_tn', 'tn']:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df[col] = scaler.fit_transform(scaled_df[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 8: Armar un modelo TPOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmarchetta/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "def build_tpot_model(X_train, y_train):\n",
    "    tpot = TPOTRegressor(verbosity=2, generations=5, population_size=20)\n",
    "    tpot.fit(X_train, y_train)\n",
    "    return tpot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 9: Entrenar y predecir con el modelo LSTM para cada grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error: Input data is not in a valid format. Please confirm that the input data is scikit-learn compatible. For example, the features must be a 2-D array and target labels must be a 1-D array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/tpot/base.py:1388\u001b[0m, in \u001b[0;36mTPOTBase._check_dataset\u001b[0;34m(self, features, target, sample_weight)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1388\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imputed:\n",
      "File \u001b[0;32m~/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1273\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1271\u001b[0m     )\n\u001b[0;32m-> 1273\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n",
      "File \u001b[0;32m~/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1053\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m X, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Construir y entrenar el modelo\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tpot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     23\u001b[0m models[(cat1, cat2, cat3, brand, descripcion)] \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Hacer predicciones\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mbuild_tpot_model\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_tpot_model\u001b[39m(X_train, y_train):\n\u001b[1;32m      4\u001b[0m     tpot \u001b[38;5;241m=\u001b[39m TPOTRegressor(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, population_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtpot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tpot\n",
      "File \u001b[0;32m~/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/tpot/base.py:727\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit an optimized machine learning pipeline.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03mUses genetic programming to optimize a machine learning pipeline that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    724\u001b[0m \n\u001b[1;32m    725\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_init()\n\u001b[0;32m--> 727\u001b[0m features, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pretest(features, target)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# Randomly collect a subsample of training samples for pipeline optimization process.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/LaboratorioIII/my_env/lib/python3.10/site-packages/tpot/base.py:1400\u001b[0m, in \u001b[0;36mTPOTBase._check_dataset\u001b[0;34m(self, features, target, sample_weight)\u001b[0m\n\u001b[1;32m   1398\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAssertionError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Input data is not in a valid format. Please confirm \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1402\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat the input data is scikit-learn compatible. For example, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe features must be a 2-D array and target labels must be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1-D array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1405\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Input data is not in a valid format. Please confirm that the input data is scikit-learn compatible. For example, the features must be a 2-D array and target labels must be a 1-D array."
     ]
    }
   ],
   "source": [
    "grouped_df['periodo'] = pd.to_datetime(grouped_df['periodo'], format='%Y%m')\n",
    "\n",
    "# Crear un diccionario para almacenar los modelos por grupo\n",
    "models = {}\n",
    "predictions = []\n",
    "\n",
    "for (cat1, cat2, cat3, brand, descripcion), group_data in grouped_df.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion']):\n",
    "    # Ordenar por periodo\n",
    "    group_data = group_data.sort_values(by='periodo')\n",
    "    \n",
    "    # Crear características y etiquetas para el modelo\n",
    "    X = group_data[['cust_request_qty', 'cust_request_tn', 'tn']]\n",
    "    y = group_data['tn']\n",
    "    \n",
    "    # Entrenar el modelo con TPOT\n",
    "    model = build_tpot_model(X[:-2], y[:-2])  # Usar todas las observaciones menos las últimas 2 para entrenar\n",
    "    models[(cat1, cat2, cat3, brand, descripcion)] = model\n",
    "    \n",
    "    # Hacer predicciones usando todas las observaciones menos las primeras 2\n",
    "    pred = model.predict(X[2:])\n",
    "    predictions.extend([[cat1, cat2, cat3, brand, descripcion, p] for p in pred])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 10: Sumarizar las predicciones y aplicar ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'prediccion'])\n",
    "\n",
    "# Sumarizar las predicciones por grupo\n",
    "summarized_preds = pred_df.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion'])['prediccion'].sum().reset_index()\n",
    "\n",
    "# Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in summarized_preds.iterrows():\n",
    "    key = (row['cat1'], row['cat2'], row['cat3'], row['brand'], row['descripcion'])\n",
    "    for (cat1, cat2, cat3, brand, descripcion, product_id), ratio in ratio_dict.items():\n",
    "        if (cat1, cat2, cat3, brand, descripcion) == key:\n",
    "            final_predictions.append([product_id, row['prediccion'] * ratio])\n",
    "\n",
    "# Convertir las predicciones finales a un DataFrame\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediccion'])\n",
    "\n",
    "# Paso 11: Desescalar las predicciones finales\n",
    "# Cargar los scalers guardados\n",
    "scalers = joblib.load('scalers.pkl')\n",
    "\n",
    "# Desescalar las predicciones finales\n",
    "final_predictions_df['prediccion'] = scalers['tn'].inverse_transform(final_predictions_df[['prediccion']])\n",
    "\n",
    "# Unificar con el DataFrame \"Predicciones\"\n",
    "final_df = pd.concat([final_predictions_df, predicciones])\n",
    "\n",
    "# Guardar el resultado en un archivo CSV\n",
    "final_df.to_csv('predicciones_finales.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
